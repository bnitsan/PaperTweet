,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1039473153565773824,1027201792139042816,Forrest W. Crawford,['New paper with postdocs Daniel Eck and Olga Morozova @ysphbiostat @YaleSPH on how causal inference in vaccine RCTs can go wrong when outcomes are contagious. \n<LINK>\n#causalinference #epidemiology #infectiousdisease #randomizedtrial'],https://arxiv.org/abs/1808.05593,"Randomized trials of infectious disease interventions, such as vaccines, often focus on groups of connected or potentially interacting individuals. When the pathogen of interest is transmissible between study subjects, interference may occur: individual infection outcomes may depend on treatments received by others. Epidemiologists have defined the primary causal effect of interest -- called the ""susceptibility effect"" -- as a contrast in infection risk under treatment versus no treatment, while holding exposure to infectiousness constant. A related quantity -- the ""direct effect"" -- is defined as an unconditional contrast between the infection risk under treatment versus no treatment. The purpose of this paper is to show that under a widely recommended randomization design, the direct effect may fail to recover the sign of the true susceptibility effect of the intervention in a randomized trial when outcomes are contagious. The analytical approach uses structural features of infectious disease transmission to define the susceptibility effect. A new probabilistic coupling argument reveals stochastic dominance relations between potential infection outcomes under different treatment allocations. The results suggest that estimating the direct effect under randomization may provide misleading inferences about the effect of an intervention -- such as a vaccine -- when outcomes are contagious. ","Randomization for the susceptibility effect of an infectious disease
  intervention"
1,1037784125183352834,905696518568632320,Maarten Van Reeuwijk,"['New paper submitted on suppression of mixing and entrainment in gravity currents, see <LINK>']",http://arxiv.org/abs/1808.08980,"We explore the dynamics of inclined temporal gravity currents using direct numerical simulation, and find that the current creates an environment in which the flux Richardson number $Ri_f$, gradient Richardson number $Ri_g$, and turbulent flux coefficient $\Gamma$ are constant across a large portion of the depth. Changing the slope angle $\alpha$ modifies these mixing parameters, and the flow approaches a maximum Richardson number $Ri_\textrm{max}\approx 0.15$ as $\alpha \rightarrow 0$ at which the entrainment coefficient $E \rightarrow 0$. The turbulent Prandtl number remains $O(1)$ for all slope angles, demonstrating that $E\rightarrow 0$ is not caused by a switch-off of the turbulent buoyancy flux as conjectured by Ellison (1957). Instead, $E \rightarrow 0$ occurs as the result of the turbulence intensity going to zero as $\alpha\rightarrow 0$, due to the flow requiring larger and larger shear to maintain the same level of turbulence. We develop an approximate model valid for small $\alpha$ which is able to predict accurately $Ri_f$, $Ri_g$ and $\Gamma$ as a function of $\alpha$ and their maximum attainable values. The model predicts an entrainment law of the form $E=0.31(Ri_\textrm{max}-Ri)$, which is in good agreement with the simulation data. The simulations and model presented here contribute to a growing body of evidence that an approach to a marginally or critically stable, relatively weakly stratified equilibrium for stratified shear flows may well be a generic property of turbulent stratified flows. ",Mixing and entrainment are suppressed in inclined gravity currents
2,1037394871718301696,16495124,Sam Thomson,"['WFSAs are dead. Long live WFSAs!\n\nOur new EMNLP paper ""Rational Recurrences"" is out: <LINK>\nWork by Hao Peng, @royschwartz02, me, and @nlpnoah. We analyze several recent RNNs as finite-state automata with neural transition weights. <LINK>', 'These include quasi-RNNs (Bradbury et al), strongly-typed RNNs (Balduzzi &amp; Ghifary), simple recurrent units (Lei et al), structurally constrained RNNs (Mikolov et al), input switched affine networks (Foerster et al)...', '*hand-waving* forget gates are self-loops!']",https://arxiv.org/abs/1808.09357,"Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models. ",Rational Recurrences
3,1036990410667765760,185910194,Graham Neubig,"['#EMNLP2018 paper ""Neural Cross-lingual NER with Minimal Resources"" handles lexicon mismatch w/ embedding+nearest neighbor, and word order mismatch w/ self attention: <LINK>\nNew SOTA on 2/3 CoNLL languages, and experiments on a real low-resource language. <LINK>', 'Great work by first author JT, along w/ co-authors Zhilin, @nlpnoah, and Jaime.']",https://arxiv.org/abs/1808.09861,"For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-rich languages would be an appealing capability. However, differences in words and word order across languages make it a challenging problem. To improve mapping of lexical items across languages, we propose a method that finds translations based on bilingual word embeddings. To improve robustness to word order differences, we propose to use self-attention, which allows for a degree of flexibility with respect to word order. We demonstrate that these methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a low-resource language. ",Neural Cross-Lingual Named Entity Recognition with Minimal Resources
4,1036617608160862208,2427297348,Ashish Mehta,"['Our new paper ""Learning End-to-end Autonomous Driving using Guided Auxiliary  Supervision"", outlines a Multi-Task Learning from Demonstration framework for end-to-end autonomous driving in urban environments. \nWith @AdithyaSub86 and Anbumani Subramanian.\n\n<LINK>']",https://arxiv.org/abs/1808.10393,"Learning to drive faithfully in highly stochastic urban settings remains an open problem. To that end, we propose a Multi-task Learning from Demonstration (MT-LfD) framework which uses supervised auxiliary task prediction to guide the main task of predicting the driving commands. Our framework involves an end-to-end trainable network for imitating the expert demonstrator's driving commands. The network intermediately predicts visual affordances and action primitives through direct supervision which provide the aforementioned auxiliary supervised guidance. We demonstrate that such joint learning and supervised guidance facilitates hierarchical task decomposition, assisting the agent to learn faster, achieve better driving performance and increases transparency of the otherwise black-box end-to-end network. We run our experiments to validate the MT-LfD framework in CARLA, an open-source urban driving simulator. We introduce multiple non-player agents in CARLA and induce temporal noise in them for realistic stochasticity. ","Learning End-to-end Autonomous Driving using Guided Auxiliary
  Supervision"
5,1036579584286842880,2842875815,Raquel FernÃ¡ndez,"['New paper at #BlackboxNLP workshop #emnlp2018 with @_dieuwke_  and Sanne Bouwmeester on analyzing how seq2seq models process disfluencies, using synthetic task-oriented dialogue data  <LINK>']",https://arxiv.org/abs/1808.09178,"We investigate how encoder-decoder models trained on a synthetic dataset of task-oriented dialogues process disfluencies, such as hesitations and self-corrections. We find that, contrary to earlier results, disfluencies have very little impact on the task success of seq-to-seq models with attention. Using visualisation and diagnostic classifiers, we analyse the representations that are incrementally built by the model, and discover that models develop little to no awareness of the structure of disfluencies. However, adding disfluencies to the data appears to help the model create clearer representations overall, as evidenced by the attention patterns the different models exhibit. ","Analysing the potential of seq-to-seq models for incremental
  interpretation in task-oriented dialogue"
6,1036534470848008192,382703383,FrÃ©deric Godin,['It is important to understand what neural networks learn to build robust models.\nWe compare the character-level patterns BiLSTMs and CNNs learn and found out they generally follow linguistic intuitions!\nRead it in our new #EMNLP2018 paper &gt;\n<LINK> #dlearn #nlproc <LINK>'],https://arxiv.org/abs/1808.09551,"Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules. Our implementation can be found at this https URL . ","Explaining Character-Aware Neural Networks for Word-Level Prediction: Do
  They Discover Linguistic Rules?"
7,1036516815743184896,1710697381,Diego F. Torres,['New paper (ApJ Letters) announced today in astro-ph <LINK> Discovery of an amazing phase in the evolution of all pulsar wind nebula where the spin-down power of the pulsar is no longer the ruler. <LINK>'],https://arxiv.org/abs/1808.10613,"We numerically study the radiative properties of the reverberation phase of pulsar wind nebulae. Reverberation brings a significant evolution in a short period of time. We show that even the Crab nebula, associated to the more energetic pulsar of the sample we consider, has a period in its future time evolution where the X-ray luminosity will exceed the spin-down power at the time. In fact, all nebulae in our sample are expected to have a period of radio, X-ray, and GeV superefficiency, and most will also have a period of TeV superefficiency. We analyze and characterize these superefficient phases. ",Discovery and characterization of superefficiency in pulsar wind nebulae
8,1035566279317483520,185910194,Graham Neubig,"['#EMNLP2018 paper on adapting word embeddings to new languages using linguistic features: <LINK>\nWe use morphological and phonemic features, allowing better knowledge sharing across languages w/ different writing systems or rich morphology. Nice results on NER/MT! <LINK>', 'Particular congrats to first author Aditi, and the other co-authors @violet_zct, Lori, David, and Jaime.']",https://arxiv.org/abs/1808.09500,"Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU. ","Adapting Word Embeddings to New Languages with Morphological and
  Phonological Subword Representations"
9,1035349002034786309,3301643341,Roger Grosse,"['New paper with Kevin Luk on the invariances of K-FAC. We construct the algorithm out of coordinate-free objects (affine spaces, Riemannian metrics, etc.) so the invariances follow automatically. Extends to metrics other than Fisher.\n\n<LINK>', 'A network corresponds to a series of affine maps. Affine bases for the activations and pre-activations determine a natural basis for the maps. So affine change-of-basis of activ. and pre-activ. captures all the interesting reparameterizations. K-FAC is invariant to these.', ""You're not limited to the Fisher metric. Come up with a sensible metric on output space (e.g. Euclidean), pull it back to the parameters, and apply the K-FAC approximation. You get invariance to affine change-of-basis, except possibly of the output layer.""]",https://arxiv.org/abs/1808.10340,"Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC applied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric. ",A Coordinate-Free Construction of Scalable Natural Gradient
10,1035241034505232384,191872322,Jan Claes,"['New paper with Philippe De Meyer: ""An overview of process model quality literature - The Comprehensive Process Model Quality Framework"" <LINK>, published at <LINK>. <LINK>']",https://arxiv.org/abs/1808.07930,"The rising interest in the construction and the quality of (business) process models resulted in an abundancy of emerged research studies and different findings about process model quality. The lack of overview and the lack of consensus hinder the development of the research field. The research objective is to collect, analyse, structure, and integrate the existing knowledge in a comprehensive framework that strives to find a balance between completeness and relevance without hindering the overview. The Systematic Literature Review methodology was applied to collect the relevant studies. Because several studies exist that each partially addresses this research objective, the review was performed at a tertiary level. Based on a critical analysis of the collected papers, a comprehensive, but structured overview of the state of the art in the field was composed. The existing academic knowledge about process model quality was carefully integrated and structured into the Comprehensive Process Model Quality Framework (CPMQF). The framework summarizes 39 quality dimensions, 21 quality metrics, 28 quality (sub)drivers, 44 (sub)driver metrics, 64 realization initiatives and 15 concrete process model purposes related to 4 types of organizational benefits, as well as the relations between all of these. This overview is thus considered to form a valuable instrument for both researchers and practitioners that are concerned about process model quality. The framework is the first to address the concept of process model quality in such a comprehensive way. ","An overview of process model quality literature - The Comprehensive
  Process Model Quality Framework"
11,1035161495007625218,34376328,Tal Linzen,"['New #emnlp2018 paper with Marten van Schijndel: a simple continuously adapting neural language model (with gradient updates after each test sentence) improves fit to human reading time and rapidly adapts to abstract syntactic constructions <LINK> <LINK>', '(The plot shows reading times and surprisal on sentences with a main verb / reduced relative clause temporary ambiguity, e.g.:\n""The experienced soldiers warned about the dangers conducted the midnight raid"".)']",https://arxiv.org/abs/1808.09930,It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context. We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model. We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures. ,A Neural Model of Adaptation in Reading
12,1035134747549356033,88806960,Dr. Vivienne Baldassare,"['Check out our new paper on arxiv today on optical variability as a tool for identifying AGNs in low-mass galaxies. Plot below shows population of low-mass galaxies with AGN-like variability and star-formation dominated narrow line ratios. <LINK> <LINK>', 'Also see https://t.co/gZ8UTbGEKJ for light curves and difference imaging videos :)', '@profjsb Thanks! We actually used your qsofit code to select our AGNs!']",https://arxiv.org/abs/1808.09578,"We present an analysis of the nuclear variability of $\sim28,000$ nearby ($z<0.15$) galaxies with Sloan Digital Sky Survey (SDSS) spectroscopy in Stripe 82. We construct light curves using difference imaging of SDSS g-band images, which allows us to detect subtle variations in the central light output. We select variable AGN by assessing whether detected variability is well-described by a damped random walk model. We find 135 galaxies with AGN-like nuclear variability. While most of the variability-selected AGNs have narrow emission lines consistent with the presence of an AGN, a small fraction have narrow emission lines dominated by star formation. The star-forming systems with nuclear AGN-like variability tend to be low-mass ($M_{\ast}<10^{10}~M_{\odot}$), and may be AGNs missed by other selection techniques due to star formation dilution or low-metallicities. We explore the AGN fraction as a function of stellar mass, and find that the fraction of variable AGN increases with stellar mass, even after taking into account the fact that lower mass systems are fainter. There are several possible explanations for an observed decline in the fraction of variable AGN with decreasing stellar mass, including a drop in the supermassive black hole occupation fraction, a decrease in the ratio of black hole mass to galaxy stellar mass, or a change in the variability properties of lower-mass AGNs. We demonstrate that optical photometric variability is a promising avenue for detecting AGNs in low-mass, star formation-dominated galaxies, which has implications for the upcoming Large Synoptic Survey Telescope. ",Identifying AGNs in low-mass galaxies via long-term optical variability
13,1035093765822853121,20703003,Peter B Denton,"['I have another new neutrino paper out with Stephen Parke: ""The Effective Î”m^2_{ee} in Matter"" <LINK>\n\nWhat does it mean? [1/4]', 'For vacuum oscillations, Stephen and others pointed out that an approximate two-flavor picture works quite well for reactor experiments (Daya Bay, Reno) with an average of Î”m^2_{31} and Î”m^2_{32}. But what was the correct way to average these? [2/4]', 'It turns out that the optimal way is with the electron neutrino weighting of each mass state, that is, the theta_{12} weighting, dubbed Î”m^2_{ee}. [3/4] https://t.co/lAbLKD47rD', 'In this new paper out today, we show how to do the same thing for matter effects which has a surprisingly simple form.\n\nAnd by using previous results, we found an even simpler way to write it (hats mean ""in matter""). \n\n(We also verified that everything is super precise.) [4/4] https://t.co/L8U0r85xw3']",https://arxiv.org/abs/1808.09453,"In this paper we generalize the concept of an effective $\Delta m^2_{ee}$ for $\nu_e/\bar{\nu}_e$ disappearance experiments, which has been extensively used by the short baseline reactor experiments, to include the effects of propagation through matter for longer baseline $\nu_e/\bar{\nu}_e$ disappearance experiments. This generalization is a trivial, linear combination of the neutrino mass squared eigenvalues in matter and thus is not a simple extension of the usually vacuum expression, although, as it must, it reduces to the correct expression in the vacuum limit. We also demonstrated that the effective $\Delta m^2_{ee}$ in matter is very useful conceptually and numerically for understanding the form of the neutrino mass squared eigenstates in matter and hence for calculating the matter oscillation probabilities. Finally we analytically estimate the precision of this two-flavor approach and numerically verify that it is precise at the sub-percent level. ",The Effective $\Delta m^2_{ee}$ in Matter
14,1034789586772348929,382393,Ciro Cattuto,"['new #CIKM2018 paper with Edoardo Galimberti @FrancescoBonchi @alainbarrat Francesco Gullo: We introduce a time-aware core decomposition for temporal #networks , <LINK> #complexnetworks #DataScience <LINK>']",https://arxiv.org/abs/1808.09376,"When analyzing temporal networks, a fundamental task is the identification of dense structures (i.e., groups of vertices that exhibit a large number of links), together with their temporal span (i.e., the period of time for which the high density holds). We tackle this task by introducing a notion of temporal core decomposition where each core is associated with its span: we call such cores span-cores. As the total number of time intervals is quadratic in the size of the temporal domain $T$ under analysis, the total number of span-cores is quadratic in $|T|$ as well. Our first contribution is an algorithm that, by exploiting containment properties among span-cores, computes all the span-cores efficiently. Then, we focus on the problem of finding only the maximal span-cores, i.e., span-cores that are not dominated by any other span-core by both the coreness property and the span. We devise a very efficient algorithm that exploits theoretical findings on the maximality condition to directly compute the maximal ones without computing all span-cores. Experimentation on several real-world temporal networks confirms the efficiency and scalability of our methods. Applications on temporal networks, gathered by a proximity-sensing infrastructure recording face-to-face interactions in schools, highlight the relevance of the notion of (maximal) span-core in analyzing social dynamics and detecting/correcting anomalies in the data. ",Mining (maximal) span-cores from temporal networks
15,1034711166835216387,319518346,Jose Camacho-Collados,"['Humans are very good at distinguishing senses given different contexts but computers (including SotA models like #ELMo, sense embeddings...) still struggle with it. Check out our new dataset on this topic! #NLProc\n\nDataset: <LINK>\nPaper: <LINK> <LINK>', 'With @tpilehvar', '@yogarshi Thank you for the pointer, very interesting! And yes, not sure whether the conclusions of these studies are actually very positive... ðŸ˜• But at least they should encourage further research on modeling meaning in context, as there definitely seems to be room for improvement']",https://arxiv.org/abs/1808.09121,"By design, word embeddings are unable to model the dynamic nature of words' semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in this https URL ","WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive
  Meaning Representations"
16,1034607219164295169,5850692,Aaron Roth,"['A new paper with Sampath Kannan and Juba Ziani: ""The Downstream Effects of Affirmative Action"": <LINK> We study a two stage model with a school and employers. Suppose the school only see noisy signals about student types (exam scores). 1/5', 'A school can choose an admissions policy (mapping from scores to acceptance probability) for each group, and a grading policy (variance of grade, which is also a noisy signal). 2/5', 'Employers are Bayesians, condition on everything, and hire students if their posterior expectation is above a threshold. What goals can the school try and achieve via differential admissions policies if group type distributions differ? Here are two: 3/5', ""1) Equal opportunity: The probability of making it through the pipeline (admitted to school and then hired by employer) should be independent of group membership given type. 2) Group independent hiring: Employers' hiring rules should be independent of group membership. 4/5"", 'The punchline: In general, its not possible to achieve either one of these goals if the school gives out informative grades (finite, nonzero variance). But a sufficiently selective school can achieve both if it withholds grades. 5/5']",https://arxiv.org/abs/1808.09004,"We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy. ",Downstream Effects of Affirmative Action
17,1034454643139641345,2869101210,Jenn Wortman Vaughan,['Uncanny case of great-minds-think-alike:\n\nNew paper on The Disparate Effects of Strategic Manipulation w/ @uhlily @immorlica\n<LINK> \n\nWe study the social impact of classification in systems marked by inequality+potential for manipulation. Complementary analyses. <LINK>'],https://arxiv.org/abs/1808.08646,"When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed ""strategic manipulation,"" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to ""trick"" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off--even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's ""quality"" when agents' capacities to adaptively respond differ. ",The Disparate Effects of Strategic Manipulation
18,1034363142757908480,809336237194706944,Rico Sennrich,"['Why Self-Attention? New #EMNLP2018 paper doing targeted evaluation of different NMT architectures. Analysis shows that word sense disambiguation is a big strength of Transformer, but not long-distance dependencies, as speculated by Vaswani et al. (2017). <LINK>']",https://arxiv.org/abs/1808.08946,"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation. ","Why Self-Attention? A Targeted Evaluation of Neural Machine Translation
  Architectures"
19,1034270298621730816,296161364,Chris Power,"['Great to see new paper on the arXiv today by Carlton Baugh, @violegp, @CDPLagos, @abensonca et al. on modelling the atomic hydrogen content of dark matter halos using the P-Millennium Simulation coupled to the GALFORM semi-analytical model - <LINK>']",https://arxiv.org/abs/1808.08276,"We present recalibrations of the GALFORM semi-analytical model of galaxy formation in a new N-body simulation with the Planck cosmology. The Planck Millennium simulation uses more than 128 billion particles to resolve the matter distribution in a cube of $800$ Mpc on a side, which contains more than 77 million dark matter haloes with mass greater than $2.12 \times 10^{9} h^{-1} {\rm M_{\odot}}$ at the present day. Only minor changes to a very small number of model parameters are required in the recalibration. We present predictions for the atomic hydrogen content (HI) of dark matter halos, which is a key input into the calculation of the HI intensity mapping signal expected from the large-scale structure of the Universe. We find that the HI mass $-$ halo mass relation displays a clear break at the halo mass above which AGN heating suppresses gas cooling, $\approx 3 \times 10^{11} h^{-1} M_{\rm \odot}$. Below this halo mass, the HI content of haloes is dominated by the central galaxy; above this mass it is the combined HI content of satellites that prevails. We find that the HI mass - halo mass relation changes little with redshift up to $z=3$. The bias of HI sources shows a scale dependence that gets more pronounced with increasing redshift. ","Galaxy formation in the Planck Millennium: the atomic hydrogen content
  of dark matter halos"
20,1034267523997478912,368330699,Christian GagnÃ©,['New paper on arXiv: Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection <LINK>'],https://arxiv.org/abs/1808.08282,"Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries. ","Controlling Over-generalization and its Effect on Adversarial Examples
  Generation and Detection"
21,1034072573225648130,911474423412219904,Julien Tierny,['Need to track singularities in time-varying scalar data? Checkout our new algorithm based on #OptimalTransport and #TopologicalDataAnalysis! Exact and 2 orders of magnitude faster than the seminal Munkres algorithm.\nSee our new IEEE #LDAV 2018 paper: <LINK> <LINK>'],https://arxiv.org/abs/1808.05870,"This paper presents a robust and efficient method for tracking topological features in time-varying scalar data. Structures are tracked based on the optimal matching between persistence diagrams with respect to the Wasserstein metric. This fundamentally relies on solving the assignment problem, a special case of optimal transport, for all consecutive timesteps. Our approach relies on two main contributions. First, we revisit the seminal assignment algorithm by Kuhn and Munkres which we specifically adapt to the problem of matching persistence diagrams in an efficient way. Second, we propose an extension of the Wasserstein metric that significantly improves the geometrical stability of the matching of domain-embedded persistence pairs. We show that this geometrical lifting has the additional positive side-effect of improving the assignment matrix sparsity and therefore computing time. The global framework implements a coarse-grained parallelism by computing persistence diagrams and finding optimal matchings in parallel for every couple of consecutive timesteps. Critical trajectories are constructed by associating successively matched persistence pairs over time. Merging and splitting events are detected with a geometrical threshold in a post-processing stage. Extensive experiments on real-life datasets show that our matching approach is an order of magnitude faster than the seminal Munkres algorithm. Moreover, compared to a modern approximation method, our method provides competitive runtimes while yielding exact results. We demonstrate the utility of our global framework by extracting critical point trajectories from various simulated time-varying datasets and compare it to the existing methods based on associated overlaps of volumes. Robustness to noise and temporal resolution downsampling is empirically demonstrated. ",Lifted Wasserstein Matcher for Fast and Robust Topology Tracking
22,1034066310978826240,22652982,Fabrizio Riguzzi,"['New paper ""Using SWISH to realise interactive web based tutorials for logic based languages"" by J. Wielemaker, F. Riguzzi, R. Kowalski, T. Lager, F. Sadri, M. Calejo\n<LINK>']",https://arxiv.org/abs/1808.08042,"Programming environments have evolved from purely text based to using graphical user interfaces, and now we see a move towards web based interfaces, such as Jupyter. Web based interfaces allow for the creation of interactive documents that consist of text and programs, as well as their output. The output can be rendered using web technology as, e.g., text, tables, charts or graphs. This approach is particularly suitable for capturing data analysis workflows and creating interactive educational material. This article describes SWISH, a web front-end for Prolog that consists of a web server implemented in SWI-Prolog and a client web application written in JavaScript. SWISH provides a web server where multiple users can manipulate and run the same material, and it can be adapted to support Prolog extensions. In this paper we describe the architecture of SWISH, and describe two case studies of extensions of Prolog, namely Probabilistic Logic Programming (PLP) and Logic Production System (LPS), which have used SWISH to provide tutorial sites. ","Using SWISH to realise interactive web based tutorials for logic based
  languages"
23,1033894762858733569,140770306,Masafumi Oizumi,"['Our new paper ""Fisher Information and Natural Gradient Learning of Random Deep Networks"" is available on arXiv. We showed that in random deep networks, the Fisher information matrix is unit-wise block diagonal, which speeds up the natural gradient. <LINK>']",https://arxiv.org/abs/1808.07172,"A deep neural network is a hierarchical nonlinear model transforming input signals to output signals. Its input-output relation is considered to be stochastic, being described for a given input by a parameterized conditional probability distribution of outputs. The space of parameters consisting of weights and biases is a Riemannian manifold, where the metric is defined by the Fisher information matrix. The natural gradient method uses the steepest descent direction in a Riemannian manifold, so it is effective in learning, avoiding plateaus. It requires inversion of the Fisher information matrix, however, which is practically impossible when the matrix has a huge number of dimensions. Many methods for approximating the natural gradient have therefore been introduced. The present paper uses statistical neurodynamical method to reveal the properties of the Fisher information matrix in a net of random connections under the mean field approximation. We prove that the Fisher information matrix is unit-wise block diagonal supplemented by small order terms of off-block-diagonal elements, which provides a justification for the quasi-diagonal natural gradient method by Y. Ollivier. A unitwise block-diagonal Fisher metrix reduces to the tensor product of the Fisher information matrices of single units. We further prove that the Fisher information matrix of a single unit has a simple reduced form, a sum of a diagonal matrix and a rank 2 matrix of weight-bias correlations. We obtain the inverse of Fisher information explicitly. We then have an explicit form of the natural gradient, without relying on the numerical matrix inversion, which drastically speeds up stochastic gradient learning. ",Fisher Information and Natural Gradient Learning of Random Deep Networks
24,1033875904252932096,1033824122818555904,Michael Tucker,"['New paper on the arxiv today: ""ASASSN-18ey: The Rise of a New Black-Hole X-ray Binary"" (<LINK>) detailing our recent discovery of ASASSN-18ey. I made a 4min ""Aloha Brief"" video explaining the main parts of the paper: <LINK>']",https://arxiv.org/abs/1808.07875,"We present the discovery of ASASSN-18ey (MAXI J1820+070), a new black hole low-mass X-ray binary discovered by the All-Sky Automated Survey for SuperNovae (ASAS-SN). A week after ASAS-SN discovered ASASSN-18ey as an optical transient, it was detected as an X-ray transient by MAXI/GCS. Here, we analyze ASAS-SN and Asteroid Terrestrial-impact Last Alert System (ATLAS) pre-outburst optical light curves, finding evidence of intrinsic variability for several years prior to the outburst. While there was no long-term rise leading to outburst, as has been seen in several other systems, the start of the outburst in the optical preceded that in the X-rays by $7.20\pm0.97~\rm days$. We analyze the spectroscopic evolution of ASASSN-18ey from pre-maximum to $> 100~\rm days$ post-maximum. The spectra of ASASSN-18ey exhibit broad, asymmetric, double-peaked H$\alpha$ emission. The Bowen blend ($\lambda \approx 4650$\AA) in the post-maximum spectra shows highly variable double-peaked profiles, likely arising from irradiation of the companion by the accretion disk, typical of low-mass X-ray binaries. The optical and X-ray luminosities of ASASSN-18ey are consistent with black hole low-mass X-ray binaries, both in outburst and quiescence. ",ASASSN-18ey: The Rise of a New Black-Hole X-ray Binary
25,1032977805649371136,185910194,Graham Neubig,"['New #EMNLP2018 paper proposing 1) a mathematical framework for describing data augmentation methods for text 2) a super-simple augmentation method for NMT: replace random source/target words. Nice results on several tasks vs. strong baselines! <LINK> <LINK>', 'Great work by authors @cindyxinyiwang, @hieupham789, and Zihang Dai!']",https://arxiv.org/abs/1808.07512,"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix. ","SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine
  Translation"
26,1032972769678565376,2279676642,JJ Hermes,"[""A few weeks ago at #CoolStars20 I teased a new paper by Steven Parsons w/ extremely precise (&lt;2%) determinations of M dwarf masses and radii, from eclipsing WD+dM binaries. Steven's paper is now accepted and on arXiv: <LINK>. Check it out! <LINK>""]",https://arxiv.org/abs/1808.07780,"M dwarfs are prime targets in the hunt for habitable worlds around other stars. This is due to their abundance as well as their small radii and low masses and temperatures, which facilitate the detection of temperate, rocky planets in orbit around them. However, the fundamental properties of M dwarfs are difficult to constrain, often limiting our ability to characterise the planets they host. Here we test several theoretical relationships for M dwarfs by measuring 23 high precision, model-independent masses and radii for M dwarfs in binaries with white dwarfs. We find a large scatter in the radii of these low-mass stars, with 25 per cent having radii consistent with theoretical models while the rest are up to 12 per cent over-inflated. This scatter is seen in both partially- and fully-convective M dwarfs. No clear trend is seen between the over-inflation and age or metallicity, but there are indications that the radii of slowly rotating M dwarfs are more consistent with predictions, albeit with a similar amount of scatter in the measurements compared to more rapidly rotating M dwarfs. The sample of M dwarfs in close binaries with white dwarfs appears indistinguishable from other M dwarf samples, implying that common envelope evolution has a negligible impact on their structure. We conclude that theoretical and empirical mass-radius relationships lack the precision and accuracy required to measure the fundamental parameters of M dwarfs well enough to determine the internal structure and bulk composition of the planets they host. ",The scatter of the M dwarf mass-radius relationship
27,1032749470868557824,809864,Andrew Jaffe,"['Very pleased with the new paper from my student, our collaborator @Dr_CMingarelli, and me: On the Amplitude and Stokes Parameters of a Stochastic Gravitational-Wave Background â€” lots of good (and useful!) math and physics.\n\n<LINK>']",https://arxiv.org/abs/1808.05920,"The direct detection of gravitational waves has provided new opportunities for studying the universe, but also new challenges, such as the detection and characterization of stochastic gravitational-wave backgrounds at different gravitational-wave frequencies. In this paper we examine two different methods for their description, one based on the amplitude of a gravitational-wave signal and one on its Stokes parameters. We find that the Stokes parameters are able to describe anisotropic and correlated backgrounds, whereas the usual power spectra of the amplitudes cannot -- i.e. the Stokes spectra are sensitive to properties such as the spatial distribution of the gravitational-wave sources in a realistic backgrounds. ","On the Amplitude and Stokes Parameters of a Stochastic
  Gravitational-Wave Background"
28,1032659716873306112,961972787077373952,Aykut Erdem,"['In a new paper w/ @lvntkrcn, Zeynep Akata (UvA) and @erkuterdem, we manipulate transient attributes of natural scenes via hallucination. And, of course, all this is powered by GANs!\n\n<LINK>\n<LINK>\n\n@goodfellow_ian @phillip_isola @liu_mingyu <LINK>']",https://arxiv.org/abs/1808.07413,"In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network which can hallucinate images of a scene as if they were taken at a different season (e.g. during winter), weather condition (e.g. in a cloudy day) or time of the day (e.g. at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrate the effectiveness of our approach against the competing methods. ",Manipulating Attributes of Natural Scenes via Hallucination
29,1032599096547323905,191872322,Jan Claes,"['New paper with Francis Bru: ""The perceived quality of process discovery tools"" <LINK>, published at <LINK>. #processmining @fluxicon @Celonis @minit_io @SoftwareAG @myinvenio @QPR_Software @ProcessGold @apromore_bpm @Fujitsu_Global @Appian <LINK>']",https://arxiv.org/abs/1808.06475,"Process discovery has seen a rise in popularity in the last decade for both researchers and businesses. Recent developments mainly focused on the power and the functionalities of the discovery algorithm. While continuous improvement of these functional aspects is very important, non-functional aspects such as visualization and usability are often overlooked. However, these aspects are considered valuable for end-users and play an important part in the experience of these end-users when working with a process discovery tool. A questionnaire has been sent out to give end-users the opportunity to voice their opinion on available process discovery tools and about the state of process discovery as a domain in general. The results of 66 respondents are presented and compared with the answers of 63 respondents that were contacted through one particular software vendor's employee and customer base (i.e., Celonis). ",The perceived quality of process discovery tools
30,1032578845147320321,523241142,Juste Raimbault,"['New paper out in the field of #Scientometrics and #QuantitativeEpistemology with @ClementineCttn, @pumain1, PO. Chasset, A. Banos, H. Commenges, using #BigData, #ComplexNetworks  and #TextMining to foster #OpenScience, studying @RevueCybergeo <LINK>']",https://arxiv.org/abs/1808.07282,"Bibliometrics have become commonplace and widely used by authors and journals to monitor, to evaluate and to identify their readership in an ever-increasingly publishing scientific world. With this contribution, we aim to investigate the semantic proximities and evolution of the papers published in the online journal Cybergeo since its creation in 1996. We propose a dedicated interactive application that compares three strategies for building semantic networks, using keywords (self-declared themes), citations (areas of research using the papers published in Cybergeo) and full-texts (themes derived from the words used in writing). We interpret these networks and semantic proximities with respect to their temporal evolution as well as to their spatial expressions, by considering the countries studied in the papers under inquiry (Cybergeo being a journal of geography, most articles refer to a well-defined spatial envelope). Finally, we compare the three methods and conclude that their complementarity can help go beyond simple statistics to better understand the epistemological evolution of a scientific community and the readership target of the journal. ",Empowering open science with reflexive and spatialised indicators
31,1032504981469913088,30989098,Karin Sandstrom,"[""Interested in the dust-to-gas ratio and its dependence on metallicity? Or maybe dust spectral energy distribution fitting? Check out the new paper by my grad student I-Da Chiang! <LINK>\n\nAnd wish him good luck on his candidacy exam tomorrow am :) (it'll be great)"", 'Update: exam was indeed great! On to the next papers of the thesis!']",https://arxiv.org/abs/1808.07164,"The dust-to-metals ratio describes the fraction of the heavy elements contained in dust grains, and its variation provides key insights into the life cycle of dust. We measure the dust-to-metals ratio in M101, a nearby galaxy with a radial metallicity (Z) gradient spanning $\sim$1 dex. We fit the dust spectral energy distribution from 100 to 500 $\mu m$ with five variants of the modified blackbody dust emission model in which we vary the temperature distribution and how emissivity depends on wavelength. Among them, the model with a single temperature blackbody modified by a broken power-law emissivity gives the statistically best fit and physically most plausible results. Using these results, we show that the dust-to-gas ratio is proportional to $\rm Z^{1.7}$. This implies that the dust-to-metals ratio is not constant in M101, but decreases as a function of radius, equivalent to a lower fraction of metals trapped in dust at low metallicity (large radius). The dust-to-metals ratio in M101 remains at or above what would be predicted by the minimum depletion level of metals observed in the Milky Way. Our current knowledge of metallicity-dependent CO-to-H$_2$ conversion factor suggests that variations in the conversion factor cannot be responsible for the dust-to-metals ratio trends we observe. This change of dust-to-metals ratio is significantly correlated with molecular hydrogen fraction, which suggests that the accretion of gas phase metals onto existing dust grains could be a mechanism contributing to a variable dust-to-metals ratio. ",The Spatially Resolved Dust-to-Metals Ratio in M101
32,1032431626431746049,65137727,Jonathan Mboyo Esole,"['My new paper on ""Characteristic numbers of elliptic fibrations with non-trivial Mordell-Weil groups"" in collaboration with Monica Kang is out! I would like to dedicate it to my students from @MalaikaDRC as they get ready for a new year. @NextEinsteinFor  <LINK> <LINK>']",https://arxiv.org/abs/1808.07054,"We compute characteristic numbers of elliptically fibered fourfolds with multisections or non-trivial Mordell-Weil groups. We first consider the models of type E$_{9-d}$ with $d=1,2,3,4$ whose generic fibers are normal elliptic curves of degree $d$. We then analyze the characteristic numbers of the $Q_7$-model, which provides a smooth model for elliptic fibrations of rank one and generalizes the E$_5$, E$_6$, and E$_7$-models. Finally, we examine the characteristic numbers of $G$-models with $G=\text{SO}(n)$ with $n=3,4,5,6$ and $G=\text{PSU}(3)$ whose Mordell-Weil groups are respectively $\mathbb{Z}/2\mathbb{Z}$ and $\mathbb{Z}/3 \mathbb{Z}$. In each case, we compute the Chern and Pontryagin numbers, the Euler characteristic, the holomorphic genera, the Todd-genus, the L-genus, the A-genus, and the eight-form curvature invariant from M-theory. ","Characteristic numbers of elliptic fibrations with non-trivial
  Mordell-Weil groups"
33,1032325033841094656,414014820,Adam Trischler,['Our new paper â€œLearning deep representations by mutual information estimation and maximizationâ€ is now on arXiv: <LINK>'],https://arxiv.org/abs/1808.06670,"In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals. ","Learning deep representations by mutual information estimation and
  maximization"
34,1032248080735842304,835680883,Jason Wright,"['New paper from @nexssinfo/@ASU/@PSUScience postdoc @ehlbodman (working with me &amp; @Deschscoveries), who has exciting news about @NASAWebb: \n\nIt can directly measure the compositions of rocky planet *interiors*! \n\n<LINK>\n\nNow on the arXiv:\n\n<LINK>']",https://arxiv.org/abs/1808.07043,"Disintegrating planets allow for the unique opportunity to study the composition of the interiors of small, hot, rocky exoplanets because the interior is evaporating and that material is condensing into dust, which is being blown away and then transiting the star. Their transit signal is dominated by dusty effluents forming a comet-like tail trailing the host planet (or leading it, in the case of K2-22b), making these good candidates for transmission spectroscopy. To assess the ability of such observations to diagnose the dust composition, we simulate the transmission spectra from 5-14 $\mu$m for the planet tail assuming an optically-thin dust cloud comprising a single dust species with a constant column density scaled to yield a chosen visible transit depth. We find that silicate resonant features near 10 $\mu$m can produce transit depths that are at least as large as those in the visible. For the average transit depth of 0.55% in the Kepler band for K2-22b, the features in the transmission spectra can be as large as 1%, which is detectable with the JWST MIRI low-resolution spectrograph in a single transit. The detectability of compositional features is easier with an average grain size of 1 $\mu$m despite features being more prominent with smaller grain sizes. We find most features are still detectable for transit depths of ~0.3% in the visible range. If more disintegrating planets are found with future missions such as the space telescope TESS, follow-up observations with JWST can explore the range of planetary compositions. ","Inferring the Composition of Disintegrating Planet Interiors from Dust
  Tails with Future James Webb Space Telescope Observations"
35,1032123573131984896,18850305,Zachary Lipton,"['In new paper w Aditya Siddhant, we ran a (more) realistic eval of deep active learning 4 NLP: 3 tasks w 2+ datasets, 2+ models, 4 acquisition fns, &amp; multiple runs (for each), w/o typical target leaks. Results are surprisingly positive. #EMNLP2018 <LINK> @LTIatCMU', '@innerproduct @LTIatCMU There are loads of easily accessible datasets out there: https://t.co/kTZkXh0MdN']",https://arxiv.org/abs/1808.05697,"Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, an active learner has no opportunity to compare models and acquisition functions. This paper provides a large scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling. ","Deep Bayesian Active Learning for Natural Language Processing: Results
  of a Large-Scale Empirical Study"
36,1032098785646731264,15520108,Rudi Podgornik å²³å„’è¿ª,['Our new review paper on recent developments in Charge Regulation with Fixed and Mobile Charges. <LINK> <LINK>'],https://arxiv.org/abs/1808.06771,"Uncompensated charges do not occur in Nature and any local charge should be a result of charge separation. Dissociable chemical groups at interfaces in contact with ions in solution, whose chemical equilibrium depends both on short-range non-electrostatic and long-range electrostatic interactions, are the physical basis of this charge separation, known as charge regulation phenomena. The charged groups can be either fixed and immobile, as in the case of solvent-exposed solid substrate and soft bounding surfaces, (e.g., molecularly smooth mica surfaces and soft phospholipid membranes), or free and mobile, as in the case of charged macro-ions, (e.g., protein or other biomolecules). Here, we review the mean-field formalism used to describe both cases, with a focus on recent advances in the modeling of mobile charge-regulated macro-ions in an ionic solution. The general form of the screening length in such a solution is derived, and is shown to combine the concept of intrinsic capacitance, introduced by Lund and J\""{o}nsson, and bulk capacitance, resulting from the mobility of small ions and macro-ions. The advantages and disadvantages of different formulations, such as the cell model vs. the collective approach, are discussed, along with several suggestions for future experiments and modeling. ",Charge Regulation with Fixed and Mobile Charges
37,1031875253029556225,14104309,Darren Price,"['In a new paper released today, we present the first global analysis of CP-sensitive measurements of the #Higgs sector from #LHC data, highlight specific new measurements that are needed to resolve ambiguities, and study future prospects: <LINK> <LINK>', 'The #Higgs boson may play a special role in the origin of the matter anti-matter asymmetry of the Universe (where did all the anti-matter go?). \nEven without knowing the details, we can study #Higgs data from the #LHC to constrain models that could explain this asymmetry ðŸ”¬']",https://arxiv.org/abs/1808.06577,"CP-violation in the Higgs sector remains a possible source of the baryon asymmetry of the universe. Recent differential measurements of signed angular distributions in Higgs boson production provide a general experimental probe of the CP structure of Higgs boson interactions. We interpret these measurements using the Standard Model Effective Field Theory and show that they do not distinguish the various CP-violating operators that couple the Higgs and gauge fields. However, the constraints can be sharpened by measuring additional CP-sensitive observables and exploiting phase-space-dependent effects. Using these observables, we demonstrate that perturbatively meaningful constraints on CP-violating operators can be obtained at the LHC with luminosities of ${\cal{O}}$(100/fb). Our results provide a roadmap to a global Higgs boson coupling analysis that includes CP-violating effects. ",Angles on CP-violation in Higgs boson interactions
38,1031762606749626368,823277120944242689,Will Kinney,['New paper.\n<LINK> <LINK>'],https://arxiv.org/abs/1808.06424,"We consider single-field inflation in light of string-motivated ""swampland"" conjectures suggesting that effective scalar field theories with a consistent UV completion must have field excursion $\Delta \phi \lesssim M_{\rm Pl}$, in combination with a sufficiently steep potential, $M_{\rm Pl} V_\phi/V \gtrsim {\cal O}(1)$. Here, we show that the swampland conjectures are inconsistent with existing observational constraints on single-field inflation. Focusing on the observationally favoured class of concave potentials, we map the allowed swampland region onto the $n_S$-$r$ ""zoo plot"" of inflationary models, and find that consistency with the Planck satellite and BICEP2/Keck Array requires $M_{\rm Pl} V_\phi/V \lesssim 0.1$ and $-0.02 \lesssim M_{\rm Pl}^2 V_{\phi\phi}/V < 0$, in strong tension with swampland conjectures. Extension to non-canonical models such as DBI Inflation does not significantly weaken the bound. ","The zoo plot meets the swampland: mutual (in)consistency of single-field
  inflation, string conjectures, and cosmological data"
39,1031473861466968064,481539448,Richard Alexander,"['New paper, led by @bec_nealon, investigating how newly-formed planets can warp their parent protoplanetary discs. Bec ran a large suite of calculations to look at how discs respond to planets on different masses and at different (small) inclinations.\n<LINK> <LINK>', 'Very short summary is that planets with only modest inclinations (few degrees) can produce potentially observable tilts/warps in the disc, and that the disc size (i.e. outer radius) is very important in determining the amplitude of any misalignment between the inner &amp; outer disc.']",https://arxiv.org/abs/1808.05693,"Recent observations of several protoplanetary discs have found evidence of departures from flat, circular motion in the inner regions of the disc. One possible explanation for these observations is a disc warp, which could be induced by a planet on a misaligned orbit. We present three-dimensional numerical simulations of the tidal interaction between a protoplanetary disc and a misaligned planet. For low planet masses we show that our simulations accurately model the evolution of inclined planet orbit (up to moderate inclinations). For a planet massive enough to carve a gap, the disc is separated into two components and the gas interior and exterior to the planet orbit evolve separately, forming an inner and outer disc. Due to the inclination of the planet, a warp develops across the planet orbit such that there is a relative tilt and twist between these discs. We show that when other parameters are held constant, the relative inclination that develops between the inner and outer disc depends on the outer radius of the total disc modelled. For a given disc mass, our results suggest that the observational relevance of the warp depends more strongly on the mass of the planet rather than the inclination of the orbit. ",Warping a protoplanetary disc with a planet on an inclined orbit
40,1030424284106317824,2766925212,Andrew Childs,"['New paper with Tran, @imagAndrewGuo, @yuansu_umd, Garrison, @zeldredge, Foss-Feig, Gorshkov: Lieb-Robinson bounds give quantum simulation, quantum simulation gives Lieb-Robinson bounds <LINK>']",http://arxiv.org/abs/1808.05225,"The propagation of information in non-relativistic quantum systems obeys a speed limit known as a Lieb-Robinson bound. We derive a new Lieb-Robinson bound for systems with interactions that decay with distance $r$ as a power law, $1/r^\alpha$. The bound implies an effective light cone tighter than all previous bounds. Our approach is based on a technique for approximating the time evolution of a system, which was first introduced as part of a quantum simulation algorithm by Haah et al., FOCS'18. To bound the error of the approximation, we use a known Lieb-Robinson bound that is weaker than the bound we establish. This result brings the analysis full circle, suggesting a deep connection between Lieb-Robinson bounds and digital quantum simulation. In addition to the new Lieb-Robinson bound, our analysis also gives an error bound for the Haah et al. quantum simulation algorithm when used to simulate power-law decaying interactions. In particular, we show that the gate count of the algorithm scales with the system size better than existing algorithms when $\alpha>3D$ (where $D$ is the number of dimensions). ",Locality and digital quantum simulation of power-law interactions
41,1030421802714058754,864555701783474179,julesh,"[""New preprint: Limits of bimorphic lenses\n\nAs a paper this isn't really finished, but I wanted to push out a preprint so I could switch back to working on some other stuff.\n<LINK>"", 'cc @jer_gib\nAlso cc @valeriadepaiva -- I conjecture this can be generalised to saying intuitionistic dialectica categories are complete under reasonable hypotheses']",https://arxiv.org/abs/1808.05545,"Bimorphic lenses are a simplification of polymorphic lenses that (like polymorphic lenses) have a type defined by 4 parameters, but which are defined in a monomorphic type system (i.e. an ordinary category with finite products). We show that the category of bimorphic lenses is complete when the base category is complete, cocomplete and cartesian closed, and so symmetric bimorphic lenses can be defined as spans of ordinary bimorphic lenses. This is in contrast to monomorphic lenses, which do not have pullbacks, and for which the category of spans can be defined in an ad-hoc way only when the lenses satisfy a certain axiom (the put-get law). This is a step towards a theory of symmetric polymorphic lenses. Bimorphic lenses additionally play an essential role in compositional game theory, and spans of bimorphic lenses are a step towards a compact closed category of open games. ",Limits of bimorphic lenses
42,1030065821777911809,1027201792139042816,Forrest W. Crawford,"['New review paper with Si Cheng (now @UWBiostat), Daniel Eck on models, methods, asymptotics for estimating the size of a hidden set, e.g. risk groups, hard-to-reach populations in #epidemiology, #demography.  \n<LINK>']",https://arxiv.org/abs/1808.04753,"A finite set is ""hidden"" if its elements are not directly enumerable or if its size cannot be ascertained via a deterministic query. In public health, epidemiology, demography, ecology and intelligence analysis, researchers have developed a wide variety of indirect statistical approaches, under different models for sampling and observation, for estimating the size of a hidden set. Some methods make use of random sampling with known or estimable sampling probabilities, and others make structural assumptions about relationships (e.g. ordering or network information) between the elements that comprise the hidden set. In this review, we describe models and methods for learning about the size of a hidden finite set, with special attention to asymptotic properties of estimators. We study the properties of these methods under two asymptotic regimes, ""infill"" in which the number of fixed-size samples increases, but the population size remains constant, and ""outfill"" in which the sample size and population size grow together. Statistical properties under these two regimes can be dramatically different. ","Estimating the size of a hidden finite set: large-sample behavior of
  estimators"
43,1029996174789562370,129802464,Niall Deacon,"['New paper out today. How to distinguish ultracool dwarfs from background reddened objects by combining data from @ESA_Euclid or @NASAWFIRST with ground-based data <LINK>', '@ESA_Euclid will mostly be at high galactic latitude. However if the mission got extended past its current proposed lifetime there would be a great science case for a deep IR plane survey', 'Also note the water imaging technique is based on one invented by @KatelynAllers &amp; Mike Liu', '@dalcashdvinsky Not used young templates as there arenâ€™t many in the SpeX archive. I wouldnâ€™t expect there to be much change as Allers &amp; Liu 2013 show that water indices at the the wavelengths the colour terms sample are fairly independent of gravity', '@dalcashdvinsky Yeah, the technique doesnâ€™t work well for things that early. Would need spectra of things like PSO318, WISE0047 or 2MASS0355', '@dalcashdvinsky OK, here you go.\n\nThe two objects that seem to be the best distinguished from the reddening line are Usco24 and Usco2 which are the two latest-type objects in your sample. Usco, 5, 11 &amp; 12 line close to the reddening line https://t.co/yBHmXrk3OJ', '@dalcashdvinsky @ESA_Euclid @NASAWFIRST Yeah, given Euclid wonâ€™t survey within 30-40 degrees of the plane in the main survey, anything at lower latitudes at some point would be great.']",https://arxiv.org/abs/1808.04828,"The next decade will see two large-scale space-based near-infrared surveys, Euclid and WFIRST. This paper shows that the subtle differences between the filters proposed for these surveys and those from ground-based photometric systems will produce a ground-space colour term that is dependent on water absorption in the spectra of astronomical objects. This colour term can be used to identify free-floating planets in star forming regions, mimicking a successful ground-based technique that uses a filter sensitive to water absorption. This paper shows that this colour term is an effective discriminant between reddened background stars and ultracool dwarfs. This represents just one science justification for a Galactic Plane survey in the event of an extension to the Euclid mission beyond its original timeframe. ","Detecting free-floating planets using water-depend colour terms in the
  next generation of infrared space-based surveys"
44,1029944868066316288,18850305,Zachary Lipton,"['New paper with @dkaushik96 is up: ***How Much Reading Does Reading Comprehension Require? A Critical  Investigation of Popular Benchmarks*** We establish sensible RC baselines, finding question- and passage-only models perform surprisingly well. \n<LINK>', ""Another weird finding. On the CBT task, one predicts the answer given a question and a 20-sentence passage. Turns out you may only need the last sentence. So perhaps fancy architectures for attending over the passage aren't justified by this task."", '@AlecRad Right the actual point is more subtle. The particular proposed architectures that do no better than a model that sees only the last sentence might not be justified, but there may exist some that could be.', '@AlecRad Such a demonstration would benefit from presenting the results alongside the recommended baselines.', ""@samfin55 @AlecRad There's a fundamental error here. Prediction on holdout data != comprehension. This a point we avoided biting off partly b/c I already rang that bell here: https://t.co/xrifOMQDXt. That aside, depends on the task. For some, they are at 100%, for others possibly near noise ceiling"", '@samfin55 @AlecRad I\'m saying that characterizing performing well at ***so-called ""reading-comprehension"" tasks*** (for any level of performance) as ""achieving anything that could be fairly termed comprehension"" is an error. That\'s why I advocate changing the name of the task.', '@samfin55 @AlecRad Getting buy-in might be tough ""passage-based answer prediction"" doesn\'t quite have the same ring.', ""@samfin55 @AlecRad That's the clearly intended meaning of the title. And I think if you actually read the paper it's hard to come away with any other interpretation."", ""@samfin55 @AlecRad No worries! I don't find the discussion mean-spirited or confrontational. I think you're highlighting an important point that when we use confusing terminology, even in the context of criticizing it, we can sow more confusion. Thanks for the comments and happy to discuss more.""]",https://arxiv.org/abs/1808.04926,"Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On $14$ out of $20$ bAbI tasks, passage-only models achieve greater than $50\%$ accuracy, sometimes matching the full model. Interestingly, while CBT provides $20$-sentence stories only the last is needed for comparably accurate prediction. By comparison, SQuAD and CNN appear better-constructed. ","How Much Reading Does Reading Comprehension Require? A Critical
  Investigation of Popular Benchmarks"
45,1029906021047664640,2816636344,Anirudh Goyal,"['Our new paper, which generalizes Equillibrium propagation to general vector field dynamics is finally out! Joint work with Benjamin Scellier, Jonathan Binas, Thomas Mesnard and Yoshua Bengio! <LINK>.']",https://arxiv.org/abs/1808.04873,"The biological plausibility of the backpropagation algorithm has long been doubted by neuroscientists. Two major reasons are that neurons would need to send two different types of signal in the forward and backward phases, and that pairs of neurons would need to communicate through symmetric bidirectional connections. We present a simple two-phase learning procedure for fixed point recurrent networks that addresses both these issues. In our model, neurons perform leaky integration and synaptic weights are updated through a local mechanism. Our learning method generalizes Equilibrium Propagation to vector field dynamics, relaxing the requirement of an energy function. As a consequence of this generalization, the algorithm does not compute the true gradient of the objective function, but rather approximates it at a precision which is proven to be directly related to the degree of symmetry of the feedforward and feedback weights. We show experimentally that our algorithm optimizes the objective function. ",Generalization of Equilibrium Propagation to Vector Field Dynamics
46,1029900172157845505,149627584,Catherine Olsson,"['Our paper ""Skill Rating for Generative Models"" is now up! <LINK> \n\ntl;dr: A new idea &amp; proof-of-concept for evaluating generative models. Train a bunch of GANs. Have the generators ""play against"" all the discriminator snapshots. Rate them like chess players. 1/n', 'I don\'t think this approach is ready to replace FID yet, but I do think it\'s\n1) a new approach to the GAN eval problem &amp; worth exploring, and\n2) useful as a complementary method, e.g. as a ""second opinion"" alongside others, or in domains that don\'t have an Inception embedding 2/n', ""NIPS reviewers pointed out we don't explain *why* tournament evaluation should work. And in a sense it shouldn't, bc the discriminators are doing out-of-distribution judgment - why should that be okay? But (preliminarily, empirically) it indeed does seem to work anyway. 3/n"", 'I borrowed the ""skill rating"" idea from my work on Dota2 at @OpenAI, where I built a system to skill rate 1v1 bots (https://t.co/wo8NHmkvMw). When I applied to work at Brain, @goodfellow_ian said ""Bot eval sounds a lot like the GAN eval problem, could we skill rate GANs too?"" 4/n', ""To wrap up here - there's a lot more work to be done, to figure out what's going on with these patterns, and whether this could be made a really watertight approach to GAN evaluation (which is a big open problem!) If you're interested in extending this and want to chat, LMK 5/n"", ""Finally, yes, code forthcoming, ping me if you want any part of it sooner. I pasted the architectures in the appendix (thanks @karpathy for the tip: https://t.co/12LteSLS7k) but there's more code to run the battles and make the graphs, and I can release the checkpoints too. 6/n"", 'Thanks to my coauthors @suryabhupa @nottombrown @gstsdn @goodfellow_ian! /fin', '@bensprecher Seems worth trying! Could be combined with something like https://t.co/d4FBwA5vQz', '@JeffDean @suryabhupa @nottombrown @gstsdn @goodfellow_ian ðŸ˜‚']",http://arxiv.org/abs/1808.04888,"We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages. ",Skill Rating for Generative Models
47,1029723584942329857,17797390,Sharad Goel,"[""In a new review paper, @scorbettdavies and I summarize the recent literature on fair machine learning, describe critical limitations in the foundation of fair ML, and identify promising directions to advance the field. We'd love to hear your comments! <LINK>""]",https://arxiv.org/abs/1808.00023,"The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes---like race, gender, and their proxies---are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area. ","The Measure and Mismeasure of Fairness: A Critical Review of Fair
  Machine Learning"
48,1029621156486242304,880494299816878081,Akis Konstantinoudis,['Our new paper comparing an attractive class of continuous domain models with the common BYM choice in Spatial Epidemiology <LINK>  @ISPMBern @RPanczak'],https://arxiv.org/abs/1808.04765,"The main goal of disease mapping is to estimate disease risk and identify high-risk areas. Such analyses are hampered by the limited geographical resolution of the available data. Typically the available data are counts per spatial unit and the common approach is the Besag--York--Molli{\'e} (BYM) model. When precise geocodes are available, it is more natural to use Log-Gaussian Cox processes (LGCPs). In a simulation study mimicking childhood leukaemia incidence using actual residential locations of all children in the canton of Z\""urich, Switzerland, we compare the ability of these models to recover risk surfaces and identify high-risk areas. We then apply both approaches to actual data on childhood leukaemia incidence in the canton of Z\""urich during 1985-2015. We found that LGCPs outperform BYM models in almost all scenarios considered. Our findings suggest that there are important gains to be made from the use of LGCPs in spatial epidemiology. ",Discrete versus continuous domain models for disease mapping
49,1029481281766260736,922847904058011649,Romy RodrÃ­guez,['My new paper is on the arxiv today! In this work we derive the physical properties of a small sample of M dwarfs from the K2 mission from their NIR spectra and infer the properties of their transiting exoplanets.   <LINK>'],https://arxiv.org/abs/1808.03652,"We present moderate resolution near-infrared spectra in $H, J$ and $K$ band of M dwarf hosts to candidate transiting exoplanets discovered by NASA's K2 mission. We employ known empirical relationships between spectral features and physical stellar properties to measure the effective temperature, radius, metallicity, and luminosity of our sample. Out of an initial sample of 56 late-type stars in K2, we identify 35 objects as M dwarfs. For that sub-sample, we derive temperatures ranging from $2,870$ to $4,187$ K, radii of $0.09-0.83$ $R_{\odot}$, luminosities of $-2.67<log L/L_{\odot}<-0.67$ and [Fe/H] metallicities between $-0.49$ and $0.83$ dex. We then employ the stellar properties derived from spectra, in tandem with the K2 lightcurves, to characterize their planets. We report 33 exoplanet candidates with orbital periods ranging from 0.19 to 21.16 days, and median radii and equilibrium temperatures of 2.3 $R_{\oplus}$ and 986 K, respectively. Using planet mass-radius relationships from the literature, we identify 7 exoplanets as potentially rocky, although we conclude that probably none reside in the habitable zone of their parent stars. ","Characterization of Low Mass K2 Planet Hosts Using Near-Infrared
  Spectroscopy"
50,1029379403523022854,20810416,Dr. Roman Yampolskiy,"['New paper, with MichaÃ«l Trazzi, on the importance of Artificial Stupidity in passing the Turing Test and making Safer AI. \n<LINK> <LINK>', '@MichaelTrazzi Faking?']",https://arxiv.org/abs/1808.03644,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI. ",Building Safer AGI by introducing Artificial Stupidity
51,1029250786751066112,2309687984,Arno Solin,"['New paper with @scortes480 and Juho: ""Deep Learning Based Speed Estimation for Constraining Strapdown Inertial  Navigation on Smartphones"". To be presented at #MLSP2018 in a couple of weeks. Paper <LINK>, codes <LINK> <LINK>']",https://arxiv.org/abs/1808.03485,"Strapdown inertial navigation systems are sensitive to the quality of the data provided by the accelerometer and gyroscope. Low-grade IMUs in handheld smart-devices pose a problem for inertial odometry on these devices. We propose a scheme for constraining the inertial odometry problem by complementing non-linear state estimation by a CNN-based deep-learning model for inferring the momentary speed based on a window of IMU samples. We show the feasibility of the model using a wide range of data from an iPhone, and present proof-of-concept results for how the model can be combined with an inertial navigation system for three-dimensional inertial navigation. ","Deep Learning Based Speed Estimation for Constraining Strapdown Inertial
  Navigation on Smartphones"
52,1029220406861946880,846074077571772416,Nicolas Cimerman,"['You like your disks eccentric and your planets in resonance? Then please check out our new paper on the Laplace resonance around GJ876, that was just accepted by A&amp;A!\n<LINK>']",https://arxiv.org/abs/1808.04223,"Orbital mean motion resonances in planetary systems originate from dissipative processes in disk-planet interactions that lead to orbital migration. In multi-planet systems that host giant planets, the perturbation of the protoplanetary disk strongly affects the migration of companion planets. By studying the well-characterized resonant planetary system around GJ 876 we aim to explore which effects shape disk-driven migration in such a multi-planet system to form resonant chains. We modelled the orbital migration of three planets embedded in a protoplanetary disk using two-dimensional locally isothermal hydrodynamical simulations. We performed a parameter study by varying the disk thickness, $\alpha$ viscosity, mass as well as the initial position of the planets. Moreover, we have analysed and compared simulations with various boundary conditions at the disk's inner rim. We find that due to the high masses of the giant planets, substantial eccentricity can be excited in the disk. This results in large variations of the torque acting on the outer lower-mass planet, which we attribute to a shift of Lindblad and corotation resonances due to disk eccentricity. Depending on disk parameters, the migration of the outer planet can be stopped in a non-resonant state. In other models, the outer planet is able to open a partial gap and to circularize the disk again, later entering a 2:1 resonance with the most massive planet in the system to complete the observed Laplace resonance. Disk-mediated interactions between planets due to spiral waves and excitation of disk eccentricity cause deviations from smooth inward migration of exterior lower mass planets. Self-consistent modelling of the disk-driven migration of multi-planet systems is thus mandatory. Our results are compatible with a late migration of the outermost planet into the resonant chain, when the giant planet pair already is in resonance. ","Formation of a planetary Laplace resonance through migration in an
  eccentric disk - The case of GJ876"
53,1029162643875979264,2153578286,Matthew Harding ðŸ³ï¸â€ðŸŒˆ,"['New paper on ""A Panel Quantile Approach to Attrition Bias in Big Data"" with a nice application to smart meter data from a randomized trial forthcoming in the Journal of Econometrics <LINK>']",https://arxiv.org/abs/1808.03364,"This paper introduces a quantile regression estimator for panel data models with individual heterogeneity and attrition. The method is motivated by the fact that attrition bias is often encountered in Big Data applications. For example, many users sign-up for the latest program but few remain active users several months later, making the evaluation of such interventions inherently very challenging. Building on earlier work by Hausman and Wise (1979), we provide a simple identification strategy that leads to a two-step estimation procedure. In the first step, the coefficients of interest in the selection equation are consistently estimated using parametric or nonparametric methods. In the second step, standard panel quantile methods are employed on a subset of weighted observations. The estimator is computationally easy to implement in Big Data applications with a large number of subjects. We investigate the conditions under which the parameter estimator is asymptotically Gaussian and we carry out a series of Monte Carlo simulations to investigate the finite sample properties of the estimator. Lastly, using a simulation exercise, we apply the method to the evaluation of a recent Time-of-Day electricity pricing experiment inspired by the work of Aigner and Hausman (1980). ","A Panel Quantile Approach to Attrition Bias in Big Data: Evidence from a
  Randomized Experiment"
54,1029088137782259712,15732617,Alex Sherstinsky,['My new paper demystifying RNN/LSTM is now available: <LINK> -- I hope that it will be useful to you. <LINK>'],https://arxiv.org/abs/1808.03314,"Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of ""unrolling"" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the ""Vanilla LSTM"" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well. ","Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term
  Memory (LSTM) Network"
55,1027888142714593285,156804540,Francisco Rodrigues,['Our new paper on @arxiv_org: Mobility helps problem-solving systems to avoid Groupthink \n<LINK>\n#Physics #Networks #Science #Mobility #Complexity #socialmobility #socialnetwork\n@LeverhulmeTrust <LINK>'],https://arxiv.org/abs/1808.02931,"Groupthink occurs when everyone in a group starts thinking alike, as when people put unlimited faith in a leader. Avoiding this phenomenon is a ubiquitous challenge to problem-solving enterprises and typical countermeasures involve the mobility of group members. Here we use an agent-based model of imitative learning to study the influence of the mobility of the agents on the time they require to find the global maxima of NK-fitness landscapes. The agents cooperate by exchanging information on their fitness and use this information to copy the fittest agent in their influence neighborhoods, which are determined by face-to-face interaction networks. The influence neighborhoods are variable since the agents perform random walks in a two-dimensional space. We find that mobility is slightly harmful for solving easy problems, i.e. problems that do not exhibit suboptimal solutions or local maxima. For difficult problems, however, mobility can prevent the imitative search being trapped in suboptimal solutions and guarantees a better performance than the independent search for any system size. ",Mobility helps problem-solving systems to avoid Groupthink
56,1027855425100832768,16015499,ProfGhristMath,['a new paper with Jakob Hansen on lifting spectral graph theory to cellular sheaves: <LINK>'],https://arxiv.org/abs/1808.01513,"This paper outlines a program in what one might call spectral sheaf theory --- an extension of spectral graph theory to cellular sheaves. By lifting the combinatorial graph Laplacian to the Hodge Laplacian on a cellular sheaf of vector spaces over a regular cell complex, one can relate spectral data to the sheaf cohomology and cell structure in a manner reminiscent of spectral graph theory. This work gives an exploratory introduction, and includes results on eigenvalue interlacing, sparsification, effective resistance, and sheaf approximation. These results and subsequent applications are prefaced by an introduction to cellular sheaves and Laplacians. ",Toward a Spectral Theory of Cellular Sheaves
57,1027587199750025218,475760077,Dr Sarah Casewell,"['Our paper on a new low mass eclipsing M dwarf system has been accepted for publication! <LINK> This is an eclipsing system of two M5 dwarfs in a 1.75 day orbit from @NextGenTransits and was in some of the first NGTS data taken!', '@TimSlingsby @NextGenTransits This is two small stars orbiting each other. They both pass on front of each other, so by looking at the change in brightness and the wobble as they orbit each other  we can measure their masses and radii directly. There are very few objects this mass we can do this for!']",https://arxiv.org/abs/1808.02761,"We have discovered a new, near-equal mass, eclipsing M dwarf binary from the Next Generation Transit Survey. This system is only one of 3 field age ($>$ 1 Gyr), late M dwarf eclipsing binaries known, and has a period of 1.74774 days, similar to that of CM~Dra and KOI126. Modelling of the eclipses and radial velocities shows that the component masses are $M_{\rm pri}$=0.17391$^{+0.00153}_{0.00099}$ $M_{\odot}$, $M_{\rm sec}$=0.17418$^{+0.00193}_{-0.00059}$ $M_{\odot}$; radii are $R_{\rm pri}$=0.2045$^{+0.0038}_{-0.0058}$ $R_{\odot}$, $R_{\rm sec}$=0.2168$^{+0.0047}_{-0.0048}$ $R_{\odot}$. The effective temperatures are $T_{\rm pri} = 2995\,^{+85}_{-105}$ K and $T_{\rm sec} = 2997\,^{+66}_{-101}$ K, consistent with M5 dwarfs and broadly consistent with main sequence models. This pair represents a valuable addition which can be used to constrain the mass-radius relation at the low mass end of the stellar sequence. ","A low-mass eclipsing binary within the fully convective zone from the
  NGTS"
58,1027547137075294221,1202760024,Stacy McGaugh,"['New paper on the arxiv: <LINK> The EDGES 21cm absorption signal that is impossible in LCDM occurs naturally if there is no dark matter. Also extend the prediction to the dark ages (z~100) where the absorption should again be stronger than possible in LCDM.', '@MatBethermin I specifically chose the model that fits the CMB up for L&lt;600. It does not fit L&gt;600. LCDM does not fit EDGES, and cannot do so without serious modification. So once again, we are up the sh*t creek without a paddle: DM modifications may not even be the right sh*t creek.', '@MatBethermin Yâ€™know, thatâ€™s exactly what folks said when my prediction for the second peak came true. â€œItâ€™ll falsify MOND If it gets biggerâ€, the assumption being it would. It didnâ€™t. Still exactly what I predicted with no CDM. The third peak is bigger than that model predicts, so something', '@MatBethermin more has to be going on. That falsified the simple ansatz I made, but that had to happen at some level. Lots of people wanted to constrain MOND to the no CDM line (when it itself makes no prediction at all) whereas pretty much anything else was taken as a victory for LCDM.', '@MatBethermin Now itâ€™s the other way around. Planck constrains LCDM so well it has to be exactly the red line in the figure in my paper. Anything else kills it. It may be indicative that no CDM fits, or that might be a remarkable coincidence again. More interesting to me is how LCDM fares.', '@MatBethermin Thatâ€™d be neat: Iâ€™d like to think thereâ€™s still something new to learn about the universe. If only weâ€™ll listen...', '@MatBethermin Indeed. Iâ€™m not pretending MOND is a complete answer, for all the reasons you give. But there is also something more to it than most scientists seem to appreciate. Trying to put it all together has repeatedly left me up olde proverbial creek.', '@maximetrebitsch @MatBethermin Of course we want confirmation. But EDGES have done an excellent job, so Iâ€™m also reluctant to play blame the observer. I was just mulling over ideas like the one you suggest and... it is really hard to come up with something that helps without messing up the CMB.']",https://arxiv.org/abs/1808.02532,"We consider the 21cm absorption signal expected at high redshift in cosmologies with and without non-baryonic cold dark matter. The expansion of the early universe decelerates strongly with dark matter, but approximately coasts without it. This results in a different path length across the epochs when absorption is expected, with the consequence that the absorption is predicted to be a factor of $\sim 2$ greater without dark matter than with it. Observation of such a signal would motivate consideration of extended theories of gravity in lieu of dark matter. ","Predictions for the sky-averaged depth of the 21cm absorption signal at
  high redshift in cosmologies with and without non-baryonic cold dark matter"
59,1027482936373858305,156804540,Francisco Rodrigues,['Our new paper on arxiv: Pattern Recognition Approach to Violin Shapes of MIMO database with @thomas_peron \n<LINK>\nWe show that the average violin outline has remained mostly stable over time. #music #ClassicalMusic #DataScience #Analytics #MachineLearning <LINK>'],https://arxiv.org/abs/1808.02848,"Since the landmarks established by the Cremonese school in the 16th century, the history of violin design has been marked by experimentation. While great effort has been invested since the early 19th century by the scientific community on researching violin acoustics, substantially less attention has been given to the statistical characterization of how the violin shape evolved over time. In this paper we study the morphology of violins retrieved from the Musical Instrument Museums Online (MIMO) database -- the largest freely accessible platform providing information about instruments held in public museums. From the violin images, we derive a set of measurements that reflect relevant geometrical features of the instruments. The application of Principal Component Analysis (PCA) uncovered similarities between violin makers and their respective copyists, as well as among luthiers belonging to the same family lineage, in the context of historical narrative. Combined with a time-windowed approach, thin plate splines visualizations revealed that the average violin outline has remained mostly stable over time, not adhering to any particular trends of design across different periods in music history. ",Pattern Recognition Approach to Violin Shapes of MIMO database
60,1027476293963730945,41105698,Moncho Rey Raposo,['<LINK>\n\nNew paper out there. The stars that are part of the same cluster tend to spin in the same direction. <LINK>'],http://arxiv.org/abs/1808.02830,"We simulate star formation in two molecular clouds extracted from a larger disc-galaxy simulation with a spatial resolution of ~0.1 pc, one exiting a spiral arm dominated by compression, and another in an inter-arm region more strongly affected by galactic shear. Treating the stars as 'sink particles', we track their birth angular momentum, and the later evolution of their angular momentum due to gas accretion. We find that in both clouds, the sinks have spin vectors that are aligned with one another, and with the global angular momentum vector of the star cluster. This alignment is present at birth, but enhanced by later gas accretion. In the compressive cloud, the sink-spins remain aligned with the gas for at least a free fall time. By contrast, in the shear cloud, the increased turbulent mixing causes the sinks to rapidly misalign with their birth cloud on approximately a gas free-fall time. In spite of this, both clouds show a strong alignment of sink-spins at the end of our simulations, independently of environment. ","The alignment is in their stars: on the spin-alignment of stars in star
  clusters"
61,1027368062419259393,822304279037784065,Terry Taewoong Um,"['""Parkinson\'s Disease Assessment from a Wrist-Worn Wearable Sensor in Free-Living Conditions: Deep Ensemble Learning and Visualization""\n<LINK>\nMy new paper! Unreliable predictions can be overcome by introducing an ensemble of CNNs &amp; a simple prediction smoothing. <LINK>']",https://arxiv.org/abs/1808.02870,"Parkinson's Disease (PD) is characterized by disorders in motor function such as freezing of gait, rest tremor, rigidity, and slowed and hyposcaled movements. Medication with dopaminergic medication may alleviate those motor symptoms, however, side-effects may include uncontrolled movements, known as dyskinesia. In this paper, an automatic PD motor-state assessment in free-living conditions is proposed using an accelerometer in a wrist-worn wearable sensor. In particular, an ensemble of convolutional neural networks (CNNs) is applied to capture the large variability of daily-living activities and overcome the dissimilarity between training and test patients due to the inter-patient variability. In addition, class activation map (CAM), a visualization technique for CNNs, is applied for providing an interpretation of the results. ","Parkinson's Disease Assessment from a Wrist-Worn Wearable Sensor in
  Free-Living Conditions: Deep Ensemble Learning and Visualization"
62,1027352291718836224,23980621,"Brett Morris, PhD","['Building on my most recent paper (<LINK>), my new ApJ Letter applies the ""self-contamination"" technique to TRAPPIST-1 w/ Spitzer to find:\n\nNon-detection of Contamination by Stellar Activity in the Spitzer Transit Light Curves of TRAPPIST-1\n<LINK> <LINK>']",https://arxiv.org/abs/1807.04886,"We typically measure the radii of transiting exoplanets from the transit depth, which is given by the ratio of cross-sectional areas of the planet and star. However, if a star has dark starspots (or bright regions) distributed throughout the transit chord, the transit depth will be biased towards smaller (larger) values, and thus the inferred planet radius will be smaller (larger) if unaccounted for. We reparameterize the transit light curve to account for ""self-contamination"" by photospheric inhomogeneities by splitting the parameter $R_p/R_\star$ into two parameters: one for the radius ratio -- which controls the duration of ingress and egress -- and another which measures the possibly contaminated transit depth. We show that this is equivalent to the formulation for contamination by a second star (with positive or negative flux), and that it is sensitive to time-steady inhomogeneity of the stellar photosphere. We use synthetic light curves of spotted stars at high signal-to-noise to show that the radius recovered from measurement of the ingress/egress duration can recover the true radii of planets transiting spotted stars with axisymmetric spot distributions if the limb-darkening parameters are precisely known. We fit time-averaged high signal-to-noise transit light curves from Kepler and Spitzer of ten planets to measure the planet radii and search for evidence of spot distributions. We find that this sample has a range of measured depths and ingress durations which are self-consistent, providing no strong evidence for contamination by spots. However, there is suggestive evidence for occultation of starspots on Kepler-17, and that relatively bright regions are occulted by the planets of Kepler-412 and HD 80606. Future observations with the James Webb Space Telescope may enable this technique to yield accurate planetary radii in the presence of stellar inhomogeneities. ","Robust Transiting Exoplanet Radii in the Presence of Starspots from
  Ingress and Egress Durations"
63,1027321990829330432,7984662,Clayton Shonkwiler,"[""A fractal dimension for measures via persistent homology\n\nNew paper with the Pattern Analysis Lab at CSU. The title says pretty well what the paper is about; I'll just note that minimal spanning trees pop up as well.\n\n<LINK>""]",https://arxiv.org/abs/1808.01079,"We use persistent homology in order to define a family of fractal dimensions, denoted $\mathrm{dim}_{\mathrm{PH}}^i(\mu)$ for each homological dimension $i\ge 0$, assigned to a probability measure $\mu$ on a metric space. The case of $0$-dimensional homology ($i=0$) relates to work by Michael J Steele (1988) studying the total length of a minimal spanning tree on a random sampling of points. Indeed, if $\mu$ is supported on a compact subset of Euclidean space $\mathbb{R}^m$ for $m\ge2$, then Steele's work implies that $\mathrm{dim}_{\mathrm{PH}}^0(\mu)=m$ if the absolutely continuous part of $\mu$ has positive mass, and otherwise $\mathrm{dim}_{\mathrm{PH}}^0(\mu)<m$. Experiments suggest that similar results may be true for higher-dimensional homology $0<i<m$, though this is an open question. Our fractal dimension is defined by considering a limit, as the number of points $n$ goes to infinity, of the total sum of the $i$-dimensional persistent homology interval lengths for $n$ random points selected from $\mu$ in an i.i.d. fashion. To some measures $\mu,$ we are able to assign a finer invariant, a curve measuring the limiting distribution of persistent homology interval lengths as the number of points goes to infinity. We prove this limiting curve exists in the case of $0$-dimensional homology when $\mu$ is the uniform distribution over the unit interval, and conjecture that it exists when $\mu$ is the rescaled probability measure for a compact set in Euclidean space with positive Lebesgue measure. ",A fractal dimension for measures via persistent homology
64,1027240285737234432,8383522,DJ Strouse,"['New paper on training agents to cooperate / compete by regularizing the reward-relevant information they share with other agents. Allows for agents trained alone to nevertheless perform well in a multi-agent setting. With @maxhkw, @davidjschwab, et al. <LINK>']",http://arxiv.org/abs/1808.02093,"Learning to cooperate with friends and compete with foes is a key component of multi-agent reinforcement learning. Typically to do so, one requires access to either a model of or interaction with the other agent(s). Here we show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction. Our approach is to encourage an agent to reveal or hide their intentions using an information-theoretic regularizer. We consider both the mutual information between goal and action given state, as well as the mutual information between goal and state. We show how to optimize these regularizers in a way that is easy to integrate with policy gradient reinforcement learning. Finally, we demonstrate that cooperative (competitive) policies learned with our approach lead to more (less) reward for a second agent in two simple asymmetric information games. ",Learning to Share and Hide Intentions using Information Regularization
65,1027152596354654208,63441844,David Roberson,"['Put a new paper on the @arXiv today, with @LauraMancinska, Irene Pivotto, and Gordon Royle. We show that the core of a cubelike graph must share many properties of cubelike graphs. In other words, they are very cubelike-like (cubesque?). <LINK>']",https://arxiv.org/abs/1808.02051,"A graph is $\textit{cubelike}$ if it is a Cayley graph for some elementary abelian $2$-group $\mathbb{Z}_2^n$. The core of a graph is its smallest subgraph to which it admits a homomorphism. More than ten years ago, Ne\v{s}et\v{r}il and \v{S}\'amal (On tension-continuous mappings. $\textit{European J. Combin.,}$ 29(4):1025--1054, 2008) asked whether the core of a cubelike graph is cubelike, but since then very little progress has been made towards resolving the question. Here we investigate the structure of the core of a cubelike graph, deducing a variety of structural, spectral and group-theoretical properties that the core ""inherits"" from the host cubelike graph. These properties constrain the structure of the core quite severely --- even if the core of a cubelike graph is not actually cubelike, it must bear a very close resemblance to a cubelike graph. Moreover we prove the much stronger result that not only are these properties inherited by the core of a cubelike graph, but also by the orbital graphs of the core. Even though the core and its orbital graphs look very much like cubelike graphs, we are unable to show that this is sufficient to characterise cubelike graphs. However, our results are strong enough to eliminate all non-cubelike vertex-transitive graphs on up to $32$ vertices as potential cores of cubelike graphs (of any size). Thus, if one exists at all, a cubelike graph with a non-cubelike core has at least $128$ vertices and its core has at least $64$ vertices. ",Cores of Cubelike Graphs
66,1027110135787859968,2191249633,Sebastiano Carpi,"['New paper on the arXiv: ""Positive energy representations of Sobolev diffeomorphism groups of the circle"" (with Simone Del Vecchio, Stefano Iovieno and Yoh Tanimoto)\n<LINK>']",https://arxiv.org/abs/1808.02384,"We show that any positive energy projective unitary representation of Diff(S^1) extends to a strongly continuous projective unitary representation of the fractional Sobolev diffeomorphisms D^s(S^1) for any real s>3, and in particular to C^k-diffeomorphisms Diff^k(S^1) with k>=4. A similar result holds for the universal covering groups provided that the representation is assumed to be a direct sum of irreducibles. As an application we show that a conformal net of von Neumann algebras on S^1 is covariant with respect to D^s(S^1), s > 3. Moreover every direct sum of irreducible representations of a conformal net is also D^s(S^1)-covariant. ","Positive energy representations of Sobolev diffeomorphism groups of the
  circle"
67,1027088768401113088,280083723,Yoh Tanimoto,"['new paper~ <LINK> positive energy representations of the Virasoro algebra extend to Sobolev diffeomorphisms D^s, s&gt;3~']",https://arxiv.org/abs/1808.02384,"We show that any positive energy projective unitary representation of Diff(S^1) extends to a strongly continuous projective unitary representation of the fractional Sobolev diffeomorphisms D^s(S^1) for any real s>3, and in particular to C^k-diffeomorphisms Diff^k(S^1) with k>=4. A similar result holds for the universal covering groups provided that the representation is assumed to be a direct sum of irreducibles. As an application we show that a conformal net of von Neumann algebras on S^1 is covariant with respect to D^s(S^1), s > 3. Moreover every direct sum of irreducible representations of a conformal net is also D^s(S^1)-covariant. ","Positive energy representations of Sobolev diffeomorphism groups of the
  circle"
68,1027037843045023744,315718949,ClÃ©ment Canonne,"[""New paper out, with J. Acharya, C. Freitag, and H. Tyagi: Locally private algorithms for hypothesis testing. I'm pretty excited about the underlying technique: dinosaurs! \n<LINK> \n\n(picture from <LINK>) <LINK>"", 'Blimey, I do suck at Twitter handles. w/ @AcharyaJayadev, tagging #privacy #statistics #bambiraptor']",https://arxiv.org/abs/1808.02174,"We study the problem of distribution testing when the samples can only be accessed using a locally differentially private mechanism and focus on two representative testing questions of identity (goodness-of-fit) and independence testing for discrete distributions. We are concerned with two settings: First, when we insist on using an already deployed, general-purpose locally differentially private mechanism such as the popular RAPPOR or the recently introduced Hadamard Response for collecting data, and must build our tests based on the data collected via this mechanism; and second, when no such restriction is imposed, and we can design a bespoke mechanism specifically for testing. For the latter purpose, we introduce the Randomized Aggregated Private Testing Optimal Response (RAPTOR) mechanism which is remarkably simple and requires only one bit of communication per sample. We propose tests based on these mechanisms and analyze their sample complexities. Each proposed test can be implemented efficiently. In each case (barring one), we complement our performance bounds for algorithms with information-theoretic lower bounds and establish sample optimality of our proposed algorithm. A peculiar feature that emerges is that our sample-optimal algorithm based on RAPTOR uses public-coins, and any test based on RAPPOR or Hadamard Response, which are both private-coin mechanisms, requires significantly more samples. ",Test without Trust: Optimal Locally Private Distribution Testing
69,1027009013744103424,305751067,Dr. Nickolas Pingel,['Check out my new paper on our GBT survey of the HALOGAS galaxy sample. The tldr: no evidence for accreting neutral gas. What does that mean? More telescope time plz!\n\n<LINK>'],https://arxiv.org/abs/1808.02041,"We present initial results from a deep neutral hydrogen (HI) survey of the HALOGAS galaxy sample, which includes the spiral galaxies NGC891, NGC925, NGC4414, and NGC4565, performed with the Robert C. Byrd Green Bank Telescope (GBT). The resulting observations cover at least four deg$^2$ around these galaxies with an average 5$\sigma$ detection limit of 1.2$\times$10$^{18}$ cm$^{-2}$ over a velocity range of 20 km s$^{-1}$ and angular scale of 9.1$'$. In addition to detecting the same total flux as the GBT data, the spatial distribution of the GBT and original Westerbork Synthesis Radio Telescope (WSRT) data match well at equal spatial resolutions. The HI mass fraction below HI column densities of 10$^{19}$ cm$^{-2}$ is, on average, 2\%. We discuss the possible origins of low column density HI of nearby spiral galaxies. The absence of a considerable amount of newly detected HI by the GBT indicates these galaxies do not have significant extended diffuse HI structures, and suggests future surveys planned with the SKA and its precursors must go \textit{at least} as deep as 10$^{17}$ cm$^{-2}$ in column density to significantly increase the probability of detecting HI associated with the cosmic web and/or cold mode accretion. ","A GBT Survey of the HALOGAS Galaxies and Their Environments I: Revealing
  the full extent of HI around NGC891, NGC925, NGC4414 & NGC4565"
70,1025337456974094337,2432886163,Oscar BarragÃ¡n,['Check our new paper on the discovery of a four planet system observed by K2.  \n\nTwo of them with measured masses!\n\ncheck it here -&gt; <LINK> <LINK>'],https://arxiv.org/abs/1808.00575,"The Kepler extended mission, also known as K2, has provided the community with a wealth of planetary candidates that orbit stars typically much brighter than the targets of the original mission. These planet candidates are suitable for further spectroscopic follow-up and precise mass determinations, leading ultimately to the construction of empirical mass-radius diagrams. Particularly interesting is to constrain the properties of planets between the Earth and Neptune in size, the most abundant type of planets orbiting Sun-like stars with periods less than a few years. Among many other K2 candidates, we discovered a multi-planetary system around EPIC246471491, with four planets ranging in size from twice the size of Earth, to nearly the size of Neptune. We measure the mass of the planets of the EPIC246471491 system by means of precise radial velocity measurements using the CARMENES spectrograph and the HARPS-N spectrograph. With our data we are able to determine the mass of the two inner planets of the system with a precision better than 15%, and place upper limits on the masses of the two outer planets. We find that EPIC246471491b has a mass of 9.68 Me, and a radius of 2.59 Re, yielding a mean density of 3.07 g/cm3, while EPIC246471491c has a mass of 15.68 Me, radius of 3.53 Re, and a mean density of 19.5 g/cm3. For EPIC246471491d (R=2.48Re) and EPIC246471491e (R=1.95Re) the upper limits for the masses are 6.5 and 10.7 Me, respectively. The system is thus composed of a nearly Neptune-twin planet (in mass and radius), two sub-Neptunes with very different densities and presumably bulk composition, and a fourth planet in the outermost orbit that resides right in the middle of the super-Earth/sub-Neptune radius gap. Future comparative planetology studies of this system can provide useful insights into planetary formation, and also a good test of atmospheric escape and evolution theories. ","Detection and Doppler monitoring of EPIC 246471491, a system of four
  transiting planets smaller than Neptune"
71,1025179493399318528,1558538456,Rodrigo FernÃ¡ndez,"['Converged mass ejection (and jet) from NS merger accretion disk around (promptly-formed) BH remnant, modeled in 3D GRMHD\n\nNew paper in collaboration with Sasha Tchekhovskoy and Quataert, Foucart, &amp; Kasen:\n\n<LINK> <LINK>', ""Sasha's code is publicly available:\n\nhttps://t.co/rBpJohYan5""]",https://arxiv.org/abs/1808.00461,"We investigate the long-term evolution of black hole accretion disks formed in neutron star mergers. These disks expel matter that contributes to an $r$-process kilonova, and can produce relativistic jets powering short gamma-ray bursts. Here we report the results of a three-dimensional, general-relativistic magnetohydrodynamic (GRMHD) simulation of such a disk which is evolved for long enough ($\sim 9$s, or $\sim 6\times 10^5 r_{\rm g}/c$) to achieve completion of mass ejection far from the disk. Our model starts with a poloidal field, and fully resolves the most unstable mode of the magnetorotational instability. We parameterize the dominant microphysics and neutrino cooling effects, and compare with axisymmetric hydrodynamic models with shear viscosity. The GRMHD model ejects mass in two ways: a prompt MHD-mediated outflow and a late-time, thermally-driven wind once the disk becomes advective. The total amount of unbound mass ejected ($0.013M_\odot$, or $\simeq 40\%$ of the initial torus mass) is twice as much as in hydrodynamic models, with higher average velocity ($0.1c$) and a broad electron fraction distribution with a lower average value ($0.16$). Scaling the ejected fractions to a disk mass of $\sim 0.1M_\odot$ can account for the red kilonova from GW170817 but underpredicts the blue component. About $\sim 10^{-3}M_\odot$ of material should undergo neutron freezout and could produce a bright kilonova precursor in the first few hours after the merger. With our idealized initial magnetic field configuration, we obtain a robust jet and sufficient ejecta with Lorentz factor $\sim 1-10$ to (over)produce the non-thermal emission from GW1708107. ","Long-term GRMHD Simulations of Neutron Star Merger Accretion Disks:
  Implications for Electromagnetic Counterparts"
72,1024931990146039808,3325105445,Ben F. Maier,"['Things diffuse faster on weak modular hierarchical and other small-world network models!\n\nnew paper on the arXiv with @Cristian_Huepe and @DirkBrockmann <LINK>', ""@ulfaslak @Cristian_Huepe @DirkBrockmann couldn't resist ðŸ˜Œ""]",https://arxiv.org/abs/1808.00240,"Networks that are organized as a hierarchy of modules have been the subject of much research, mainly focusing on algorithms that can extract this community structure from data. The question of why modular hierarchical organizations are so ubiquitous in nature, however, has received less attention. One hypothesis is that modular hierarchical topologies may provide an optimal structure for certain dynamical processes. We revisit a modular hierarchical network model that interpolates, using a single parameter, between two known network topologies: from strong hierarchical modularity to an Erd\H{o}s-R\'enyi random connectivity structure. We show that this model displays a similar small-world effect as the Kleinberg model, where the connection probability between nodes decays algebraically with distance. We find that there is an optimal structure, in both models, for which the pair-averaged first passage time (FPT) and mean cover time of a discrete-time random walk are minimal, and provide a heuristic explanation for this effect. Finally, we show that analytic predictions for the pair-averaged FPT based on an effective medium approximation fail to reproduce these minima, which implies that their presence is due to a network structure effect. ","Modular hierarchical and power-law small-world networks bear structural
  optima for minimal first passage times and cover time"
73,1036854256463044608,131183554,Sergi Caelles,['New paper on road topology estimation from aerial images at #BMVC2018! ðŸŽ‰\nPoster 257 on Wednesday 5th from 14:00 - 16:15\nPaper: <LINK>\nMore info: <LINK>\n@carles_ventura @jponttuset @kmaninis #BMVC <LINK>'],https://arxiv.org/abs/1808.09814,"This paper tackles the task of estimating the topology of road networks from aerial images. Building on top of a global model that performs a dense semantical classification of the pixels of the image, we design a Convolutional Neural Network (CNN) that predicts the local connectivity among the central pixel of an input patch and its border points. By iterating this local connectivity we sweep the whole image and infer the global topology of the road network, inspired by a human delineating a complex network with the tip of their finger. We perform an extensive and comprehensive qualitative and quantitative evaluation on the road network estimation task, and show that our method also generalizes well when moving to networks of retinal vessels. ",Iterative Deep Learning for Road Topology Extraction
74,1034435832130752512,3323459854,Ronnie Clark,"['Our new work on object-centric scene reconstruction is online! Check out the video, <LINK>, and paper here: <LINK> #computervision #deeplearning #3dv2018 <LINK>']",https://arxiv.org/abs/1808.08378,"We propose an online object-level SLAM system which builds a persistent and accurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera browses a cluttered indoor scene, Mask-RCNN instance segmentations are used to initialise compact per-object Truncated Signed Distance Function (TSDF) reconstructions with object size-dependent resolutions and a novel 3D foreground mask. Reconstructed objects are stored in an optimisable 6DoF pose graph which is our only persistent map representation. Objects are incrementally refined via depth fusion, and are used for tracking, relocalisation and loop closure detection. Loop closures cause adjustments in the relative pose estimates of object instances, but no intra-object warping. Each object also carries semantic information which is refined over time and an existence probability to account for spurious instance predictions. We demonstrate our approach on a hand-held RGB-D sequence from a cluttered office scene with a large number and variety of object instances, highlighting how the system closes loops and makes good use of existing objects on repeated loops. We quantitatively evaluate the trajectory error of our system against a baseline approach on the RGB-D SLAM benchmark, and qualitatively compare reconstruction quality of discovered objects on the YCB video dataset. Performance evaluation shows our approach is highly memory efficient and runs online at 4-8Hz (excluding relocalisation) despite not being optimised at the software level. ",Fusion++: Volumetric Object-Level SLAM
75,1032705009102860288,264542169,Katherine Freese,['New Simons Observatory forecast paper is out! <LINK> <LINK>'],https://arxiv.org/abs/1808.07445,"The Simons Observatory (SO) is a new cosmic microwave background experiment being built on Cerro Toco in Chile, due to begin observations in the early 2020s. We describe the scientific goals of the experiment, motivate the design, and forecast its performance. SO will measure the temperature and polarization anisotropy of the cosmic microwave background in six frequency bands: 27, 39, 93, 145, 225 and 280 GHz. The initial configuration of SO will have three small-aperture 0.5-m telescopes (SATs) and one large-aperture 6-m telescope (LAT), with a total of 60,000 cryogenic bolometers. Our key science goals are to characterize the primordial perturbations, measure the number of relativistic species and the mass of neutrinos, test for deviations from a cosmological constant, improve our understanding of galaxy evolution, and constrain the duration of reionization. The SATs will target the largest angular scales observable from Chile, mapping ~10% of the sky to a white noise level of 2 $\mu$K-arcmin in combined 93 and 145 GHz bands, to measure the primordial tensor-to-scalar ratio, $r$, at a target level of $\sigma(r)=0.003$. The LAT will map ~40% of the sky at arcminute angular resolution to an expected white noise level of 6 $\mu$K-arcmin in combined 93 and 145 GHz bands, overlapping with the majority of the LSST sky region and partially with DESI. With up to an order of magnitude lower polarization noise than maps from the Planck satellite, the high-resolution sky maps will constrain cosmological parameters derived from the damping tail, gravitational lensing of the microwave background, the primordial bispectrum, and the thermal and kinematic Sunyaev-Zel'dovich effects, and will aid in delensing the large-angle polarization signal to measure the tensor-to-scalar ratio. The survey will also provide a legacy catalog of 16,000 galaxy clusters and more than 20,000 extragalactic sources. ",The Simons Observatory: Science goals and forecasts
76,1032704701672972289,264542169,Katherine Freese,['New Simons Observatory forecast paper is out! @SimonsObs \n<LINK>'],https://arxiv.org/abs/1808.07445,"The Simons Observatory (SO) is a new cosmic microwave background experiment being built on Cerro Toco in Chile, due to begin observations in the early 2020s. We describe the scientific goals of the experiment, motivate the design, and forecast its performance. SO will measure the temperature and polarization anisotropy of the cosmic microwave background in six frequency bands: 27, 39, 93, 145, 225 and 280 GHz. The initial configuration of SO will have three small-aperture 0.5-m telescopes (SATs) and one large-aperture 6-m telescope (LAT), with a total of 60,000 cryogenic bolometers. Our key science goals are to characterize the primordial perturbations, measure the number of relativistic species and the mass of neutrinos, test for deviations from a cosmological constant, improve our understanding of galaxy evolution, and constrain the duration of reionization. The SATs will target the largest angular scales observable from Chile, mapping ~10% of the sky to a white noise level of 2 $\mu$K-arcmin in combined 93 and 145 GHz bands, to measure the primordial tensor-to-scalar ratio, $r$, at a target level of $\sigma(r)=0.003$. The LAT will map ~40% of the sky at arcminute angular resolution to an expected white noise level of 6 $\mu$K-arcmin in combined 93 and 145 GHz bands, overlapping with the majority of the LSST sky region and partially with DESI. With up to an order of magnitude lower polarization noise than maps from the Planck satellite, the high-resolution sky maps will constrain cosmological parameters derived from the damping tail, gravitational lensing of the microwave background, the primordial bispectrum, and the thermal and kinematic Sunyaev-Zel'dovich effects, and will aid in delensing the large-angle polarization signal to measure the tensor-to-scalar ratio. The survey will also provide a legacy catalog of 16,000 galaxy clusters and more than 20,000 extragalactic sources. ",The Simons Observatory: Science goals and forecasts
77,1032170111803838464,899968956253044737,Yanai Elazar,"['using adversarial training for removing sensitive features from text? do not trust its results! new #emnlp2018 paper with @yoavgo now in arxiv\n<LINK>', ""@julius_adebayo @yoavgo glad you liked it =)\nregarding other domains, I hope not, but I think it can happen. \nimo the most important thing is to be aware that it can happen, and if the features you're trying to remove are sensitive, make sure to verify they're gone entirely""]",https://arxiv.org/abs/1808.06640,"Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in -- and can be recovered from -- the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to -- and likely condition on -- demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features. ",Adversarial Removal of Demographic Attributes from Text Data
78,1032086132765417477,1003652696723873792,Max Gaspari,"['New paper by Anna Juranova, a very smart Astro student who I had the pleasure to mentor this summer. Great work Anna!\n<LINK>\n#astrophysics #astronomy @AstroPHYPapers']",https://arxiv.org/abs/1808.05761,"The relative importance of the physical processes shaping the thermodynamics of the hot gas permeating rotating, massive early-type galaxies is expected to be different from that in non-rotating systems. Here, we report the results of the analysis of XMM-Newton data for the massive, lenticular galaxy NGC 7049. The galaxy harbours a dusty disc of cool gas and is surrounded by an extended hot X-ray emitting gaseous atmosphere with unusually high central entropy. The hot gas in the plane of rotation of the cool dusty disc has a multi-temperature structure, consistent with ongoing cooling. We conclude that the rotational support of the hot gas is likely capable of altering the multiphase condensation regardless of the $t_{\rm cool}/t_{\rm ff}$ ratio, which is here relatively high, $\sim 40$. However, the measured ratio of cooling time and eddy turnover time around unity ($C$-ratio $\approx 1$) implies significant condensation, and at the same time, the constrained ratio of rotational velocity and the velocity dispersion (turbulent Taylor number) ${\rm Ta_t} > 1$ indicates that the condensing gas should follow non-radial orbits forming a disc instead of filaments. This is in agreement with hydrodynamical simulations of massive rotating galaxies predicting a similarly extended multiphase disc. ","Cooling in the X-ray halo of the rotating, massive early-type galaxy NGC
  7049"
79,1039885487715037186,2191799629,Sebastian Gehrmann,"['A little late to the party, but check out our #emnlp2018 paper on bottom-up abstractive summarization! <LINK>\nWe find that constraining copy-attention to predetermined words and phrases greatly improves results, with potential application to low-resource domains!']",https://arxiv.org/abs/1808.10792,"Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but which can be poor at content selection. This work proposes a simple technique for addressing this issue: use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain. ",Bottom-Up Abstractive Summarization
80,1038139514248560640,17354555,Emilio Ferrara,"['In our latest, we propose a new computational framework to predict human behavior from sensor data via tensor embedding!\n\nw/@homahmrd @Amir_Ghasemian @KristinaLerman \n\nTensor Embedding: A Supervised Framework for Human Behavioral Data Mining and Prediction <LINK> <LINK>']",https://arxiv.org/abs/1808.10867,"Today's densely instrumented world offers tremendous opportunities for continuous acquisition and analysis of multimodal sensor data providing temporal characterization of an individual's behaviors. Is it possible to efficiently couple such rich sensor data with predictive modeling techniques to provide contextual, and insightful assessments of individual performance and wellbeing? Prediction of different aspects of human behavior from these noisy, incomplete, and heterogeneous bio-behavioral temporal data is a challenging problem, beyond unsupervised discovery of latent structures. We propose a Supervised Tensor Embedding (STE) algorithm for high dimension multimodal data with join decomposition of input and target variable. Furthermore, we show that features selection will help to reduce the contamination in the prediction and increase the performance. The efficiently of the methods was tested via two different real world datasets. ","Tensor Embedding: A Supervised Framework for Human Behavioral Data
  Mining and Prediction"
81,1035527715670183936,185910194,Graham Neubig,"['#EMNLP2018 ""A Tree-based Decoder for NMT"", a framework for incorporating trees in target side of MT systems. We compare constituency/dependency/non-syntactic binary trees, find surprising result that non-syntactic trees perform best, and try to explain why <LINK> <LINK>', ""Great, thorough work by @cindyxinyiwang w/ @hieupham789, @pengchengyin, and me. I'm excited about this because it made me re-think my assumptions about how we should think about syntax in NMT models. Lots of potential future directions!"", ""@yoavgo My guess is that high-level planning of the structure of the output sentence is difficult from the input surface form, and the model occasionally makes early mistakes that it can't recover from. In binary trees, sentence structure is trivial, so this won't happen."", ""@yoavgo Why do binary trees improve over no trees? Not 100% sure, but my guess is that it's in a way like multi-scale RNNs or dilated convolutions, with shorter paths between the beginning and end of the sentence."", '@yoavgo Quite possibly. Although arguably the constituency or dependency trees are making life even harder than the binary ones.', ""@ihsgnef Yes, I'd seen this, but thanks for noting it! I view this, in a way, as a further validation of Choi et al.'s results where latent tree learners that perform best tend to learn balanced trees."", '@zehavoc @yoavgo Definitely!']",https://arxiv.org/abs/1808.09374,"Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like constituency and dependency parse trees. This is often done via a standard RNN decoder that operates on a linearized target tree structure. However, it is an open question of what specific linguistic formalism, if any, is the best structural representation for NMT. In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU. ",A Tree-based Decoder for Neural Machine Translation
82,1034607219164295169,5850692,Aaron Roth,"['A new paper with Sampath Kannan and Juba Ziani: ""The Downstream Effects of Affirmative Action"": <LINK> We study a two stage model with a school and employers. Suppose the school only see noisy signals about student types (exam scores). 1/5', 'A school can choose an admissions policy (mapping from scores to acceptance probability) for each group, and a grading policy (variance of grade, which is also a noisy signal). 2/5', 'Employers are Bayesians, condition on everything, and hire students if their posterior expectation is above a threshold. What goals can the school try and achieve via differential admissions policies if group type distributions differ? Here are two: 3/5', ""1) Equal opportunity: The probability of making it through the pipeline (admitted to school and then hired by employer) should be independent of group membership given type. 2) Group independent hiring: Employers' hiring rules should be independent of group membership. 4/5"", 'The punchline: In general, its not possible to achieve either one of these goals if the school gives out informative grades (finite, nonzero variance). But a sufficiently selective school can achieve both if it withholds grades. 5/5']",https://arxiv.org/abs/1808.09004,"We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy. ",Downstream Effects of Affirmative Action
83,1034454643139641345,2869101210,Jenn Wortman Vaughan,['Uncanny case of great-minds-think-alike:\n\nNew paper on The Disparate Effects of Strategic Manipulation w/ @uhlily @immorlica\n<LINK> \n\nWe study the social impact of classification in systems marked by inequality+potential for manipulation. Complementary analyses. <LINK>'],https://arxiv.org/abs/1808.08646,"When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed ""strategic manipulation,"" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to ""trick"" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off--even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's ""quality"" when agents' capacities to adaptively respond differ. ",The Disparate Effects of Strategic Manipulation
84,1034267735616827392,203639204,Dan Elton,"['We have a new #matsci #preprint on the #arXiv:\n""A Phonon Boltzmann Study of Microscale Thermal Transport in Î±-RDX Cook-Off"" <LINK>  \nThis is the 1st full Brillouin zone phonon calculation for RDX.. to better understand thermal transport &amp; mechanism of initiation']",https://arxiv.org/abs/1808.08295,"The microscale thermal transport properties of $\alpha$RDX are believed to be major factors in the initiation process. In this study we present a thorough examination of phonon properties which dominate energy storage and transport in $\alpha$RDX. The phonon lifetimes are determined for all phonon branches, revealing the characteristic time scale of energy transfer amongst phonon modes. The phonon parameters also serve as inputs to a full Brillouin zone three dimensional phonon transport simulation in the presence of a hotspot. In addition to identifying the phonon mode contributions to thermal transport, and as N-N bond breaking is integral to disassociation, we identify phonon modes corresponding to large N-N bond stretch analyzing the manner in which these modes store and transfer energy. ","A Phonon Boltzmann Study of Microscale Thermal Transport in $\alpha$-RDX
  Cook-Off"
85,1033397237064327169,3439194748,Alexander Jung,"['""AI is the new electricity!"" \nIn our recent work, we study the flow of this new electricity within semi-supervised regression problems. \n<LINK>\n@AndrewYNg #ArtificialIntelligence #MachineLearning #networks']",https://arxiv.org/abs/1808.07249,"We apply network Lasso to semi-supervised regression problems involving network structured data. This approach lends quite naturally to highly scalable learning algorithms in the form of message passing over an empirical graph which represents the network structure of the data. By using a simple non-parametric regression model, which is motivated by a clustering hypothesis, we provide an analysis of the estimation error incurred by network Lasso. This analysis reveals conditions on the the network structure and the available training data which guarantee network Lasso to be accurate. Remarkably, the accuracy of network Lasso is related to the existence of sufficiently large network flows over the empirical graph. Thus, our analysis reveals a connection between network Lasso and maximum flow problems. ",Analysis of Network Lasso for Semi-Supervised Regression
86,1032632275664613376,16971666,Benjamin Wandelt,['In this study led by Christina Kreisch and Alice Pisani we find that massive neutrinos leave a unique fingerprint in the properties and statistics of cosmic voids. <LINK>'],https://arxiv.org/abs/1808.07464,"Do void statistics contain information beyond the tracer 2-point correlation function? Yes! As we vary the sum of the neutrino masses, we find void statistics contain information absent when using just tracer 2-point statistics. Massive neutrinos uniquely affect cosmic voids. We explore their impact on void clustering using both the DEMNUni and MassiveNuS simulations. For voids, neutrino effects depend on the observed void tracers. As the neutrino mass increases, the number of small voids traced by cold dark matter particles increases and the number of large voids decreases. Surprisingly, when massive, highly biased, halos are used as tracers, we find the opposite effect. The scale at which voids cluster, as well as the void correlation, is similarly sensitive to the sum of neutrino masses and the tracers. This scale dependent trend is not due to simulation volume or halo density. The interplay of these signatures in the void abundance and clustering leaves a distinct fingerprint that could be detected with observations and potentially help break degeneracies between different cosmological parameters. This paper paves the way to exploit cosmic voids in future surveys to constrain the mass of neutrinos. ",Massive Neutrinos Leave Fingerprints on Cosmic Voids
87,1031875253029556225,14104309,Darren Price,"['In a new paper released today, we present the first global analysis of CP-sensitive measurements of the #Higgs sector from #LHC data, highlight specific new measurements that are needed to resolve ambiguities, and study future prospects: <LINK> <LINK>', 'The #Higgs boson may play a special role in the origin of the matter anti-matter asymmetry of the Universe (where did all the anti-matter go?). \nEven without knowing the details, we can study #Higgs data from the #LHC to constrain models that could explain this asymmetry ðŸ”¬']",https://arxiv.org/abs/1808.06577,"CP-violation in the Higgs sector remains a possible source of the baryon asymmetry of the universe. Recent differential measurements of signed angular distributions in Higgs boson production provide a general experimental probe of the CP structure of Higgs boson interactions. We interpret these measurements using the Standard Model Effective Field Theory and show that they do not distinguish the various CP-violating operators that couple the Higgs and gauge fields. However, the constraints can be sharpened by measuring additional CP-sensitive observables and exploiting phase-space-dependent effects. Using these observables, we demonstrate that perturbatively meaningful constraints on CP-violating operators can be obtained at the LHC with luminosities of ${\cal{O}}$(100/fb). Our results provide a roadmap to a global Higgs boson coupling analysis that includes CP-violating effects. ",Angles on CP-violation in Higgs boson interactions
88,1031787400236879873,3245949691,Rebecca Leane,"['Two new papers out in collaboration with @HAWC_Obs! \nWe study the Sun as a source of gamma rays, finding:\n<LINK>: no TeV gamma-ray flux, setting limits,\n<LINK>: strong new limits on dark matter SD-scattering! <LINK>']",http://arxiv.org/abs/1808.05620,"Steady gamma-ray emission up to at least 200 GeV has been detected from the solar disk in the Fermi-LAT data, with the brightest, hardest emission occurring during solar minimum. The likely cause is hadronic cosmic rays undergoing collisions in the Sun's atmosphere after being redirected from ingoing to outgoing in magnetic fields, though the exact mechanism is not understood. An important new test of the gamma-ray production mechanism will follow from observations at higher energies. Only the High Altitude Water Cherenkov (HAWC) Observatory has the required sensitivity to effectively probe the Sun in the TeV range. Using three years of HAWC data from November 2014 to December 2017, just prior to the solar minimum, we search for 1--100 TeV gamma rays from the solar disk. No evidence of a signal is observed, and we set strong upper limits on the flux at a few $10^{-12}$ TeV$^{-1}$ cm$^{-2}$ s$^{-1}$ at 1 TeV. Our limit, which is the most constraining result on TeV gamma rays from the Sun, is $\sim10\%$ of the theoretical maximum flux (based on a model where all incoming cosmic rays produce outgoing photons), which in turn is comparable to the Fermi-LAT data near 100 GeV. The prospects for a first TeV detection of the Sun by HAWC are especially high during solar minimum, which began in early 2018. ","First HAWC Observations of the Sun Constrain Steady TeV Gamma-Ray
  Emission"
89,1030372400322543618,2876220447,Mark van der Wilk,"['Find our paper ""Learning invariances using the Marginal Likelihood"" at <LINK>!\n\nBy describing invariances using samples, we embed them in GPs and learn the invariance distribution. We only need unbiased kernel estimates, and the marginal likelihood is crucial: <LINK>']",https://arxiv.org/abs/1808.05563,"Generalising well in supervised learning tasks relies on correctly extrapolating the training data to a large region of the input space. One way to achieve this is to constrain the predictions to be invariant to transformations on the input that are known to be irrelevant (e.g. translation). Commonly, this is done through data augmentation, where the training set is enlarged by applying hand-crafted transformations to the inputs. We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by back-propagating through it to maximise the marginal likelihood. ",Learning Invariances using the Marginal Likelihood
90,1030092489154330624,103634999,Thorsten Holz,"['Our measurement study on the effect of GDPR on popular European websites is now available at <LINK>: ""We Value Your Privacy ... Now Take Some Cookies: Measuring the GDPR\'s Impact on Web Privacy""']",https://arxiv.org/abs/1808.05096,"The European Union's General Data Protection Regulation (GDPR) went into effect on May 25, 2018. Its privacy regulations apply to any service and company collecting or processing personal data in Europe. Many companies had to adjust their data handling processes, consent forms, and privacy policies to comply with the GDPR's transparency requirements. We monitored this rare event by analyzing the GDPR's impact on popular websites in all 28 member states of the European Union. For each country, we periodically examined its 500 most popular websites - 6,579 in total - for the presence of and updates to their privacy policy. While many websites already had privacy policies, we find that in some countries up to 15.7 % of websites added new privacy policies by May 25, 2018, resulting in 84.5 % of websites having privacy policies. 72.6 % of websites with existing privacy policies updated them close to the date. Most visibly, 62.1 % of websites in Europe now display cookie consent notices, 16 % more than in January 2018. These notices inform users about a site's cookie use and user tracking practices. We categorized all observed cookie consent notices and evaluated 16 common implementations with respect to their technical realization of cookie consent. Our analysis shows that core web security mechanisms such as the same-origin policy pose problems for the implementation of consent according to GDPR rules, and opting out of third-party cookies requires the third party to cooperate. Overall, we conclude that the GDPR is making the web more transparent, but there is still a lack of both functional and usable mechanisms for users to consent to or deny processing of their personal data on the Internet. ","We Value Your Privacy ... Now Take Some Cookies: Measuring the GDPR's
  Impact on Web Privacy"
91,1029036431078055943,17354555,Emilio Ferrara,"[""Our latest work w/ @adambbadawy &amp; @KristinaLerman: Who Falls for Online Political Manipulation? <LINK>\n\nFirst, we characterize the activity of Russian trolls re: 2016 US election. We also propose models to identify adopters of trolls' content and their features! <LINK>""]",https://arxiv.org/abs/1808.03281v1,"Social media, once hailed as a vehicle for democratization and the promotion of positive social change across the globe, are under attack for becoming a tool of political manipulation and spread of disinformation. A case in point is the alleged use of trolls by Russia to spread malicious content in Western elections. This paper examines the Russian interference campaign in the 2016 US presidential election on Twitter. Our aim is twofold: first, we test whether predicting users who spread trolls' content is feasible in order to gain insight on how to contain their influence in the future; second, we identify features that are most predictive of users who either intentionally or unintentionally play a vital role in spreading this malicious content. We collected a dataset with over 43 million elections-related posts shared on Twitter between September 16 and November 9, 2016, by about 5.7 million users. This dataset includes accounts associated with the Russian trolls identified by the US Congress. Proposed models are able to very accurately identify users who spread the trolls' content (average AUC score of 96%, using 10-fold validation). We show that political ideology, bot likelihood scores, and some activity-related account meta data are the most predictive features of whether a user spreads trolls' content or not. ",] Who Falls for Online Political Manipulation?
92,1027894632502439936,907593528288694272,Dirk Groenendijk,"['Transport at (111) LAO/STO interfaces occurs through two sets of electron-like sub-bands, with equal orbital character. We study the impact of this unique feature on spin-orbit coupling and superconductivity. <LINK> <LINK>']",https://arxiv.org/abs/1808.03063,"Quantum confinement at complex oxide interfaces establishes an intricate hierarchy of the strongly correlated $d$-orbitals which is widely recognized as a source of emergent physics. The most prominent example is the (001) LaAlO$_3$/SrTiO$_3$(LAO/STO) interface, which features a dome-shaped phase diagram of superconducting critical temperature and spin-orbit coupling (SOC) as a function of electrostatic doping, arising from a selective occupancy of $t_{2g}$ orbitals of different character. Here we study (111)-oriented LAO/STO interfaces - where the three $t_{2g}$ orbitals contribute equally to the sub-band states caused by confinement - and investigate the impact of this unique feature on electronic transport. We show that transport occurs through two sets of electron-like sub-bands, and the carrier density of one of the sets shows a non-monotonic dependence on the sample conductance. Using tight-binding modeling, we demonstrate that this behavior stems from a band inversion driven by on-site Coulomb interactions. The balanced contribution of all $t_{2g}$ orbitals to electronic transport is shown to result in strong SOC with reduced electrostatic modulation. ","Band inversion driven by electronic correlations at the (111)
  LaAlO$_3$/SrTiO$_3$ interface"
93,1026507002447581189,112462367,Dan Lucas,['New research submitted: <LINK>\nWe find near-wall layering in stratified plane Couette flow oriented with horizontal shear. Transition to turbulence is controlled by the intersection of the buoyancy/layer scale and the streak spacing. <LINK>'],https://arxiv.org/abs/1808.01178,"Recent research has shed light on the role of coherent structures in forming layers when stably stratified turbulence is forced with horizontal shear (Lucas, Caulfield & Kerswell, J. Fluid Mech., vol. 832, 2017, pp. 409-437). Here we extend our previous work to investigate the effect of rigid boundaries on the dynamics by studying stably-stratified plane Couette flow with gravity oriented in the spanwise direction. We observe near-wall layering and associated new mean flows in the form of large scale spanwise-flattened streamwise rolls. The layers exhibit the expected buoyancy scaling $l_z\sim U/N$ where $U$ is a typical horizontal velocity scale and $N$ the buoyancy frequency. We associate the new coherent structures with a stratified modification of the well-known large scale secondary flow in plane Couette and find that the possibility of the transition to sustained turbulence is controlled by the relative size of this buoyancy scale to the spanwise spacing of the streaks. We also investigate the influence on the transition to turbulence of the newly discovered linear instability in this system (Facchini et. al. 2018 arXiv:1711.11312). ","Layer formation and relaminarisation in plane Couette flow with spanwise
  stratification"
94,1025220939410284544,1978330974,Jacob D Biamonte,['We forecast that quantum supremacy will not be possible for gate-depths of less than 100 on systems of 80 to 150 qubits. These findings are just above the constant entanglement interval bound derived in the study. <LINK> <LINK>'],https://arxiv.org/abs/1808.00460,"A contemporary technological milestone is to build a quantum device performing a computational task beyond the capability of any classical computer, an achievement known as quantum adversarial advantage. In what ways can the entanglement realized in such a demonstration be quantified? Inspired by the area law of tensor networks, we derive an upper bound for the minimum random circuit depth needed to generate the maximal bipartite entanglement correlations between all problem variables (qubits). This bound is (i) lattice geometry dependent and (ii) makes explicit a nuance implicit in other proposals with physical consequence. The hardware itself should be able to support super-logarithmic ebits of entanglement across some poly($n$) number of qubit-bipartitions, otherwise the quantum state itself will not possess volumetric entanglement scaling and full-lattice-range correlations. Hence, as we present a connection between quantum advantage protocols and quantum entanglement, the entanglement implicitly generated by such protocols can be tested separately to further ascertain the validity of any quantum advantage claim. ",Entanglement Scaling in Quantum Advantage Benchmarks
95,1025084915573055488,736190574328467457,Aline Vidotto,"['our 2 papers @ arxiv: we studied stellar wind, planetary atmosphere, magnetism, transits, and more of the super interesting multi-planet system HD219134 (6 planets and countingâ€¦) <LINK> <LINK> you can find some of us in #coolstars20 this week <LINK>']",https://arxiv.org/abs/1808.00404,"We present a 3D study of the formation of refractory-rich exospheres around the rocky planets HD219134b and c. These exospheres are formed by surface particles that have been sputtered by the wind of the host star. The stellar wind properties are derived from magnetohydrodynamic simulations, which are driven by observationally-derived stellar magnetic field maps, and constrained by Ly-alpha observations of wind mass-loss rates, making this one of the most well constrained model of winds of low-mass stars. The proximity of the planets to their host star implies a high flux of incident stellar wind particles, thus the sputtering process is sufficiently effective to build up relatively dense, refractory-rich exospheres. The sputtering releases refractory elements from the entire dayside surfaces of the planets, with elements such as O and Mg creating an extended neutral exosphere with densities larger than 10/cm3, extending to several planetary radii. For planet b, the column density of OI along the line of sight reaches 10^{13}/cm2, with the highest values found ahead of its orbital motion. This asymmetry would create asymmetric transit profiles. To assess its observability, we use a ray tracing technique to compute the expected transit depth of the OI exosphere of planet b. We find that the transit depth in the OI 1302.2A line is 0.042%, which is a small increase relative to the continuum transit (0.036%). This implies that the sputtered exosphere of HD219134b is unlikely to be detectable with our current UV instruments. ","Characterisation of the HD219134 multi-planet system II. Stellar-wind
  sputtered exospheres in rocky planets b & c"
96,1032975075488612352,1438835354,Kalpesh Krishna,['Excited to share our #emnlp2018 paper with co-authors Preethi Jyothi and @MohitIyyer. We present a study highlighting issues of reproducibility in sentiment classification. We also show ELMo embeddings implicitly learn logic rules for this task.\xa0#NLProc <LINK>'],https://arxiv.org/abs/1808.07733,"We analyze the performance of different sentiment classification models on syntactically complex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in arXiv:1603.06318v4 [cs.LG], which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo's ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels. ","Revisiting the Importance of Encoding Logic Rules in Sentiment
  Classification"
97,1026852950688362496,373457376,Manuel J Marin-Jimenez,"['We have studied the impact, in terms of #energy, of using #GPUs for solving #ComputerVision problems by using #DeepLearning. Do you want to know the conclusions? \n""Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs"" (<LINK>)']",https://arxiv.org/abs/1808.00286,"Deep Learning (DL) applications are gaining momentum in the realm of Artificial Intelligence, particularly after GPUs have demonstrated remarkable skills for accelerating their challenging computational requirements. Within this context, Convolutional Neural Network (CNN) models constitute a representative example of success on a wide set of complex applications, particularly on datasets where the target can be represented through a hierarchy of local features of increasing semantic complexity. In most of the real scenarios, the roadmap to improve results relies on CNN settings involving brute force computation, and researchers have lately proven Nvidia GPUs to be one of the best hardware counterparts for acceleration. Our work complements those findings with an energy study on critical parameters for the deployment of CNNs on flagship image and video applications: object recognition and people identification by gait, respectively. We evaluate energy consumption on four different networks based on the two most popular ones (ResNet/AlexNet): ResNet (167 layers), a 2D CNN (15 layers), a CaffeNet (25 layers) and a ResNetIm (94 layers) using batch sizes of 64, 128 and 256, and then correlate those with speed-up and accuracy to determine optimal settings. Experimental results on a multi-GPU server endowed with twin Maxwell and twin Pascal Titan X GPUs demonstrate that energy correlates with performance and that Pascal may have up to 40% gains versus Maxwell. Larger batch sizes extend performance gains and energy savings, but we have to keep an eye on accuracy, which sometimes shows a preference for small batches. We expect this work to provide a preliminary guidance for a wide set of CNN and DL applications in modern HPC times, where the GFLOPS/w ratio constitutes the primary goal. ",Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs
