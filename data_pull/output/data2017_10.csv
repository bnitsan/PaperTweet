,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,926468319682662400,123664603,Marcelo Gleiser,['My new paper w/ D. Sowinski on applying information theory to how we make sense of the world is out.... <LINK>'],https://arxiv.org/abs/1710.09944,"Science is a constructed narrative of the natural world based on information gathering and its subsequent analysis. In this essay, we develop a novel approach to the epistemic foundations of the scientific narrative, as based on our experiential interactions with the natural world. We first review some of the basic aspects of both Bayesian statistics and Shannon's information theory as applied to the construction of meaningful conceptualization of the natural world. This conceptualization is rendered through the maps we construct of the world based on our limited knowledge of reality. We propose a path from experience to information and physics based on the notion that information is experience that induces change in an Epistemic Agent (EA): the change may be local and focused to a minor aspect of reality or it may be broad and worldview-changing. We illustrate our approach through an analysis of a measure of spatial complexity proposed by one of us called Configuration Entropy (CE), and establish a link between experience at the cognitive level and information content, showing that the CE is a quantitative measure of how much information in spatial-complexity the external world hides from an EA. ","How We Make Sense of the World: Information, Map-Making, and The
  Scientific Narrative"
1,926439388552327169,916679210,Dr. Yanjun Qi,['Our new paper on Arxiv: Fast and Scalable Learning of Sparse Changes in High-Dimensional Gaussian Graphical Model: <LINK>'],https://arxiv.org/abs/1710.11223,"We focus on the problem of estimating the change in the dependency structures of two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for sparse change estimation in GGMs involve expensive and difficult non-smooth optimization. We propose a novel method, DIFFEE for estimating DIFFerential networks via an Elementary Estimator under a high-dimensional situation. DIFFEE is solved through a faster and closed form solution that enables it to work in large-scale settings. We conduct a rigorous statistical analysis showing that surprisingly DIFFEE achieves the same asymptotic convergence rates as the state-of-the-art estimators that are much more difficult to compute. Our experimental results on multiple synthetic datasets and one real-world data about brain connectivity show strong performance improvements over baselines, as well as significant computational benefits. ","Fast and Scalable Learning of Sparse Changes in High-Dimensional
  Gaussian Graphical Model Structure"
2,925769176039297024,1861941044,Curtis Hawthorne,['Check out our new paper on state of the art automatic piano transcription! Onsets and Frames: Dual-Objective Piano Transcription\n\n<LINK> <LINK>'],https://arxiv.org/abs/1710.11153,"We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions. ",Onsets and Frames: Dual-Objective Piano Transcription
3,925738539299590146,261324356,Ray Jayawardhana,"['Our new paper, with @njcuk9999 &amp; @dalcashdvinsky, on young  brown dwarfs with @ESAGaia: <LINK> #astronomy @AAS_Publishing']",https://arxiv.org/abs/1710.11625,"Our understanding of the brown dwarf population in star forming regions is dependent on knowing distances and proper motions, and therefore will be improved through the Gaia space mission. In this paper, we select new samples of very low mass objects (VLMOs) in Upper Scorpius using UKIDSS colors and optimised proper motions calculated using Gaia DR1. The scatter in proper motions from VLMOs in Upper Scorpius is now (for the first time) dominated by the kinematic spread of the region itself, not by the positional uncertainties. With age and mass estimates updated using Gaia parallaxes for early type stars in the same region, we determine masses for all VLMOs. Our final most complete sample includes 453 VLMOs of which $\sim$125 are expected to be brown dwarfs. The cleanest sample is comprised of 131 VLMOs, with $\sim$105 brown dwarfs. We also compile a joint sample from the literature which includes 415 VLMOs, out of which 152 are likely brown dwarfs. The disc fraction among low-mass brown dwarfs ($M<0.05 M_{\odot}$) is substantially higher than in more massive objects, indicating that discs around low-mass brown dwarfs survive longer than in low-mass stars overall. The mass function for $0.01<M<0.1$ $M_{\odot}$ is consistent with the Kroupa IMF. We investigate the possibility that some 'proper motion outliers' have undergone a dynamical ejection early in their evolution. Our analysis shows that the color-magnitude cuts used when selecting samples introduce strong bias into the population statistics due to varying level of contamination and completeness. ","Very Low-Mass Stars and Brown Dwarfs in Upper Scorpius using Gaia DR1:
  Mass Function, Disks and Kinematics"
4,925488788817588224,1576235694,Michael Brown,"['.@amelia_fmc\'s new paper, ""Multiple Mechanisms Quench Passive Spiral Galaxies,"" examines some quirky Virgo galaxies. <LINK> <LINK>', '@amelia_fmc Interesting to compare the SDSS image of NGC 4440 with the deeper DECaLS DR5 image.  https://t.co/0JvSn5d6ku https://t.co/VidhpeBXWl']",https://arxiv.org/abs/1710.10843,"We examine the properties of a sample of 35 nearby passive spiral galaxies in order to determine their dominant quenching mechanism(s). All five low mass ($\textrm{M}_{\star} < 1 \times 10^{10} \textrm{M}_{\odot}$) passive spiral galaxies are located in the rich Virgo cluster. This is in contrast to low mass spiral galaxies with star formation, which inhabit a range of environments. We postulate that cluster-scale gas stripping and heating mechanisms operating only in rich clusters are required to quench low mass passive spirals, and ram-pressure stripping and strangulation are obvious candidates. For higher mass passive spirals, while trends are present, the story is less clear. The passive spiral bar fraction is high: 74$\pm$15%, compared with 36$\pm$5% for a mass, redshift, and T-type matched comparison sample of star forming spiral galaxies. The high mass passive spirals occur mostly, but not exclusively, in groups, and can be central or satellite galaxies. The passive spiral group fraction of 74$\pm$15% is similar to that of the comparison sample of star forming galaxies at 61$\pm$7%. We find evidence for both quenching via internal structure and environment in our passive spiral sample, though some galaxies have evidence of neither. From this, we conclude no one mechanism is responsible for quenching star formation in passive spiral galaxies - rather, a mixture of mechanisms are required to produce the passive spiral distribution we see today. ",Multiple Mechanisms Quench Passive Spiral Galaxies
5,925328706725347328,944436600,Jean Michel Sellier,"['RT: A new interesting paper on the use of neural networks in quantum mechanics, <LINK>']",https://arxiv.org/abs/1710.10940,"Recently a new formulation of quantum mechanics has been suggested which describes systems by means of ensembles of classical particles provided with a sign. This novel approach mainly consists of two steps: the computation of the Wigner kernel, a multi-dimensional function describing the effects of the potential over the system, and the field-less evolution of the particles which eventually create new signed particles in the process. Although this method has proved to be extremely advantageous in terms of computational resources - as a matter of fact it is able to simulate in a time-dependent fashion many- body systems on relatively small machines - the Wigner kernel can represent the bottleneck of simulations of certain systems. Moreover, storing the kernel can be another issue as the amount of memory needed is cursed by the dimensionality of the system. In this work, we introduce a new technique which drastically reduces the computation time and memory requirement to simulate time-dependent quantum systems which is based on the use of an appropriately tailored neural network combined with the signed particle formalism. In particular, the suggested neural network is able to compute efficiently and reliably the Wigner kernel without any training as its entire set of weights and biases is specified by analytical formulas. As a consequence, the amount of memory for quantum simulations radically drops since the kernel does not need to be stored anymore as it is now computed by the neural network itself, only on the cells of the (discretized) phase-space which are occupied by particles. As its is clearly shown in the final part of this paper, not only this novel approach drastically reduces the computational time, it also remains accurate. The author believes this work opens the way towards effective design of quantum devices, with incredible practical implications. ","Combining Neural Networks and Signed Particles to Simulate Quantum
  Systems More Efficiently"
6,925289294851461121,892059194240532480,Mikel Artetxe,"['Check our new paper on ""Unsupervised Neural Machine Translation"". Training NMT models with only monolingual corpora!\n<LINK> <LINK>', ""@ngutten It would be fun to see it take its own source code and output the paper! But I don't think that it would actually learn anything meaningful."", '@ngutten It should degrade smoothly for actual (natural) languages. But trying to connect C and English out of nothing sounds too crazy.']",https://arxiv.org/abs/1710.11041,"In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project. ",Unsupervised Neural Machine Translation
7,925171469587378177,913238472357437445,Fuminobu TAKAHASHI,['Our new paper appeared today on arxiv:  <LINK>  It is about the unification of inflaton and dark matter by a single ALP!'],https://arxiv.org/abs/1710.11107,"We revisit the ALP miracle scenario where the inflaton and dark matter are unified by a single axion-like particle (ALP). We first extend our previous analysis on the inflaton dynamics to identify the whole viable parameter space consistent with the CMB observation. Then, we evaluate the relic density of the ALP dark matter by incorporating uncertainties of the model-dependent couplings to the weak gauge bosons as well as the dissipation effect. The preferred ranges of the ALP mass and coupling to photons are found to be $0.01\lesssim m_\phi \lesssim 1$\,eV and $g_{\phi \gamma \gamma} = {\cal O}(10^{-11})$\,GeV$^{-1}$, which slightly depend on these uncertainties. Interestingly, the preferred regions are within reach of future solar axion helioscope experiments, IAXO and TASTE, and laser-based stim ",The ALP miracle revisited
8,925146940949405696,3433220662,Anthony Bonato,"['Oops, broken link! Here is the new Hyperopic Cop Number paper on arXiv:\n<LINK>']",https://arxiv.org/abs/1710.10112,"We introduce a new variant of the game of Cops and Robbers played on graphs, where the robber is invisible unless outside the neighbor set of a cop. The hyperopic cop number is the corresponding analogue of the cop number, and we investigate bounds and other properties of this parameter. We characterize the cop-win graphs for this variant, along with graphs with the largest possible hyperopic cop number. We analyze the cases of graphs with diameter 2 or at least 3, focusing on when the hyperopic cop number is at most one greater than the cop number. We show that for planar graphs, as with the usual cop number, the hyperopic cop number is at most 3. The hyperopic cop number is considered for countable graphs, and it is shown that for connected chains of graphs, the hyperopic cop density can be any real number in $[0,1/2].$ ",Hyperopic Cops and Robbers
9,923952066271223808,349215461,Ken Shen,"['Our new paper: double WD binaries can explain diff. host galaxy distributions of bright and faint Type Ia supernovae\n<LINK>', '2/ White dwarfs (WDs) are the endpoints for most stars; e.g., our Sun will eventually become one.', '3/ Some WDs have stellar companions that can somehow trigger a thermonuclear explosion in the WD, leading to a Type Ia supernova (SN Ia).', ""4/ But that's pretty much all we know: precisely how SNe Ia happen is still a mystery!"", '5/ SNe Ia come in a variety of shapes and sizes: e.g., the brightest ones are 10 times brighter than the faintest.', '6/ They also differ in where they occur.  The bright ones happen in young galaxies, and the faint ones in old galaxies.', '7/ In our paper, we assume SNe Ia happen in binary systems where both stars are WDs, and in a way that low-mass WDs lead to faint SNe Ia.', '8/ Then, we show that binaries with low-mass WDs explode in old stellar populations and vice versa.', '9/ So far, this is the only explanation in the literature for why the luminosity of a SN Ia is connected to its host galaxy properties.', '10/10 Our paper provides further evidence that SNe Ia are produced by double WD binaries.']",https://arxiv.org/abs/1710.09384,"Type Ia supernovae (SNe Ia) exhibit a wide diversity of peak luminosities and light curve shapes: the faintest SNe Ia are 10 times less luminous and evolve more rapidly than the brightest SNe Ia. Their differing characteristics also extend to their stellar age distributions, with fainter SNe Ia preferentially occurring in old stellar populations and vice versa. In this Letter, we quantify this SN Ia luminosity - stellar age connection using data from the Lick Observatory Supernova Search (LOSS). Our binary population synthesis calculations agree qualitatively with the observed trend in the >1 Gyr-old populations probed by LOSS if the majority of SNe Ia arise from prompt detonations of sub-Chandrasekhar mass white dwarfs (WDs) in double WD systems. Under appropriate assumptions, we show that double WD systems with less massive primaries, which yield fainter SNe Ia, interact and explode at older ages than those with more massive primaries. We find that prompt detonations in double WD systems are capable of reproducing the observed evolution of the SN Ia luminosity function, a constraint that any SN Ia progenitor scenario must confront. ",The Evolution of the Type Ia Supernova Luminosity Function
10,923828113783607297,304928205,Guido Caldarelli 贵多,['<LINK> Tackling information asymmetry in networks: a new entropy-based ranking index. A fundamental  paper by @PaoloBarucca'],https://arxiv.org/abs/1710.09656,"Information is a valuable asset for agents in socio-economic systems, a significant part of the information being entailed into the very network of connections between agents. The different interlinkages patterns that agents establish may, in fact, lead to asymmetries in the knowledge of the network structure; since this entails a different ability of quantifying relevant systemic properties (e.g. the risk of financial contagion in a network of liabilities), agents capable of providing a better estimate of (otherwise) unaccessible network properties, ultimately have a competitive advantage. In this paper, we address for the first time the issue of quantifying the information asymmetry arising from the network topology. To this aim, we define a novel index - InfoRank - intended to measure the quality of the information possessed by each node, computing the Shannon entropy of the ensemble conditioned on the node-specific information. Further, we test the performance of our novel ranking procedure in terms of the reconstruction accuracy of the (unaccessible) network structure and show that it outperforms other popular centrality measures in identifying the ""most informative"" nodes. Finally, we discuss the socio-economic implications of network information asymmetry. ","Tackling information asymmetry in networks: a new entropy-based ranking
  index"
11,923487163148357632,412548718,Kit Yates,['Our new paper on persistence in crowded environments is out on @arxiv <LINK> . Well done  @egavagninmath @MathsatBath'],http://arxiv.org/abs/1710.09264,"Persistence of motion is the tendency of an object to maintain motion in a direction for short time scales without necessarily being biased in any direction in the long term. One of the most appropriate mathematical tools to study this behaviour is an agent-based velocity-jump process. In the absence of agent-agent interaction, the mean-field continuum limit of the agent-based model (ABM) gives rise to the well known hyperbolic telegraph equation. When agent-agent interaction is included in the ABM, a strictly advective system of partial differential equations (PDEs) can be derived at the population-level. However, no diffusive limit of the ABM has been obtained from such a model. Connecting the microscopic behaviour of the ABM to a diffusive macroscopic description is desirable, since it allows the exploration of a wider range of scenarios and establishes a direct connection with commonly used statistical tools of movement analysis. In order to connect the ABM at the population-level to a diffusive PDE at the population-level, we consider a generalisation of the agent-based velocity-jump process on a two-dimensional lattice with three forms of agent interaction. This generalisation allows us to take a diffusive limit and obtain a faithful population-level description. We investigate the properties of the model at both the individual and population-level and we elucidate some of the models' key characteristic features. In particular, we show an intrinsic anisotropy inherent to the models and we find evidence of a spontaneous form of aggregation at both the micro- and macro-scales. ","Modelling persistence of motion in a crowded environment: the diffusive
  limit of excluding velocity-jump processes"
12,923475412184981504,2427184074,Christopher Berry,"['New @LIGO/@ego_virgo #GW170817 paper on the search for a postmerger signal <LINK> #nondetection <LINK>', 'Measuring the postmerger #GravitaitonalWave signal could tell us what happens to the remnant of the two neutron stars', 'Does the remnant collapse quickly to a black hole? Does is spend some time as a extra large neutron star?', ""This information would tell us about what neutron stars are made of. We can't study this stuff in the lab, so it's valuable data"", ""We didn't detect a signal, which isn't surprising—the source would need to be about 100 × closer at current sensitivity for sensible models"", ""But, if you're interested in the details of our search &amp; background on postmerger signals, this paper's for you! https://t.co/NX8qaHbT11""]",https://arxiv.org/abs/1710.09320,"The first observation of a binary neutron star coalescence by the Advanced LIGO and Advanced Virgo gravitational-wave detectors offers an unprecedented opportunity to study matter under the most extreme conditions. After such a merger, a compact remnant is left over whose nature depends primarily on the masses of the inspiralling objects and on the equation of state of nuclear matter. This could be either a black hole or a neutron star (NS), with the latter being either long-lived or too massive for stability implying delayed collapse to a black hole. Here, we present a search for gravitational waves from the remnant of the binary neutron star merger GW170817 using data from Advanced LIGO and Advanced Virgo. We search for short ($\lesssim1$ s) and intermediate-duration ($\lesssim 500$ s) signals, which includes gravitational-wave emission from a hypermassive NS or supramassive NS, respectively. We find no signal from the post-merger remnant. Our derived strain upper limits are more than an order of magnitude larger than those predicted by most models. For short signals, our best upper limit on the root-sum-square of the gravitational-wave strain emitted from 1--4 kHz is $h_{\rm rss}^{50\%}=2.1\times 10^{-22}$ Hz$^{-1/2}$ at 50% detection efficiency. For intermediate-duration signals, our best upper limit at 50% detection efficiency is $h_{\rm rss}^{50\%}=8.4\times 10^{-22}$ Hz$^{-1/2}$ for a millisecond magnetar model, and $h_{\rm rss}^{50\%}=5.9\times 10^{-22}$ Hz$^{-1/2}$ for a bar-mode model. These results indicate that post-merger emission from a similar event may be detectable when advanced detectors reach design sensitivity or with next-generation detectors. ","Search for post-merger gravitational waves from the remnant of the
  binary neutron star merger GW170817"
13,923464629438775296,341126513,Francesca Fragkoudi,"[""New paper on metallicity of MW bulge + why thick discs are important. Get it while it's hot <LINK> #shamelesspromotion""]",https://arxiv.org/abs/1710.06864,"We examine the metallicity trends in the Milky Way (MW) bulge - using APOGEE DR13 data - and explore their origin by comparing two N-body models of isolated galaxies which develop a bar and a boxy/peanut (b/p) bulge. Both models have been proposed as scenarios for reconciling a disc origin of the MW bulge with a negative vertical metallicity gradient. The first model is a superposition of co-spatial, i.e. overlapping, disc populations with different scaleheights, kinematics and metallicities. In this model the thick, metal-poor, and centrally concentrated disc populations contribute significantly to the stellar mass budget in the inner galaxy. The second model is a single disc with an initial steep radial metallicity gradient, which is mapped by the bar into the b/p bulge in such a way that the vertical metallicity gradient of the MW bulge is reproduced -- as shown already in previous works in the literature. However, as we show here, the latter model does not reproduce the positive longitudinal metallicity gradient of the inner disc, nor the metal-poor innermost regions of the Bulge seen in the data. On the other hand, the model with co-spatial thin and thick disc populations reproduces all the aforementioned trends. We therefore see that it is possible to reconcile a (primarily) disc origin for the MW bulge with the observed trends in metallicity by mapping the inner thin and thick discs of the MW into a b/p. For this scenario to reproduce the observations, the $\alpha$-enhanced, metal-poor, thick disc populations must have a significant mass contribution in the inner regions -- as has been suggested for the Milky Way. ","What the Milky Way bulge reveals about the initial metallicity gradients
  in the disc"
14,923430135260467200,112961086,Kenneth Benoit,"['""Scaling Text with the Class Affinity Model"", new paper with @ptrckprry <LINK>. It\'s Wordscores 2017. #textasdata #nlp']",https://arxiv.org/abs/1710.08963,"Probabilistic methods for classifying text form a rich tradition in machine learning and natural language processing. For many important problems, however, class prediction is uninteresting because the class is known, and instead the focus shifts to estimating latent quantities related to the text, such as affect or ideology. We focus on one such problem of interest, estimating the ideological positions of 55 Irish legislators in the 1991 D\'ail confidence vote. To solve the D\'ail scaling problem and others like it, we develop a text modeling framework that allows actors to take latent positions on a ""gray"" spectrum between ""black"" and ""white"" polar opposites. We are able to validate results from this model by measuring the influences exhibited by individual words, and we are able to quantify the uncertainty in the scaling estimates by using a sentence-level block bootstrap. Applying our method to the D\'ail debate, we are able to scale the legislators between extreme pro-government and pro-opposition in a way that reveals nuances in their speeches not captured by their votes or party affiliations. ",Scaling Text with the Class Affinity Model
15,922829031002853376,1726662073,Jean-Loup Baudino,['Proud to present our new paper with @paulmolli \n@oliviavenot et al. <LINK> : benchmark + diff between models vs JWST SNR <LINK>'],https://arxiv.org/abs/1710.08235,"Given the forthcoming launch of the James Webb Space Telescope (JWST) which will allow observ- ing exoplanet atmospheres with unprecedented signal-over-noise ratio, spectral coverage and spatial resolution, the uncertainties in the atmosphere modelling used to interpret the data need to be as- sessed. As the first step, we compare three independent 1D radiative-convective models: ATMO, Exo-REM and petitCODE. We identify differences in physical and chemical processes taken into ac- count thanks to a benchmark protocol we developed. We study the impact of these differences on the analysis of observable spectra. We show the importance of selecting carefully relevant molecular linelists to compute the atmospheric opacity. Indeed, differences between spectra calculated with Hitran and ExoMol exceed the expected uncertainties of future JWST observations. We also show the limitation in the precision of the models due to uncertainties on alkali and molecule lineshape, which induce spectral effects also larger than the expected JWST uncertainties. We compare two chemical models, Exo-REM and Venot Chemical Code, which do not lead to significant differences in the emission or transmission spectra. We discuss the observational consequences of using equilibrium or out-of- equilibrium chemistry and the major impact of phosphine, detectable with the JWST.Each of the models has benefited from the benchmarking activity and has been updated. The protocol developed in this paper and the online results can constitute a test case for other models. ","Toward the analysis of JWST exoplanet spectra: Identifying troublesome
  model parameters"
16,922638845178290176,216729597,Marcel S. Pawlowski,"['New paper: ""The Lopsidedness of Satellite Galaxy Systems in ΛCDM simulations"", with R. Ibata &amp; @jbprime \n\n<LINK> <LINK>', 'It was motivated by @satellitegalaxy\'s study ""The Lopsided Distribution of Satellite Galaxies"", see https://t.co/XABgmIEOfK', '(Which in turn was motivated by M31 satellite system being lopsided towards MW, in particular for sats making up the plane of satellites.) https://t.co/4FuagIsOYe', 'Using SDSS, @satellitegalaxy et al. found a highly significant overdensity of satellites in the direction between paired host galaxies. https://t.co/zu8qlDbf8D', 'I wondered whether that might be another problem for LCDM. So I checked with a bunch of simulations: Millennium I and II, Illustris, ELVIS.', 'Turns out, when selecting similar host galaxy pairs, we find a similar signal. Stronger, in fact, but SDSS has more foreground/background. https://t.co/f07skrvlpv', 'The effect appears to be stronger for more massive satellites, which also should be the brightest and thus make up the observed sample.', ""Strangely, orphan sats in the MS (disrupted subhalos) show 2nd over-density on the side opposite the partner primary, which isn't observed. https://t.co/9O08lzQCny"", 'Unfortunately, Illustris does not provide a large enough sample of galaxy pairs for firm conclusions.', 'So we can not tell whether the lopsidedness persists, or is strengthened/weakened due to baryonic effects. Need bigger simulation.', 'Open question remain, too: What causes the over-density? Sub-halo infall pattern, filaments connecting pairs, simply tugging by the partner?', 'Why do orphan sats behave strangely, and would going to fainter (disrupted, quenched) satellites in observations reproduce their 2nd peak?', ""So, more work needs to be done, but (for a change?) this lopsidedness of satellite systems doesn't seem like an imminent threat to LCDM."", 'Which is weird given that the sat plane problem is present in these sims. Can we trust them? Will solving the SPP break lopsidedness? \n\n/END']",https://arxiv.org/abs/1710.07639,"The spatial distribution of satellite galaxies around pairs of galaxies in the Sloan Digital Sky Survey (SDSS) have been found to bulge significantly towards the respective partner. Highly anisotropic, planar distributions of satellite galaxies are in conflict with expectations derived from cosmological simulations. Does the lopsided distribution of satellite systems around host galaxy pairs constitute a similar challenge to the standard model of cosmology? We investigate whether such satellite distributions are present around stacked pairs of hosts extracted from the $\Lambda$CDM simulations Millennium-I, Millennium-II, ELVIS, and Illustris-1. By utilizing this set of simulations covering different volumes, resolutions, and physics, we implicitly test whether a lopsided signal exists for different ranges of satellite galaxy masses, and whether the inclusion of hydrodynamical effects produces significantly different results. All simulations display a lopsidedness similar to the observed situation. The signal is highly significant for simulations containing a sufficient number of hosts and resolved satellite galaxies (up to $5\,\sigma$ for Millennium-II). We find a projected signal that is up to twice as strong as that reported for the SDSS systems for certain opening angles ($\sim16\%$ more satellites in the direction between the pair than expected for uniform distributions). Considering that the SDSS signal is a lower limit owing to likely back- and foreground contamination, the $\Lambda$CDM simulations appear to be consistent with this particular empirical property of galaxy pairs. ",The Lopsidedness of Satellite Galaxy Systems in $\Lambda$CDM simulations
17,922458080994095104,855945227718230016,Tarraneh Eftekhari,['Our new paper on late-time radio and X-ray observations of the relativistic TDE Sw 1644+57 out on the arXiv today! <LINK>'],https://arxiv.org/abs/1710.07289,"We present continued radio and X-ray observations of the relativistic tidal disruption event Swift J164449.3+573451 extending to $\delta t \approx 2000$ d after discovery. The radio data were obtained with the VLA as part of a long-term program to monitor the energy and dynamical evolution of the relativistic jet and to characterize the parsec-scale environment around a previously dormant supermassive black hole. We combine these data with $\textit{Chandra}$ X-ray observations and demonstrate that the X-ray emission following the sharp decline at $\delta t \approx 500$ d is due to the forward shock. Using the X-ray data, in conjunction with optical/NIR data, we constrain the synchrotron cooling frequency and the microphysical properties of the outflow for the first time. We find that the cooling frequency evolves through the optical/NIR band at $\delta t \approx 10 - 200$ d, corresponding to a magnetic field energy density fraction of $\epsilon_B \approx 10^{-3}$, well below equipartition; the X-ray data demonstrate that this deviation from equipartition holds to at least $\delta t \approx 2000$ d. We thus recalculate the physical properties of the jet over the lifetime of the event, no longer assuming equipartition. We find a total kinetic energy of $E_K \approx 4 \times 10^{51}$ erg and a transition to non-relativistic expansion on the timescale of our latest observations ($\delta t \approx 700$ d). The density profile is approximately $R^{-3/2}$ at $\lesssim 0.3$ pc and $\gtrsim 0.7$ pc, with a plateau at intermediate scales, characteristic of Bondi accretion. Based on its evolution thus far, we predict that Sw 1644+57 will be detectable at centimeter wavelengths for decades to centuries with existing and upcoming radio facilities. Similar off-axis events should be detectable to $z \sim 2$, but with a slow evolution that may inhibit their recognition as transient events. ","Radio Monitoring of the Tidal Disruption Event Swift J164449.3+573451.
  III. Late-time Jet Energetics and a Deviation from Equipartition"
18,922454378438676480,2766925212,Andrew Childs,"['New paper: Automated optimization of large quantum circuits with Nam, Ross, @yuansu_umd, &amp; Maslov <LINK> @JointQuICS']",http://arxiv.org/abs/1710.07345,"We develop and implement automated methods for optimizing quantum circuits of the size and type expected in quantum computations that outperform classical computers. We show how to handle continuous gate parameters and report a collection of fast algorithms capable of optimizing large-scale quantum circuits. For the suite of benchmarks considered, we obtain substantial reductions in gate counts. In particular, we provide better optimization in significantly less time than previous approaches, while making minimal structural changes so as to preserve the basic layout of the underlying quantum algorithms. Our results help bridge the gap between the computations that can be run on existing hardware and those that are expected to outperform classical computers. ","Automated optimization of large quantum circuits with continuous
  parameters"
19,922444902692110336,869876111336914944,Nana Liu,['Our new paper out today on arxiv! Anomaly detection of quantum states meets quantum machine learning! <LINK>'],https://arxiv.org/abs/1710.07405,"Anomaly detection is used for identifying data that deviate from `normal' data patterns. Its usage on classical data finds diverse applications in many important areas like fraud detection, medical diagnoses, data cleaning and surveillance. With the advent of quantum technologies, anomaly detection of quantum data, in the form of quantum states, may become an important component of quantum applications. Machine learning algorithms are playing pivotal roles in anomaly detection using classical data. Two widely-used algorithms are kernel principal component analysis and one-class support vector machine. We find corresponding quantum algorithms to detect anomalies in quantum states. We show that these two quantum algorithms can be performed using resources logarithmic in the dimensionality of quantum states. For pure quantum states, these resources can also be logarithmic in the number of quantum states used for training the machine learning algorithm. This makes these algorithms potentially applicable to big quantum data applications. ",Quantum machine learning for quantum anomaly detection
20,922261555579727873,2337598033,Geraint F. Lewis,['Brand new paper on the arXiv <LINK> <LINK>'],https://arxiv.org/abs/1710.07377,"Linearly polarized emission is described, in general, in terms of the Stokes parameters $Q$ and $U$, from which the polarization intensity and polarization angle can be determined. Although the polarization intensity and polarization angle provide an intuitive description of the polarization, they are affected by the limitations of interferometric data, such as missing single-dish data in the u-v plane, from which radio frequency interferometric data is visualized. To negate the effects of these artefacts, it is desirable for polarization diagnostics to be rotationally and translationally invariant in the $Q$-$U$ plane. One rotationally and translationally invariant quantity, the polarization gradient, has been shown to provide a unique view of spatial variations in the turbulent interstellar medium when applied to diffuse radio frequency synchrotron emission. In this paper we develop a formalism to derive additional rotationally and translationally invariant quantities. We present new diagnostics that can be applied to diffuse or point-like polarized emission in any waveband, including a generalization of the polarization gradient, the polarization directional curvature, polarization wavelength derivative, and polarization wavelength curvature. In Paper II we will apply these diagnostics to observed and simulated images of diffuse radio frequency synchrotron emission. ","Advanced Diagnostics for the Study of Linearly Polarized Emission. I:
  Derivation"
21,921378171328647168,2423945684,Vinny Davies,['New paper is now available on @arxiv Another paper based on my PhD work <LINK> <LINK>'],https://arxiv.org/abs/1710.06366,"Understanding how genetic changes allow emerging virus strains to escape the protection afforded by vaccination is vital for the maintenance of effective vaccines. In the current work, we use structural and phylogenetic differences between pairs of virus strains to identify important antigenic sites on the surface of the influenza A(H1N1) virus through the prediction of haemagglutination inhibition (HI) assay, pairwise measures of the antigenic similarity of virus strains. We propose a sparse hierarchical Bayesian model that can deal with the pairwise structure and inherent experimental variability in the H1N1 data through the introduction of latent variables. The latent variables represent the underlying HI assay measurement of any given pair of virus strains and help account for the fact that for any HI assay measurement between the same pair of virus strains, the difference in the viral sequence remains the same. Through accurately representing the structure of the H1N1 data, the model is able to select virus sites which are antigenic, while its latent structure achieves the computational efficiency required to deal with large virus sequence data, as typically available for the influenza virus. In addition to the latent variable model, we also propose a new method, block integrated Widely Applicable Information Criterion (biWAIC), for selecting between competing models. We show how this allows us to effectively select the random effects when used with the proposed model and apply both methods to an A(H1N1) dataset. ","Improving the identification of antigenic sites in the H1N1 Influenza
  virus through accounting for the experimental structure in a sparse
  hierarchical Bayesian model"
22,921274408437772288,2337598033,Geraint F. Lewis,['New paper on the arxiv: Just what does dark energy do to the universe? <LINK> <LINK>'],https://arxiv.org/abs/1710.06861,"We investigate the effect of the accelerated expansion of the Universe due to a cosmological constant, $\Lambda$, on the cosmic star formation rate. We utilise hydrodynamical simulations from the EAGLE suite, comparing a $\Lambda$CDM Universe to an Einstein-de Sitter model with $\Lambda=0$. Despite the differences in the rate of growth of structure, we find that dark energy, at its observed value, has negligible impact on star formation in the Universe. We study these effects beyond the present day by allowing the simulations to run forward into the future ($t>13.8$ Gyr). We show that the impact of $\Lambda$ becomes significant only when the Universe has already produced most of its stellar mass, only decreasing the total co-moving density of stars ever formed by ${\approx}15\%$. We develop a simple analytic model for the cosmic star formation rate that captures the suppression due to a cosmological constant. The main reason for the similarity between the models is that feedback from accreting black holes dramatically reduces the cosmic star formation at late times. Interestingly, simulations without feedback from accreting black holes predict an upturn in the cosmic star formation rate for $t>15$ Gyr due to the rejuvenation of massive ($ > 10^{11} \mathrm{M}_{\odot}$) galaxies. We briefly discuss the implication of the weak dependence of the cosmic star formation on $\Lambda$ in the context of the anthropic principle. ","The impact of dark energy on galaxy formation. What does the future of
  our Universe hold?"
23,921015383586492416,180082519,Devin Silvia,"['Interested in learning more about galactic chemical evolution, check out our new paper: <LINK> Lead by @BenoitCt2!']",https://arxiv.org/abs/1710.06442,"We use a cosmological hydrodynamic simulation calculated with Enzo and the semi-analytic galaxy formation model (SAM) GAMMA to address the chemical evolution of dwarf galaxies in the early universe. The long-term goal of the project is to better understand the origin of metal-poor stars and the formation of dwarf galaxies and the Milky Way halo by cross-validating these theoretical approaches. We combine GAMMA with the merger tree of the most massive galaxy found in the hydrodynamic simulation and compare the star formation rate, the metallicity distribution function (MDF), and the age-metallicity relationship predicted by the two approaches. We found that the SAM can reproduce the global trends of the hydrodynamic simulation. However, there are degeneracies between the model parameters and more constraints (e.g., star formation efficiency, gas flows) need to be extracted from the simulation to isolate the correct semi-analytic solution. Stochastic processes such as bursty star formation histories and star formation triggered by supernova explosions cannot be reproduced by the current version of GAMMA. Non-uniform mixing in the galaxy's interstellar medium, coming primarily from self-enrichment by local supernovae, causes a broadening in the MDF that can be emulated in the SAM by convolving its predicted MDF with a Gaussian function having a standard deviation of ~0.2 dex. We found that the most massive galaxy in the simulation retains nearby 100% of its baryonic mass within its virial radius, which is in agreement with what is needed in GAMMA to reproduce the global trends of the simulation. ","Validating Semi-Analytic Models of High-Redshift Galaxy Formation using
  Radiation Hydrodynamical Simulations"
24,920947249869475840,60995841,Federica Tarsitano,['Check out our new #DES paper on #NGC4993 host galaxy of #GW170814 #GravitationalWaves event @des_portal @EllaPalmes \n<LINK>'],https://arxiv.org/abs/1710.06748,"We present a study of NGC 4993, the host galaxy of the GW170817 gravitational wave event, the GRB170817A short gamma-ray burst (sGRB) and the AT2017gfo kilonova. We use Dark Energy Camera imaging, AAT spectra and publicly available data, relating our findings to binary neutron star (BNS) formation scenarios and merger delay timescales. NGC4993 is a nearby (40 Mpc) early-type galaxy, with $i$-band S\'ersic index $n=4.0$ and low asymmetry ($A=0.04\pm 0.01$). These properties are unusual for sGRB hosts. However, NGC4993 presents shell-like structures and dust lanes indicative of a recent galaxy merger, with the optical transient located close to a shell. We constrain the star formation history (SFH) of the galaxy assuming that the galaxy merger produced a star formation burst, but find little to no on-going star formation in either spatially-resolved broadband SED or spectral fitting. We use the best-fit SFH to estimate the BNS merger rate in this type of galaxy, as $R_{NSM}^{gal}= 5.7^{+0.57}_{-3.3} \times 10^{-6} {\rm yr}^{-1}$. If star formation is the only considered BNS formation scenario, the expected number of BNS mergers from early-type galaxies detectable with LIGO during its first two observing seasons is $0.038^{+0.004}_{-0.022}$, as opposed to $\sim 0.5$ from all galaxy types. Hypothesizing that the binary system formed due to dynamical interactions during the galaxy merger, the subsequent time elapsed can constrain the delay time of the BNS coalescence. By using velocity dispersion estimates and the position of the shells, we find that the galaxy merger occurred $t_{\rm mer}\lesssim 200~{\rm Myr}$ prior to the BNS coalescence. ","Evidence for Dynamically Driven Formation of the GW170817 Neutron Star
  Binary in NGC 4993"
25,920556311154176000,561899047,Aki Vehtari,"['New paper with @JuhoPiironen ""Iterative Supervised Principal Components"" proposes a variable screening and dimension reduction method, which provides good results on several microarray benchmark datasets. <LINK> #AaltoPML <LINK>']",https://arxiv.org/abs/1710.06229,"In high-dimensional prediction problems, where the number of features may greatly exceed the number of training instances, fully Bayesian approach with a sparsifying prior is known to produce good results but is computationally challenging. To alleviate this computational burden, we propose to use a preprocessing step where we first apply a dimension reduction to the original data to reduce the number of features to something that is computationally conveniently handled by Bayesian methods. To do this, we propose a new dimension reduction technique, called iterative supervised principal components (ISPC), which combines variable screening and dimension reduction and can be considered as an extension to the existing technique of supervised principal components (SPCs). Our empirical evaluations confirm that, although not foolproof, the proposed approach provides very good results on several microarray benchmark datasets with very affordable computation time, and can also be very useful for visualizing high-dimensional data. ",Iterative Supervised Principal Components
26,920420632663277568,334628959,Roberto Calandra,"['Our new paper about the importance of tactile sensing in robot grasping, using deep visuo-tactile models. <LINK>']",http://arxiv.org/abs/1710.05512,"A successful grasp requires careful balancing of the contact forces. Deducing whether a particular grasp will be successful from indirect measurements, such as vision, is therefore quite challenging, and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping. However, in order to fully evaluate the value of touch sensing for grasp outcome prediction, we must understand how touch sensing can influence outcome prediction accuracy when combined with other modalities. Doing so using conventional model-based techniques is exceptionally difficult. In this work, we investigate the question of whether touch sensing aids in predicting grasp outcomes within a multimodal sensing framework that combines vision and touch. To that end, we collected more than 9,000 grasping trials using a two-finger gripper equipped with GelSight high-resolution tactile sensors on each finger, and evaluated visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually, and from both modalities together. Our experimental results indicate that incorporating tactile readings substantially improve grasping performance. ",The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?
27,920323321182457856,24034791,Cody Hyndman,"['New paper on ""Geometric Learning and Filtering in Finance"" with Anastasis Kratsios appeared on arXiv yesterday.\n<LINK> <LINK>']",https://arxiv.org/abs/1710.05829,A non-Euclidean generalization of conditional expectation is introduced and characterized as the minimizer of expected intrinsic squared-distance from a manifold-valued target. The computational tractable formulation expresses the non-convex optimization problem as transformations of Euclidean conditional expectation. This gives computationally tractable filtering equations for the dynamics of the intrinsic conditional expectation of a manifold-valued signal and is used to obtain accurate numerical forecasts of efficient portfolios by incorporating their geometric structure into the estimates. ,Non-Euclidean Conditional Expectation and Filtering
28,920322608347959296,24034791,Cody Hyndman,"['New paper on ""Arbitrage-Free Regularization"" with Anastasis Kratsios appeared on arXiv yesterday.\n<LINK> <LINK>']",https://arxiv.org/abs/1710.05114,"We introduce a regularization approach to arbitrage-free factor-model selection. The considered model selection problem seeks to learn the closest arbitrage-free HJM-type model to any prespecified factor-model. An asymptotic solution to this, a priori computationally intractable, problem is represented as the limit of a 1-parameter family of optimizers to computationally tractable model selection tasks. Each of these simplified model-selection tasks seeks to learn the most similar model, to the prescribed factor-model, subject to a penalty detecting when the reference measure is a local martingale-measure for the entire underlying financial market. A simple expression for the penalty terms is obtained in the bond market withing the affine-term structure setting, and it is used to formulate a deep-learning approach to arbitrage-free affine term-structure modelling. Numerical implementations are also performed to evaluate the performance in the bond market. ","Deep Learning in a Generalized HJM-type Framework Through Arbitrage-Free
  Regularization"
29,920298545495801856,706175245976248321,Oscar Viyuela,"['Our new paper on the arXiv: ""Staircase to Higher-Order Topological Phase Transitions"" 🎊\n<LINK>']",https://arxiv.org/abs/1710.05691,"We find a series of topological phase transitions of increasing order, beyond the more standard second-order phase transition in a one-dimensional topological superconductor. The jumps in the order of the transitions depend on the range of the pairing interaction, which is parametrized by an algebraic decay with exponent $\alpha$. Remarkably, in the limit $\alpha = 1$ the order of the topological transition becomes infinite. We compute the critical exponents for the series of higher-order transitions in exact form and find that they fulfill the hyperscaling relation. We also study the critical behaviour at the boundary of the system and discuss potential experimental platforms of magnetic atoms in superconductors. ",Staircase to Higher-Order Topological Phase Transitions
30,920236126186692615,953616889,Justin Read,['Merging supermassive black holes reshape their host galaxies. Great new paper by @astr0eli! --&gt; expect lots of GWs!\n\n<LINK> <LINK>'],https://arxiv.org/abs/1710.04658,"Massive black hole (MBH) binaries, formed as a result of galaxy mergers, are expected to harden by dynamical friction and three-body stellar scatterings, until emission of gravitational waves (GWs) leads to their final coalescence. According to recent simulations, MBH binaries can efficiently harden via stellar encounters only when the host geometry is triaxial, even if only modestly, as angular momentum diffusion allows an efficient repopulation of the binary loss cone. In this paper, we carry out a suite of N-body simulations of equal-mass galaxy collisions, varying the initial orbits and density profiles for the merging galaxies and running simulations both with and without central MBHs. We find that the presence of an MBH binary in the remnant makes the system nearly oblate, aligned with the galaxy merger plane, within a radius enclosing 100 MBH masses. We never find binary hosts to be prolate on any scale. The decaying MBHs slightly enhance the tangential anisotropy in the centre of the remnant due to angular momentum injection and the slingshot ejection of stars on nearly radial orbits. This latter effect results in about 1% of the remnant stars being expelled from the galactic nucleus. Finally, we do not find any strong connection between the remnant morphology and the binary hardening rate, which depends only on the inner density slope of the remnant galaxy. Our results suggest that MBH binaries are able to coalesce within a few Gyr, even if the binary is found to partially erase the merger-induced triaxiality from the remnant. ","The influence of Massive Black Hole Binaries on the Morphology of Merger
  Remnants"
31,919981210549604352,561899047,Aki Vehtari,"['New paper ""User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction"" with Pedram Daee, Tomi Peltola and @samikaski #AaltoPML <LINK> <LINK>']",https://arxiv.org/abs/1710.04881,"In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human--machine interaction; however, fewer studies have addressed the potential defects the designs can cause. Effective interaction often requires exposing the user to the training data or its statistics. The design of the system is then critical, as this can lead to double use of data and overfitting, if the user reinforces noisy patterns in the data. We propose a user modelling methodology, by assuming simple rational behaviour, to correct the problem. We show, in a user study with 48 participants, that the method improves predictive performance in a sparse linear regression sentiment analysis task, where graded user knowledge on feature relevance is elicited. We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning. ","User Modelling for Avoiding Overfitting in Interactive Knowledge
  Elicitation for Prediction"
32,919810361074835456,40639812,Colin Cotter,['New paper on the arXiv on the stochastic QG equation. <LINK>'],https://arxiv.org/abs/1710.04845,"A framework of variational principles for stochastic fluid dynamics was presented by Holm (2015), and these stochastic equations were also derived by Cotter et al. (2017). We present a conforming finite element discretisation for the stochastic quasi-geostrophic equation that was derived from this framework. The discretisation preserves the first two moments of potential vorticity, i.e. the mean potential vorticity and the enstrophy. Following the work of Dubinkina and Frank (2007), who investigated the statistical mechanics of discretisations of the deterministic quasi-geostrophic equation, we investigate the statistical mechanics of our discretisation of the stochastic quasi-geostrophic equation. We compare the statistical properties of our discretisation with the Gibbs distribution under assumption of these conserved quantities, finding that there is agreement between the statistics under a wide range of set-ups. ","Statistical properties of an enstrophy conserving discretisation for the
  stochastic quasi-geostrophic equation"
33,919760509489967104,141440459,Rod Van Meter 🌻,['New #quantum #networkcoding paper is up!\n<LINK>'],https://arxiv.org/abs/1710.04827,"Quantum network coding is an effective solution for alleviating bottlenecks in quantum networks. We introduce a measurement-based quantum network coding scheme for quantum repeater networks (MQNC), and analyze its behavior based on results acquired from Monte-Carlo simulation that includes various error sources over a butterfly network. By exploiting measurement-based quantum computing, operation on qubits for completing network coding proceeds in parallel. We show that such an approach offers advantages over other schemes in terms of the quantum circuit depth, and therefore improves the communication fidelity without disturbing the aggregate throughput. The circuit depth of our protocol has been reduced by 56.5% compared to the quantum network coding scheme (QNC) introduced in 2012 by Satoh, et al. For MQNC, we have found that the resulting entangled pairs' joint fidelity drops below 50% when the accuracy of local operations is lower than 98.9%, assuming that all initial Bell pairs across quantum repeaters have a fixed fidelity of 98%. Overall, MQNC showed substantially higher error tolerance compared to QNC and slightly better than buffer space multiplexing using step-by-step entanglement swapping, but not quite as strong as simultaneous entanglement swapping operations. ","Analysis of Measurement-based Quantum Network Coding over Repeater
  Networks under Noisy Conditions"
34,918439875774418945,62604839,Paulo Simões,"['Our new @fchroma paper! Hα and Hβ emission in a C3.3 solar flare: comparison between observations and simulations <LINK>', '@WhisperinJay @fchroma thanks!', ""@WhisperinJay @fchroma excuse my ignorance: what's the ESPO seminar? ;)"", ""@WhisperinJay @fchroma completely forgot about that! thanks! I saw Malcolm's talk at @espm_15 and read his paper! Indeed, it's on a similar subject to ours""]",https://arxiv.org/abs/1710.04067,"The Hydrogen Balmer series is a basic radiative loss channel from the flaring solar chromosphere. We report here on the analysis of an extremely rare set of simultaneous observations of a solar flare in the H${\alpha}$ and H${\beta}$ lines at high spatial and temporal resolution, which were acquired at the Dunn Solar Telescope. Images of the C3.3 flare (SOL2014-04-22T15:22) made at various wavelengths along the H${\alpha}$ line profile by the Interferometric Bidimensional Spectrometer (IBIS) and in the H${\beta}$ with the Rapid Oscillations in the Solar Atmosphere (ROSA) broadband imager are analyzed to obtain the intensity evolution. The H${\alpha}$ and H${\beta}$ intensity excesses in three identified flare footpoints are well correlated in time. We examine the ratio of H${\alpha}$ to H${\beta}$ flare excess, which was proposed by previous authors as a possible diagnostic of the level of electron beam energy input. In the stronger footpoints, the typical value of the the H${\alpha}$/H${\beta}$ intensity ratio observed is $\sim 0.4-0.5$, in broad agreement with values obtained from a RADYN non-LTE simulation driven by an electron beam with parameters constrained (as far as possible) by observation. The weaker footpoint has a larger H${\alpha}$/H${\beta}$ ratio, again consistent with a RADYN simulation but with a smaller energy flux. The H${\alpha}$ line profiles observed have a less prominent central reversal than is predicted by the RADYN results, but can be brought into agreement if the H${\alpha}$-emitting material has a filling factor of around 0.2--0.3. ","H${\alpha}$ and H${\beta}$ emission in a C3.3 solar flare: comparison
  between observations and simulations"
35,917957367790231552,1576235694,Michael Brown,['Michelle Cluver has a new paper on calibrating WISE star formation rates with total infrared luminosity. #astronomy <LINK> <LINK>'],https://arxiv.org/abs/1710.03469,"We present accurate resolved $WISE$ photometry of galaxies in the combined SINGS and KINGFISH sample. The luminosities in the W3 12$\mu$m and W4 23$\mu$m bands are calibrated to star formation rates (SFRs) derived using the total infrared luminosity, avoiding UV/optical uncertainties due to dust extinction corrections. The W3 relation has a 1-$\sigma$ scatter of 0.15 dex over nearly 5 orders of magnitude in SFR and 12$\mu$m luminosity, and a range in host stellar mass from dwarf (10$^7$ M$_\odot$) to $\sim3\times$M$_\star$ (10$^{11.5}$ M$_\odot$) galaxies. In the absence of deep silicate absorption features and powerful active galactic nuclei, we expect this to be a reliable SFR indicator chiefly due to the broad nature of the W3 band. By contrast the W4 SFR relation shows more scatter (1-$\sigma =$ 0.18 dex). Both relations show reasonable agreement with radio continuum-derived SFRs and excellent accordance with so-called ""hybrid"" H$\alpha + 24 \mu$m and FUV$+$24$\mu$m indicators. Moreover, the $WISE$ SFR relations appear to be insensitive to the metallicity range in the sample. We also compare our results with IRAS-selected luminous infrared galaxies, showing that the $WISE$ relations maintain concordance, but systematically deviate for the most extreme galaxies. Given the all-sky coverage of $WISE$ and the performance of the W3 band as a SFR indicator, the $L_{12\mu \rm m}$ SFR relation could be of great use to studies of nearby galaxies and forthcoming large area surveys at optical and radio wavelengths. ",Calibrating Star Formation in WISE using Total Infrared Luminosity
36,917808891932549120,307826617,Kev Abazajian ⤷⏳🌎,['New paper w/ on the γ-Ray GCE &amp; dim dwarfs tension in dark matter interpretations. Led by  #UCIrvine GSR R. Keeley. <LINK> <LINK>'],https://arxiv.org/abs/1710.03215,"The Milky Way's Galactic Center harbors a gamma-ray excess that is a candidate signal of annihilating dark matter. Dwarf galaxies remain predominantly dark in their expected commensurate emission. In this work we quantify the degree of consistency between these two observations through a joint likelihood analysis. In doing so we incorporate Milky Way dark matter halo profile uncertainties, as well as an accounting of diffuse gamma-ray emission uncertainties in dark matter annihilation models for the Galactic Center Extended gamma-ray excess (GCE) detected by the Fermi Gamma-Ray Space Telescope. The preferred range of annihilation rates and masses expands when including these unknowns. Even so, using two recent determinations of the Milky Way halo's local density leave the GCE preferred region of single-channel dark matter annihilation models to be in strong tension with annihilation searches in combined dwarf galaxy analyses. A third, higher Milky Way density determination, alleviates this tension. Our joint likelihood analysis allows us to quantify this inconsistency. We provide a set of tools for testing dark matter annihilation models' consistency within this combined dataset. As an example, we test a representative inverse Compton sourced self-interacting dark matter model, which is consistent with both the GCE and dwarfs. ","What the Milky Way's Dwarfs tell us about the Galactic Center extended
  excess"
37,917395333440770053,2424151074,Olaf Sporns,['Multiresolution Consensus Clustering | <LINK> | new methods paper with Lucas Leub &amp; @santo_fortunato <LINK>'],https://arxiv.org/abs/1710.02249,"Networks often exhibit structure at disparate scales. We propose a method for identifying community structure at different scales based on multiresolution modularity and consensus clustering. Our contribution consists of two parts. First, we propose a strategy for sampling the entire range of possible resolutions for the multiresolution modularity quality function. Our approach is directly based on the properties of modularity and, in particular, provides a natural way of avoiding the need to increase the resolution parameter by several orders of magnitude to break a few remaining small communities, necessitating the introduction of ad-hoc limits to the resolution range with standard sampling approaches. Second, we propose a hierarchical consensus clustering procedure, based on a modified modularity, that allows one to construct a hierarchical consensus structure given a set of input partitions. While here we are interested in its application to partitions sampled using multiresolution modularity, this consensus clustering procedure can be applied to the output of any clustering algorithm. As such, we see many potential applications of the individual parts of our multiresolution consensus clustering procedure in addition to using the procedure itself to identify hierarchical structure in networks. ",Multiresolution Consensus Clustering in Networks
38,916140978355920897,1558538456,Rodrigo Fernández,"['How much mass is ejected in failed supernovae for various progenitors?\n\nNew paper:\n\n<LINK>', 'Inlists for presupernova MESA models used are available here:\n\nhttps://t.co/NUfRl3VdyN\n\n(my first public repo!)', 'Also, companion paper by Eric Coughlin on the physics of how the sound pulse / shock forms in failed supernovae\n\nhttps://t.co/JE6glDE25e']",https://arxiv.org/abs/1710.01735,"We study the ejection of mass during stellar core-collapse when the stalled shock does not revive and a black hole forms. Neutrino emission during the protoneutron star phase causes a decrease in the gravitational mass of the core, resulting in an outward going sound pulse that steepens into a shock as it travels out through the star. We explore the properties of this mass ejection mechanism over a range of stellar progenitors using spherically-symmetric, time-dependent hydrodynamic simulations that treat neutrino mass loss parametrically and follow the shock propagation over the entire star. We find that all types of stellar progenitor can eject mass through this mechanism. The ejected mass is a decreasing function of the surface gravity of the star, ranging from several $M_\odot$ for red supergiants to $\sim 0.1M_\odot$ for blue supergiants and $\sim 10^{-3} M_\odot $ for Wolf-Rayet stars. We find that the final shock energy at the surface is a decreasing function of the core-compactness, and is $\lesssim 10^{47}-10^{48}$ erg in all cases. In progenitors with a sufficiently large envelope, high core-compactness, or a combination of both, the sound pulse fails to unbind mass. Successful mass ejection is accompanied by significant fallback accretion that can last from hours to years. We predict the properties of shock breakout and thermal plateau emission produced by the ejection of the outer envelope of blue supergiant and Wolf-Rayet progenitors in otherwise failed supernovae. ",Mass Ejection in Failed Supernovae: Variation with Stellar Progenitor
39,915936228159229954,702241209276829697,Cecilia Garraffo 💚,['Check our new paper about the role of the stellar wind on the protoplanetary disk ionization by energetic particles.\n<LINK>'],https://arxiv.org/abs/1710.01323,"The evolution of protoplanetary disks is believed to be driven largely by angular momentum transport resulting from magnetized disk winds and turbulent viscosity. The ionization of the disk that is essential for these processes has been thought due to host star coronal X-rays but could also arise from energetic particles produced by coronal flares or by travelling shock waves and advected by the stellar wind. We have performed test-particle numerical simulations of energetic protons propagating into a realistic T~Tauri stellar wind, including a superposed small-scale magnetostatic turbulence. The isotropic (Kolmogorov power spectrum) turbulent component is synthesised along the individual particle trajectories. We have investigated the energy range $[0.1 - 10]$ GeV, consistent with expectations from {\it Chandra} X-ray observations of large flares on T~Tauri stars and with recent indications by the {\it Herschel} Space Observatory of a significant contribution of energetic particles to the disk ionization of young stars. In contrast with a previous theoretical study finding dominance of energetic particles over X-ray in the ionization throughout the disk, we find that the disk ionization is likely dominated by X-rays over much of its area except within narrow regions where particles are channeled onto the disk by the strongly-tangled and turbulent magnetic field. The radial thickness of such regions is $\sim 5$ stellar radii close to the star and broadens with increasing radial distance. This likely continues out to large distances from the star ($10$ AU or greater) where particles can be copiously advected and diffused by the turbulent wind. ","Mottled protoplanetary disk ionization by magnetically-channeled T Tauri
  star energetic particles"
40,915799631535026176,1359666092,Kaustuv Datta,"['Shameless self-promotion, new paper on ""Novel Jet Observables from Machine Learning"" with Andrew Larkoski\n\n<LINK>']",https://arxiv.org/abs/1710.01305,"Previous studies have demonstrated the utility and applicability of machine learning techniques to jet physics. In this paper, we construct new observables for the discrimination of jets from different originating particles exclusively from information identified by the machine. The approach we propose is to first organize information in the jet by resolved phase space and determine the effective $N$-body phase space at which discrimination power saturates. This then allows for the construction of a discrimination observable from the $N$-body phase space coordinates. A general form of this observable can be expressed with numerous parameters that are chosen so that the observable maximizes the signal vs.~background likelihood. Here, we illustrate this technique applied to discrimination of $H\to b\bar b$ decays from massive $g\to b\bar b$ splittings. We show that for a simple parametrization, we can construct an observable that has discrimination power comparable to, or better than, widely-used observables motivated from theory considerations. For the case of jets on which modified mass-drop tagger grooming is applied, the observable that the machine learns is essentially the angle of the dominant gluon emission off of the $b\bar b$ pair. ",Novel Jet Observables from Machine Learning
41,915750579007344640,2562351306,Alexander Engelen,['With nearby galaxies we can learn far more about the reionization epoch than with the CMB alone - our new paper  <LINK>'],https://arxiv.org/abs/1710.01708,"Upcoming cosmic microwave background (CMB) surveys will soon make the first detection of the polarized Sunyaev-Zel'dovich effect, the linear polarization generated by the scattering of CMB photons on the free electrons present in collapsed objects. Measurement of this polarization along with knowledge of the electron density of the objects allows a determination of the quadrupolar temperature anisotropy of the CMB as viewed from the space-time location of the objects. Maps of these remote temperature quadrupoles have several cosmological applications. Here we propose a new application: reconstruction of the cosmological reionization history. We show that with quadrupole measurements out to redshift 3, constraints on the mean optical depth can be improved by an order of magnitude beyond the CMB cosmic variance limit. ","Beyond CMB cosmic variance limits on reionization with the polarized SZ
  effect"
42,915650799971323904,192826908,Jorge Lillo-Box,['The first TROY paper is out now! The new hunt for co-orbital planets has started. Check it out at: <LINK>'],https://arxiv.org/abs/1710.01138,"The detection of Earth-like planets, exocomets or Kuiper belts show that the different components found in the solar system should also be present in other planetary systems. Trojans are one of these components and can be considered fossils of the first stages in the life of planetary systems. Their detection in extrasolar systems would open a new scientific window to investigate formation and migration processes. In this context, the main goal of the TROY project is to detect exotrojans for the first time and to measure their occurrence rate (eta-Trojan). In this first paper, we describe the goals and methodology of the project. Additionally, we used archival radial velocity data of 46 planetary systems to place upper limits on the mass of possible trojans and investigate the presence of co-orbital planets down to several tens of Earth masses. We used archival radial velocity data of 46 close-in (P<5 days) transiting planets (without detected companions) with information from high-precision radial velocity instruments. We took advantage of the time of mid-transit and secondary eclipses (when available) to constrain the possible presence of additional objects co-orbiting the star along with the planet. This, together with a good phase coverage, breaks the degeneracy between a trojan planet signature and signals coming from additional planets or underestimated eccentricity. We identify nine systems for which the archival data provide 1-sigma evidence for a mass imbalance between L4 and L5. Two of these systems provide 2-sigma detection, but no significant detection is found among our sample. We also report upper limits to the masses at L4/L5 in all studied systems and discuss the results in the context of previous findings. ","The TROY project: Searching for co-orbital bodies to known planets. I.
  Project goals and first results from archival radial velocity"
43,915195959029780480,131132279,Jean-Luc Thiffeault,"[""Our new paper: What's the optimal way to cool a fluid?  How is this related to exit times for Brownian particles?\n<LINK> <LINK>""]",http://arxiv.org/abs/1710.00646,"A heat exchanger can be modeled as a closed domain containing an incompressible fluid. The moving fluid has a temperature distribution obeying the advection-diffusion equation, with zero temperature boundary conditions at the walls. Starting from a positive initial temperature distribution in the interior, the goal is to flux the heat through the walls as efficiently as possible. Here we consider a distinct but closely related problem, that of the integrated mean exit time of Brownian particles starting inside the domain. Since flows favorable to rapid heat exchange should lower exit times, we minimize a norm of the exit time. This is a time-independent optimization problem that we solve analytically in some limits, and numerically otherwise. We find an (at least locally) optimal velocity field that cools the domain on a mechanical time scale, in the sense that the integrated mean exit time is independent on molecular diffusivity in the limit of large-energy flows. ",Optimal heat transfer and optimal exit times
44,926356108788355072,1475769458,Joseph Salmon,"['New paper with @yousraBekhti, F. Lucka and @agramfort: bayesian perspective on non-convex sparse regression\n<LINK>']",https://arxiv.org/abs/1710.08747,"Majorization-minimization (MM) is a standard iterative optimization technique which consists in minimizing a sequence of convex surrogate functionals. MM approaches have been particularly successful to tackle inverse problems and statistical machine learning problems where the regularization term is a sparsity-promoting concave function. However, due to non-convexity, the solution found by MM depends on its initialization. Uniform initialization is the most natural and often employed strategy as it boils down to penalizing all coefficients equally in the first MM iteration. Yet, this arbitrary choice can lead to unsatisfactory results in severely under-determined inverse problems such as source imaging with magneto- and electro-encephalography (M/EEG). The framework of hierarchical Bayesian modeling (HBM) is an alternative approach to encode sparsity. This work shows that for certain hierarchical models, a simple alternating scheme to compute fully Bayesian maximum a posteriori (MAP) estimates leads to the exact same sequence of updates as a standard MM strategy (cf. the Adaptive Lasso). With this parallel outlined, we show how to improve upon these MM techniques by probing the multimodal posterior density using Markov Chain Monte-Carlo (MCMC) techniques. Firstly, we show that these samples can provide well-informed initializations that help MM schemes to reach better local minima. Secondly, we demonstrate how it can reveal the different modes of the posterior distribution in order to explore and quantify the inherent uncertainty and ambiguity of such ill-posed inference procedure. In the context of M/EEG, each mode corresponds to a plausible configuration of neural sources, which is crucial for data interpretation, especially in clinical contexts. Results on both simulations and real datasets show how the number or the type of sensors affect the uncertainties on the estimates. ","A hierarchical Bayesian perspective on majorization-minimization for
  non-convex sparse regression: application to M/EEG source imaging"
45,925304957456338946,2894733166,Amelia Fraser-McKelvie,['My new paper on passive spiral galaxies is on ArXiv today: <LINK> Have a read if you like weird galaxies (and pretty pictures!) 💫 😊 <LINK>'],https://arxiv.org/abs/1710.10843,"We examine the properties of a sample of 35 nearby passive spiral galaxies in order to determine their dominant quenching mechanism(s). All five low mass ($\textrm{M}_{\star} < 1 \times 10^{10} \textrm{M}_{\odot}$) passive spiral galaxies are located in the rich Virgo cluster. This is in contrast to low mass spiral galaxies with star formation, which inhabit a range of environments. We postulate that cluster-scale gas stripping and heating mechanisms operating only in rich clusters are required to quench low mass passive spirals, and ram-pressure stripping and strangulation are obvious candidates. For higher mass passive spirals, while trends are present, the story is less clear. The passive spiral bar fraction is high: 74$\pm$15%, compared with 36$\pm$5% for a mass, redshift, and T-type matched comparison sample of star forming spiral galaxies. The high mass passive spirals occur mostly, but not exclusively, in groups, and can be central or satellite galaxies. The passive spiral group fraction of 74$\pm$15% is similar to that of the comparison sample of star forming galaxies at 61$\pm$7%. We find evidence for both quenching via internal structure and environment in our passive spiral sample, though some galaxies have evidence of neither. From this, we conclude no one mechanism is responsible for quenching star formation in passive spiral galaxies - rather, a mixture of mechanisms are required to produce the passive spiral distribution we see today. ",Multiple Mechanisms Quench Passive Spiral Galaxies
46,923953524689571840,15904040,Renato Cunha,['New paper out. Learn about the past and future of HPC Cloud with our new paper - <LINK> - in press for publication at ACM Computing Surveys!'],https://arxiv.org/abs/1710.08731,"High Performance Computing (HPC) clouds are becoming an alternative to on-premise clusters for executing scientific applications and business analytics services. Most research efforts in HPC cloud aim to understand the cost-benefit of moving resource-intensive applications from on-premise environments to public cloud platforms. Industry trends show hybrid environments are the natural path to get the best of the on-premise and cloud resources---steady (and sensitive) workloads can run on on-premise resources and peak demand can leverage remote resources in a pay-as-you-go manner. Nevertheless, there are plenty of questions to be answered in HPC cloud, which range from how to extract the best performance of an unknown underlying platform to what services are essential to make its usage easier. Moreover, the discussion on the right pricing and contractual models to fit small and large users is relevant for the sustainability of HPC clouds. This paper brings a survey and taxonomy of efforts in HPC cloud and a vision on what we believe is ahead of us, including a set of research challenges that, once tackled, can help advance businesses and scientific discoveries. This becomes particularly relevant due to the fast increasing wave of new HPC applications coming from big data and artificial intelligence. ","HPC Cloud for Scientific and Business Applications: Taxonomy, Vision,
  and Research Challenges"
47,923845843161157632,15045590,Claudia Wagner,['our new paper on perception biases in social networks is online (now even with refs :-)) <LINK> @fariba_k @mstrohm'],https://arxiv.org/abs/1710.08601,"People's perceptions about the size of minority groups in social networks can be biased, often showing systematic over- or underestimation. These social perception biases are often attributed to biased cognitive or motivational processes. Here we show that both over- and underestimation of the size of a minority group can emerge solely from structural properties of social networks. Using a generative network model, we show analytically that these biases depend on the level of homophily and its asymmetric nature, as well as on the size of the minority group. Our model predictions correspond well with empirical data from a cross-cultural survey and with numerical calculations on six real-world networks. We also show under what circumstances individuals can reduce their biases by relying on perceptions of their neighbors. This work advances our understanding of the impact of network structure on social perception biases and offers a quantitative approach for addressing related issues in society. ",Homophily and minority size explain perception biases in social networks
48,918467339271786496,806058672619212800,Guillaume Lample,['New paper on fully unsupervised word translation :) <LINK>'],https://arxiv.org/abs/1710.04087,"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available. ",Word Translation Without Parallel Data
49,916104088428847104,4666231375,Konstantin Batygin,"[""Our new #PlanetNine paper 'Dynamical Evolution Induced by Planet Nine' is now online <LINK> Comments welcome""]",https://arxiv.org/abs/1710.01804,"The observational census of trans-Neptunian objects with semi-major axes greater than ~250 AU exhibits unexpected orbital structure that is most readily attributed to gravitational perturbations induced by a yet-undetected, massive planet. Although the capacity of this planet to (i) reproduce the observed clustering of distant orbits in physical space, (ii) facilitate dynamical detachment of their perihelia from Neptune, and (iii) excite a population of long-period centaurs to extreme inclinations is well established through numerical experiments, a coherent theoretical description of the dynamical mechanisms responsible for these effects remains elusive. In this work, we characterize the dynamical processes at play, from semi-analytic grounds. We begin by considering a purely secular model of orbital evolution induced by Planet Nine, and show that it is at odds with the ensuing stability of distant objects. Instead, the long-term survival of the clustered population of long-period KBOs is enabled by a web of mean-motion resonances driven by Planet Nine. Then, by taking a compact-form approach to perturbation theory, we show that it is the secular dynamics embedded within these resonances that regulates the orbital confinement and perihelion detachment of distant Kuiper belt objects. Finally, we demonstrate that the onset of large-amplitude oscillations of orbital inclinations is accomplished through capture of low-inclination objects into a high-order secular resonance and identify the specific harmonic that drives the evolution. In light of the developed qualitative understanding of the governing dynamics, we offer an updated interpretation of the current observational dataset within the broader theoretical framework of the Planet Nine hypothesis. ",Dynamical Evolution Induced by Planet Nine
50,925753719995674624,1202760024,Stacy McGaugh,"['In other Halloween scares, <LINK>\nWe find that the slope of the star forming main sequence is 1.04 +/- 0.06. <LINK>', '@MikeHudsonAstro Yes. Tried to wedge that in, but it’s gonna have to be a whole other paper.', 'Lots of results find shallower slopes. This appears to be because (a) they sample a smaller range in M* and (b) there is a break at 1E10Msun', 'So to run with the strained analogy to the stellar main sequence, it is low M galaxies that define it. Higher M spirals are a turn off pop', 'There is a sequence of thriving dwarfs (M*&lt;1E10) with ~constant SFR, and weary giants turning off the main sequence, on the way to red&amp;dead.']",https://arxiv.org/abs/1710.11236,"We explore the star forming properties of late type, low surface brightness (LSB) galaxies. The star forming main sequence (SFR-$M_*$) of LSB dwarfs has a steep slope, indistinguishable from unity ($1.04 \pm 0.06$). They form a distinct sequence from more massive spirals, which exhibit a shallower slope. The break occurs around $M_* \approx 10^{10}\;M_{\odot}$, and can also be seen in the gas mass-stellar mass plane. The global Kennicutt-Schmidt law (SFR-$M_g$) has a slope of $1.47 \pm 0.11$ without the break seen in the main sequence. There is an ample supply of gas in LSB galaxies, which have gas depletion times well in excess of a Hubble time, and often tens of Hubble times. Only $\sim 3\%$ of this cold gas need be in the form of molecular gas to sustain the observed star formation. In analogy with the faint, long-lived stars of the lower stellar main sequence, it may be appropriate to consider the main sequence of star forming galaxies to be defined by thriving dwarfs (with $M_* < 10^{10}\;M_{\odot}$) while massive spirals (with $M_* > 10^{10}\;M_{\odot}$) are weary giants that constitute more of a turn-off population. ",The Star Forming Main Sequence of Dwarf Low Surface Brightness Galaxies
51,923140151836119045,46941177,Jason Rhodes,['How will we combine @LSST and @EC_Euclid for the best cosmological constraints in the 2020s? Click  and find out!\n<LINK>'],https://arxiv.org/abs/1710.08489,"Euclid and the Large Synoptic Survey Telescope (LSST) are poised to dramatically change the astronomy landscape early in the next decade. The combination of high cadence, deep, wide-field optical photometry from LSST with high resolution, wide-field optical photometry and near-infrared photometry and spectroscopy from Euclid will be powerful for addressing a wide range of astrophysical questions. We explore Euclid/LSST synergy, ignoring the political issues associated with data access to focus on the scientific, technical, and financial benefits of coordination. We focus primarily on dark energy cosmology, but also discuss galaxy evolution, transient objects, solar system science, and galaxy cluster studies. We concentrate on synergies that require coordination in cadence or survey overlap, or would benefit from pixel-level co-processing that is beyond the scope of what is currently planned, rather than scientific programs that could be accomplished only at the catalog level without coordination in data processing or survey strategies. We provide two quantitative examples of scientific synergies: the decrease in photo-z errors (benefitting many science cases) when high resolution Euclid data are used for LSST photo-z determination, and the resulting increase in weak lensing signal-to-noise ratio from smaller photo-z errors. We briefly discuss other areas of coordination, including high performance computing resources and calibration data. Finally, we address concerns about the loss of independence and potential cross-checks between the two missions and potential consequences of not collaborating. ",Scientific Synergy Between LSST and Euclid
52,917434792232316929,1617006026,Krystal Guo,"['On arXiv today, we find when perturbing an edge in a graph gives strongly cospectral vxs.  [1710.02181]  <LINK>', 'For SRGs, we can find perfect &amp; pretty good state transfer in some of the perturbed graphs.']",https://arxiv.org/abs/1710.02181,"Quantum walks, an important tool in quantum computing, have been very successfully investigated using techniques in algebraic graph theory. We are motivated by the study of state transfer in continuous-time quantum walks, which is understood to be a rare and interesting phenomenon. We consider a perturbation on an edge $uv$ of a graph where we add a weight $\beta$ to the edge and a loop of weight $\gamma$ to each of $u$ and $v$. We characterize when for this perturbation results in strongly cospectral vertices $u$ and $v$. Applying this to strongly regular graphs, we give infinite families of strongly regular graphs where some perturbation results in perfect state transfer. Further, we show that, for every strongly regular graph, there is some perturbation which results in pretty good state transfer. We also show for any strongly regular graph $X$ and edge $e \in E(X)$, that $\phi(X\setminus e)$ does not depend on the choice of $e$. ",State transfer in strongly regular graphs with an edge perturbation
53,916109761321431040,790887180,David Sand,"['My student Paul has written a fun algorithm to find diffuse dwarf galaxies, and we tried it out around M101:\n<LINK>', 'Hey found 38 new dwarf candidates, and we are getting HST follow-up of many of them.', 'We put lots of simulated dwarfs into our data, so we know our detection efficiency very well. https://t.co/bUDWo7wh1f', '@AstroBailin Maybe?  These objects still have a half-light radius of ~5 arcsec, but if your pixels are that big...']",https://arxiv.org/abs/1710.01728,"We have conducted a search of a 9 deg$^{2}$ region of the CFHTLS around the Milky Way analog M101 (D$\sim$7 Mpc), in order to look for previously unknown low surface brightness galaxies. This search has uncovered 38 new low surface brightness dwarf candidates, and confirmed 11 previously reported galaxies, all with central surface brightness $\mu$(g,0)$>$23mag/arcsec$^{2}$, potentially extending the satellite luminosity function for the M101 group by $\sim$1.2 magnitudes. The search was conducted using an algorithm that nearly automates the detection of diffuse dwarf galaxies. The candidates small size and low surface brightness means that the faintest of these objects would likely be missed by traditional visual or computer detection techniques. The dwarf galaxy candidates span a range of $-$7.1 $\geq$ M$_g$ $\geq$ $-$10.2 and half light radii of 118-540 pc at the distance of M101, and they are well fit by simple S\'{e}rsic surface brightness profiles. These properties are consistent with dwarfs in the Local Group, and to match the Local Group luminosity function $\sim$10-20 of these candidates should be satellites of M101. Association with a massive host is supported by the lack of detected star formation and the over density of candidates around M101 compared to the field. The spatial distribution of the dwarf candidates is highly asymmetric, and concentrated to the northeast of M101 and therefore distance measurements will be required to determine if these are genuine members of the M101 group. ",Discovery of diffuse dwarf galaxy candidates around M101
