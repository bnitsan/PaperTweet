,Unnamed: 0.1,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title,Thread_length,Tweets_coarse,year,month,tweet_length
0,74,1349427132645089281,66175375,Jason Wang,"A short twitter thread on our new paper on the PDS 70 protoplanets using the GRAVITY interferometer (<LINK>)! This is one of the first papers from our ExoGRAVITY large program to study all the known directly imaged planets with interferometry <LINK> GRAVITY has amazing astrometric precision because we coherently combine light from telescopes separated by 130 meters. With just two epochs, we could already see that the inner planet has to be slightly eccentric while the outer planet is essentially circular. <LINK> Based on dynamical stability arguments, we could also place a ~10 Jupiter mass upper limit on the inner planet. We also find that this configuration is consistent with a 2:1 orbital resonance, but not required for stability. <LINK> GRAVITY also gives us a K-band spectrum of both planets. We ran A LOT of orbit fits (24 per planet), and found that the models with the best support from the data are dust-extincted planetary atmospheres. We firmly rule out blackbodies for both planets. Here's PDS 70 c: <LINK> We also did a super cool experiment to try to spatially resolve the circumplanetary environment of the protoplanets! We achieved sub-au resolution in our GRAVITY observations, but unfortunately did not resolve either planet. Any bright disk would have to be smaller than 0.3 au. <LINK> Forgot to add: this was work done with @ArthurVigan @SylvestreLacour @PlanetaryGao, a bunch of others not on twitter, and the whole GRAVITY team for helping with making these observations happen! @AstroThayne Nope. We basically found in the paper that we see no evidence of circumplanetary emission in the NIR data (maybe extinction though), but that the ALMA emission is totally consistent with cooler dust that is invisible at these shorter wavelengths.",https://arxiv.org/abs/2101.04187,"We present K-band interferometric observations of the PDS 70 protoplanets along with their host star using VLTI/GRAVITY. We obtained K-band spectra and 100 $\mu$as precision astrometry of both PDS 70 b and c in two epochs, as well as spatially resolving the hot inner disk around the star. Rejecting unstable orbits, we found a nonzero eccentricity for PDS 70 b of $0.17 \pm 0.06$, a near-circular orbit for PDS 70 c, and an orbital configuration that is consistent with the planets migrating into a 2:1 mean motion resonance. Enforcing dynamical stability, we obtained a 95% upper limit on the mass of PDS 70 b of 10 $M_\textrm{Jup}$, while the mass of PDS 70 c was unconstrained. The GRAVITY K-band spectra rules out pure blackbody models for the photospheres of both planets. Instead, the models with the most support from the data are planetary atmospheres that are dusty, but the nature of the dust is unclear. Any circumplanetary dust around these planets is not well constrained by the planets' 1-5 $\mu$m spectral energy distributions (SEDs) and requires longer wavelength data to probe with SED analysis. However with VLTI/GRAVITY, we made the first observations of a circumplanetary environment with sub-au spatial resolution, placing an upper limit of 0.3~au on the size of a bright disk around PDS 70 b. ",Constraining the Nature of the PDS 70 Protoplanets with VLTI/GRAVITY,7,"['A short twitter thread on our new paper on the PDS 70 protoplanets using the GRAVITY interferometer (<LINK>)! This is one of the first papers from our ExoGRAVITY large program to study all the known directly imaged planets with interferometry <LINK>', 'GRAVITY has amazing astrometric precision because we coherently combine light from telescopes separated by 130 meters. With just two epochs, we could already see that the inner planet has to be slightly eccentric while the outer planet is essentially circular. https://t.co/AHDqQ0RznR', 'Based on dynamical stability arguments, we could also place a ~10 Jupiter mass upper limit on the inner planet. We also find that this configuration is consistent with a 2:1 orbital resonance, but not required for stability. https://t.co/Qdvx2Y2UeU', ""GRAVITY also gives us a K-band spectrum of both planets. We ran A LOT of orbit fits (24 per planet), and found that the models with the best support from the data are dust-extincted planetary atmospheres. We firmly rule out blackbodies for both planets. Here's PDS 70 c: https://t.co/glMUJ7jlOv"", 'We also did a super cool experiment to try to spatially resolve the circumplanetary environment of the protoplanets! We achieved sub-au resolution in our GRAVITY observations, but unfortunately did not resolve either planet. Any bright disk would have to be smaller than 0.3 au. https://t.co/q2stzAHtq7', 'Forgot to add: this was work done with @ArthurVigan @SylvestreLacour @PlanetaryGao, a bunch of others not on twitter, and the whole GRAVITY team for helping with making these observations happen!', '@AstroThayne Nope. We basically found in the paper that we see no evidence of circumplanetary emission in the NIR data (maybe extinction though), but that the ALMA emission is totally consistent with cooler dust that is invisible at these shorter wavelengths.']",21,01,1769
1,41,966126604756819968,939498802767044608,Stephan,New #MachineLearning paper on multi-resolution tensor learning for large-scale spatial data with @yuqirose and @yisongyue! How do you learn basketball shot prediction models quickly? Use multi-resolution gradient descent with gradient entropy control! <LINK>,https://arxiv.org/abs/1802.06825,"High-dimensional tensor models are notoriously computationally expensive to train. We present a meta-learning algorithm, MMT, that can significantly speed up the process for spatial tensor models. MMT leverages the property that spatial data can be viewed at multiple resolutions, which are related by coarsening and finegraining from one resolution to another. Using this property, MMT learns a tensor model by starting from a coarse resolution and iteratively increasing the model complexity. In order to not ""over-train"" on coarse resolution models, we investigate an information-theoretic fine-graining criterion to decide when to transition into higher-resolution models. We provide both theoretical and empirical evidence for the advantages of this approach. When applied to two real-world large-scale spatial datasets for basketball player and animal behavior modeling, our approach demonstrate 3 key benefits: 1) it efficiently captures higher-order interactions (i.e., tensor latent factors), 2) it is orders of magnitude faster than fixed resolution learning and scales to very fine-grained spatial resolutions, and 3) it reliably yields accurate and interpretable models. ",Multi-resolution Tensor Learning for Large-Scale Spatial Data,1,['New #MachineLearning paper on multi-resolution tensor learning for large-scale spatial data with @yuqirose and @yisongyue! How do you learn basketball shot prediction models quickly? Use multi-resolution gradient descent with gradient entropy control! <LINK>'],18,02,258
2,64,996984067169116161,892059194240532480,Mikel Artetxe,"Check out our @acl2018 paper on ""A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings"" (w/ @glabaka & @eagirre). New SOTA on unsupervised word translation, while more robust than previous adversarial approaches. <LINK> <LINK> @alex_conneau @acl2018 @glabaka @eagirre In our en-fi experiments on a public dataset, MUSE failed in the 10 runs we tried for 2 different settings. Anders Søgaard, @seb_ruder and @licwu also report negative results for MUSE on en-fi, see <LINK>",https://arxiv.org/abs/1805.06297,"Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at this https URL ","A robust self-learning method for fully unsupervised cross-lingual
  mappings of word embeddings",2,"['Check out our @acl2018 paper on ""A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings"" (w/ @glabaka &amp; @eagirre). New SOTA on unsupervised word translation, while more robust than previous adversarial approaches.\n<LINK> <LINK>', '@alex_conneau @acl2018 @glabaka @eagirre In our en-fi experiments on a public dataset, MUSE failed in the 10 runs we tried for 2 different settings. Anders Søgaard, @seb_ruder and @licwu also report negative results for MUSE on en-fi, see https://t.co/fOljApqymx']",18,05,513
3,7,1400342381367541762,153384371,Jonas Schuett,"I'm super excited to share with you my new paper ""AI Certification: Advancing Ethical Practice by Reducing Information Asymmetries"" that I wrote together with my amazing colleagues Peter Cihon (@pcihon), Moritz Kleinaltenkamp, and Seth Baum (@SethBaum). <LINK> Based on a review of the management literature on certification, we show how AI certification can reduce information asymmetries and incentivize the adoption of AI ethics principles. We also survey the current landscape of AI certification schemes and briefly discuss what all this could mean for future AI systems. The article will appear in IEEE Transactions on Technology and Society: <LINK>",https://arxiv.org/abs/2105.10356,"As artificial intelligence (AI) systems are increasingly deployed, principles for ethical AI are also proliferating. Certification offers a method to both incentivize adoption of these principles and substantiate that they have been implemented in practice. This paper draws from management literature on certification and reviews current AI certification programs and proposals. Successful programs rely on both emerging technical methods and specific design considerations. In order to avoid two common failures of certification, program designs should ensure that the symbol of the certification is substantially implemented in practice and that the program achieves its stated goals. The review indicates that the field currently focuses on self-certification and third-party certification of systems, individuals, and organizations - to the exclusion of process management certifications. Additionally, the paper considers prospects for future AI certification programs. Ongoing changes in AI technology suggest that AI certification regimes should be designed to emphasize governance criteria of enduring value, such as ethics training for AI developers, and to adjust technical criteria as the technology changes. Overall, certification can play a valuable mix in the portfolio of AI governance tools. ","AI Certification: Advancing Ethical Practice by Reducing Information
  Asymmetries",4,"['I\'m super excited to share with you my new paper ""AI Certification: Advancing Ethical Practice by Reducing Information Asymmetries"" that I wrote together with my amazing colleagues Peter Cihon (@pcihon), Moritz Kleinaltenkamp, and Seth Baum (@SethBaum).\n<LINK>', 'Based on a review of the management literature on certification, we show how AI certification can reduce information asymmetries and incentivize the adoption of AI ethics principles.', 'We also survey the current landscape of AI certification schemes and briefly discuss what all this could mean for future AI systems.', 'The article will appear in IEEE Transactions on Technology and Society: https://t.co/2BKuZeelgC']",21,05,655
4,130,1455602326194974720,777776718928941056,Daniel Muthukrishna,"New paper on the Arxiv today on the real-time detection of anomalies in astronomical time series. <LINK> We compare a deep neural network and a parametric approach. The fact that NNs generalize too well to different types of data obscures anomaly detection Excited to finally have this paper out with Michelle Lochner, @Doctor_Lobster, @SaraWebbScience, and @gsnarayan",https://arxiv.org/abs/2111.00036,"New time-domain surveys, such as the Rubin Observatory Legacy Survey of Space and Time (LSST), will observe millions of transient alerts each night, making standard approaches of visually identifying new and interesting transients infeasible. We present two novel methods of automatically detecting anomalous transient light curves in real-time. Both methods are based on the simple idea that if the light curves from a known population of transients can be accurately modelled, any deviations from model predictions are likely anomalies. The first modelling approach is a probabilistic neural network built using Temporal Convolutional Networks (TCNs) and the second is an interpretable Bayesian parametric model of a transient. We demonstrate our methods' ability to provide anomaly scores as a function of time on light curves from the Zwicky Transient Facility. We show that the flexibility of neural networks, the attribute that makes them such a powerful tool for many regression tasks, is what makes them less suitable for anomaly detection when compared with our parametric model. The parametric model is able to identify anomalies with respect to common supernova classes with low false anomaly rates and high true anomaly rates achieving Area Under the Receive Operating Characteristic (ROC) Curve (AUC) scores above 0.8 for most rare classes such as kilonovae, tidal disruption events, intermediate luminosity transients, and pair-instability supernovae. Our ability to identify anomalies improves over the lifetime of the light curves. Our framework, used in conjunction with transient classifiers, will enable fast and prioritised follow-up of unusual transients from new large-scale surveys. ",Real-time detection of anomalies in large-scale transient surveys,2,"['New paper on the Arxiv today on the real-time detection of anomalies in astronomical time series. <LINK>\n\nWe compare a deep neural network and a parametric approach. The fact that NNs generalize too well to different types of data obscures anomaly detection', 'Excited to finally have this paper out with Michelle Lochner, @Doctor_Lobster, @SaraWebbScience, and @gsnarayan']",21,11,368
5,22,1134342275474046979,883039700,Lenka Zdeborova,"Generative models are the new sparsity ... or even better actually as shown in our last paper: <LINK> <LINK> @carlonicolini84 You are perfectly right, the prior is based on the whole database, it does not know which particular picture was chosen to be the spike. @DanFrederiksen2 It is not denoising, but we do want to reconstruct the images as in denoising. The point is that the shirt is there better in the lower line than in the upper line which is the standard methods. The noisy data are not shown as they do not come in the form of picture.",https://arxiv.org/abs/1905.12385,"Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generative modelling of signal distributions. Generative models based on neural networks, such as GANs or variational auto-encoders, are particularly performant and are gaining on applicability. In this paper we study spiked matrix models, where a low-rank matrix is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here, we replace the sparsity assumption by generative modelling, and investigate the consequences on statistical and algorithmic properties. We analyze the Bayes-optimal performance under specific generative models for the spike. In contrast with the sparsity assumption, we do not observe regions of parameters where statistical performance is superior to the best known algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theory, showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets. ",The spiked matrix model with generative priors,3,"['Generative models are the new sparsity ... or even better actually as shown in our last paper: <LINK> <LINK>', '@carlonicolini84 You are perfectly right, the prior is based on the whole database, it does not know which particular picture was chosen to be the spike.', '@DanFrederiksen2 It is not denoising, but we do want to reconstruct the images as in denoising. The point is that the shirt is there better in the lower line than in the upper line which is the standard methods. The noisy data are not shown as they do not come in the form of picture.']",19,05,547
6,245,1445031024551862277,2491688827,Dr. Maria Gorlatova,"We did a benchmarking study of whether holograms appear to stay where placed, in modern markerless smartphone augmented reality. Got lots of data on spatial instability for different user motions and in different environments: <LINK> #AugmentedReality",https://arxiv.org/abs/2109.14757,"Markerless augmented reality (AR) has the potential to provide engaging experiences and improve outcomes across a wide variety of industries; the overlaying of virtual content, or holograms, onto a view of the real world without the need for predefined markers provides great convenience and flexibility. However, unwanted hologram movement frequently occurs in markerless smartphone AR due to challenging visual conditions or device movement, and resulting error in device pose tracking. We develop a method for measuring hologram positional errors on commercial smartphone markerless AR platforms, implement it as an open-source AR app, HoloMeasure, and use the app to conduct systematic quantitative characterizations of hologram stability across 6 different user actions, 3 different smartphone models, and over 200 different environments. Our study demonstrates significant levels of spatial instability in holograms in all but the simplest settings, and underscores the need for further enhancements to pose tracking algorithms for smartphone-based markerless AR. ","Here To Stay: Measuring Hologram Stability in Markerless Smartphone
  Augmented Reality",1,"['We did a benchmarking study of whether holograms appear to stay where placed, in modern markerless smartphone augmented reality. Got lots of data on spatial instability for different user motions and in different environments: <LINK> #AugmentedReality']",21,09,251
7,29,1243235434672664576,80454546,Greg Gilbert,"My new paper, “An information theoretic framework for classifying exoplanetary system architectures” is on arXiv today! A thread 👇🏼 (1/n) <LINK> Headline result #1: planets really are “peas in a pod” - within a system, planets tend to be roughly the same size and roughly evenly spaced. (2/n) <LINK> Method: we define quantities to describe the dynamical mass (µ), mass partitioning (Q), gap complexity (C), spacing scale (S), and monotonicity (M) of each system, finding Q and C both strongly peaked near 0 - equal sizes and even spacings. (3/n) <LINK> Comparing to forward models (SysSim, He+ 2019; EPOS, Mulders+ 2019), we find that these are real features, not selection effects. In fact, planets are even more evenly spaced than previously realized. (4/n) <LINK> Most planets are separated by ~20 mutual Hill radii, with evidence of an “echo peak” at ~30 mutual Hill radii. The same feature shows up in forward models; a natural explanation is missing planets. (5/n) <LINK> We apply unsupervised clustering on these quantities (µ, Q, C, S), plus a measure of flatness (f) and find two populations, the smaller of which has both wide spacing and uneven spacings, more evidence for missing planets. (6/n) <LINK> It’s worth repeating: the same systems which are widely spaced also tend to be unevenly spaced. (7/n) We also find that the typical system-to-star mass ratio a bit less than ~10^4, reminiscent of the giant moon systems of Jupiter, Saturn, and Uranus. Taken together, these various observations lead us to conclude… (8/n) <LINK> Headline result #2: most of Kepler’s high-multiplicity (N≥3) systems belong to a single intrinsic population, with a subset of systems (~20%) hosting additional undetected planets intermediate in period between the known planets. (9/n) So what’s next? We plan to apply this method to other populations (e.g. RV) and search for hypothetical missing planets. These statistics will also provide improved targets for population synthesis and planet formation models. (10/n) Thanks for reading! Questions? Comments? Feel free to email or message me and I’m happy to chat!",https://arxiv.org/abs/2003.11098,"We propose several descriptive measures to characterize the arrangements of planetary masses, periods, and mutual inclinations within exoplanetary systems. These measures are based in complexity theory and capture the global, system-level trends of each architecture. Our approach considers all planets in a system simultaneously, facilitating both intra-system and inter-system analysis. We find that based on these measures, Kepler's high-multiplicity ($N\geq3$) systems can be explained if most systems belong to a single intrinsic population, with a subset of high-multiplicity systems ($\sim20\%$) hosting additional, undetected planets intermediate in period between the known planets. We confirm prior findings that planets within a system tend to be roughly the same size and approximately coplanar. We find that forward modeling has not yet reproduced the high degree of spacing similarity (in log-period) actually seen in the Kepler data. Although our classification scheme was developed using compact Kepler multis as a test sample, our methods can be immediately applied to any other population of exoplanetary systems. We apply this classification scheme to (1) quantify the similarity between systems, (2) resolve observational biases from physical trends, and (3) identify which systems to search for additional planets and where to look for these planets. ","An information theoretic framework for classifying exoplanetary system
  architectures",11,"['My new paper, “An information theoretic framework for classifying exoplanetary system architectures” is on arXiv today! A thread 👇🏼  (1/n)\n\n<LINK>', 'Headline result #1: planets really are “peas in a pod” - within a system, planets tend to be roughly the same size and roughly evenly spaced. (2/n) https://t.co/K1hevkYEDb', 'Method: we define quantities to describe the dynamical mass (µ), mass partitioning (Q), gap complexity (C), spacing scale (S), and monotonicity (M) of each system, finding Q and C both strongly peaked near 0 - equal sizes and even spacings. (3/n) https://t.co/fgBLJEDq3G', 'Comparing to forward models (SysSim, He+ 2019; EPOS, Mulders+ 2019), we find that these are real features, not selection effects. In fact, planets are even more evenly spaced than previously realized. (4/n) https://t.co/Aw3WU8Ha8B', 'Most planets are separated by ~20 mutual Hill radii, with evidence of an “echo peak” at ~30 mutual Hill radii. The same feature shows up in forward models; a natural explanation is missing planets. (5/n) https://t.co/Twi7EVTGdR', 'We apply unsupervised clustering on these quantities (µ, Q, C, S), plus a measure of flatness (f) and find two populations, the smaller of which has both wide spacing and uneven spacings, more evidence for missing planets. (6/n) https://t.co/e27imK6HBS', 'It’s worth repeating: the same systems which are widely spaced also tend to be unevenly spaced. (7/n)', 'We also find that the typical system-to-star mass ratio a bit less than ~10^4, reminiscent of the giant moon systems of Jupiter, Saturn, and Uranus. Taken together, these various observations lead us to conclude… (8/n) https://t.co/iCkRA6ftNo', 'Headline result #2: most of Kepler’s high-multiplicity (N≥3) systems belong to a single intrinsic population, with a subset of systems (~20%) hosting additional undetected planets intermediate in period between the known planets. (9/n)', 'So what’s next? We plan to apply this method to other populations (e.g. RV) and search for hypothetical missing planets. These statistics will also provide improved targets for population synthesis and planet formation models. (10/n)', 'Thanks for reading! Questions? Comments? Feel free to email or message me and I’m happy to chat!']",20,03,2109
8,70,1417412598245609489,1153677180842467333,Roman Kolcun,"In our new paper, we revisit popular ML models for IoT device identification and show how their accuracy degrades over time. Paper will appear in TMA 2021. Joined work with @diana_popescu_ Vadim Safronov @pooyadav @mort___ @ammandalari and @realhamed <LINK>",https://arxiv.org/abs/2107.07818,"Internet-of-Things (IoT) devices are known to be the source of many security problems, and as such, they would greatly benefit from automated management. This requires robustly identifying devices so that appropriate network security policies can be applied. We address this challenge by exploring how to accurately identify IoT devices based on their network behavior, while leveraging approaches previously proposed by other researchers. We compare the accuracy of four different previously proposed machine learning models (tree-based and neural network-based) for identifying IoT devices. We use packet trace data collected over a period of six months from a large IoT test-bed. We show that, while all models achieve high accuracy when evaluated on the same dataset as they were trained on, their accuracy degrades over time, when evaluated on data collected outside the training set. We show that on average the models' accuracy degrades after a couple of weeks by up to 40 percentage points (on average between 12 and 21 percentage points). We argue that, in order to keep the models' accuracy at a high level, these need to be continuously updated. ",Revisiting IoT Device Identification,1,"['In our new paper, we revisit popular ML models for IoT device identification and show how their accuracy degrades over time. Paper will appear in TMA 2021. Joined work with @diana_popescu_ Vadim Safronov @pooyadav @mort___ @ammandalari and @realhamed \n<LINK>']",21,07,257
9,196,1393134095530696704,754948023382310912,Niclas Rieger,You have some big 🌏data & want to find possible time lags between your variables for each location? Give complex MCA a try! We applied it on SST 🌊 & precipitation 🌧️ #ERA5 to identify lagged teleconnections🔗🕙 arXiv➡️ <LINK> #openaccess #openscience <LINK>,https://arxiv.org/abs/2105.04618,"A proper description of ocean-atmosphere interactions is key for a correct understanding of climate evolution. The interplay among the different variables acting over the climate is complex, often leading to correlations across long spatial distances (teleconnections). In some occasions, those teleconnections occur with quite significant temporal shifts that are fundamental for the understanding of the underlying phenomena but which are poorly captured by standard methods. Applying orthogonal decomposition such as Maximum Covariance Analysis (MCA) to geophysical data sets allows to extract common dominant patterns between two different variables, but generally suffers from (i) the non-physical orthogonal constraint as well as (ii) the consideration of simple correlations, whereby temporally offset signals are not detected. Here we propose an extension, complex rotated MCA, to address both limitations. We transform our signals using the Hilbert transform and perform the orthogonal decomposition in complex space, allowing us to correctly correlate out-of-phase signals. Subsequent Varimax rotation removes the orthogonal constraints, leading to more physically meaningful modes of geophysical variability. As an example of application, we have employed this method on sea surface temperature and continental precipitation; our method successfully captures the temporal and spatial interactions between these two variables, namely for (i) the seasonal cycle, (ii) canonical ENSO, (iii) the global warming trend, (iv) the Pacific Decadal Oscillation, (v) ENSO Modoki and finally (vi) the Atlantic Meridional Mode. The complex rotated modes of MCA provide information on the regional amplitude, and under certain conditions, the regional time lag between changes on ocean temperature and land precipitation. ","Lagged teleconnections of climate variables identified via complex
  rotated Maximum Covariance Analysis",1,['You have some big 🌏data &amp; want to find possible time lags between your variables for each location?\n\nGive complex MCA a try! We applied it on SST 🌊 &amp; precipitation 🌧️ #ERA5 to identify lagged teleconnections🔗🕙\n\narXiv➡️ <LINK>\n#openaccess #openscience <LINK>'],21,05,255
10,87,1138147747431833603,112717746,Michela Paganini,"New paper on #LotteryTickets in deep nets & transfer across datasets and optimizers now out on @arxiv_org! ""One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"". Work led by @arimorcos at @facebookai ➡️ <LINK>",https://arxiv.org/abs/1906.02773,"The success of lottery ticket initializations (Frankle and Carbin, 2019) suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these ""winning ticket"" initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods. ","One ticket to win them all: generalizing lottery ticket initializations
  across datasets and optimizers",1,"['New paper on #LotteryTickets in deep nets &amp; transfer across datasets and optimizers now out on @arxiv_org! ""One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"". Work led by @arimorcos at @facebookai ➡️\xa0<LINK>']",19,06,260
11,37,1206931405198036992,2354296903,Rahul Gopinath,"New paper ""Inferring Input Grammars from Dynamic Control Flow"". Given a program, our Mimid prototype automatically infers a human-readable context-free grammar that accurately specifies its input syntax. Great helper for fuzzing and program understanding! <LINK> A fully documented python prototype is available as a self contained Jupyter notebook here: <LINK> @tathanhdinh Thanks for letting me know. Could you please give me a link to your thesis? @AdamOfDc949 Thanks for the pointer. SynFuzz looks really cool. @tathanhdinh Thanks! Much appreciated. @tathanhdinh I will certainly read it, and cite it. Grammar fuzzers and grammar recovery have become more important these days. So I do not think that your paper would be forgotten. @reyeetengineer Thank you!",https://arxiv.org/abs/1912.05937,"A program is characterized by its input model, and a formal input model can be of use in diverse areas including vulnerability analysis, reverse engineering, fuzzing and software testing, clone detection and refactoring. Unfortunately, input models for typical programs are often unavailable or out of date. While there exist algorithms that can mine the syntactical structure of program inputs, they either produce unwieldy and incomprehensible grammars, or require heuristics that target specific parsing patterns. In this paper, we present a general algorithm that takes a program and a small set of sample inputs and automatically infers a readable context-free grammar capturing the input language of the program. We infer the syntactic input structure only by observing access of input characters at different locations of the input parser. This works on all program stack based recursive descent input parsers, including PEG and parser combinators, and can do entirely without program specific heuristics. Our Mimid prototype produced accurate and readable grammars for a variety of evaluation subjects, including expr, URLparse, and microJSON. ",Inferring Input Grammars from Dynamic Control Flow,7,"['New paper ""Inferring Input Grammars from Dynamic Control Flow"". Given a program, our Mimid prototype automatically infers a human-readable context-free grammar that accurately specifies its input syntax.  Great helper for fuzzing and program understanding! <LINK>', 'A fully documented python prototype is available as a self contained Jupyter notebook here: https://t.co/SE1Q1jd72J', '@tathanhdinh Thanks for letting me know. Could you please give me a link to your thesis?', '@AdamOfDc949 Thanks for the pointer. SynFuzz looks really cool.', '@tathanhdinh Thanks! Much appreciated.', '@tathanhdinh I will certainly read it, and cite it. Grammar fuzzers and grammar recovery have become more important these days. So I do not think that your paper would be forgotten.', '@reyeetengineer Thank you!']",19,12,762
12,47,1418491800264744960,2915749124,Dhiraj Hazra,"Our new paper 'Dark Twilight Joined with the Light of Dawn to Unveil the Reionization History' with Daniela Paoletti, Fabio Finelli and @georgesmoot — <LINK> — an extended analysis of the reionization history based on recent cosmological and astrophysical data.",https://arxiv.org/abs/2107.10693,"Improved measurement of the Cosmic Microwave Background polarization from Planck allows a detailed study of reionization beyond the average optical depth. The lower value of the optical depth disfavours an early onset and an early completion of reionization in favour of a redsfhit range where different astrophysical probes provide sensible information on the sources of reionization and the status of the intergalactic medium. In this work we extend our previous study in which we constrained reionization by combining three different probes - CMB, UV luminosity density and neutral hydrogen fraction data - in both treatment and data: we first allow variation in the UV source term varying the product of the efficiency of conversion of UV luminosity into ionizing photons and the escape fraction together with the reionization and cosmological parameters, and then we investigate the impact of a less conservative cut for the UV luminosity function. We find that the estimate for the efficiency is consistent within 95% C.L. with the fixed value we considered in our previous results and is mostly constrained by the QHII data. We find that allowing the efficiency to vary does not affect significantly our results for the average optical depth for monotonic reionization histories, recovering $\tau=0.0519_{-0.0008}^{+0.0010}$ at 68% CL , consistent with our previous studies. Using a less conservative cut for the UV luminosity function, we find $\tau=0.0541_{-0.0016}^{+0.0013}$ at 68% CL, due to the faint end of the luminosity function in the data we use, that also prefers a larger contribution from higher redshifts. ","Dark Twilight Joined with the Light of Dawn to Unveil the Reionization
  History",1,"[""Our new paper 'Dark Twilight Joined with the Light of Dawn to Unveil the Reionization History' with Daniela Paoletti, Fabio Finelli and @georgesmoot — <LINK> — an extended analysis of the reionization history based on recent cosmological and astrophysical data.""]",21,07,261
13,190,1335685290569072640,1199958835508592640,Michael Dennis,"Happy to present new work with @natashajaques, @EugeneVinitsky, et. al where we propose PAIRED, a simple training regime using insights from decision theory to generate a curriculum of increasingly complex environments in more settings than self-play <LINK> 1/6 <LINK> In this joint work along with @alexandrebayen, Stuart Russell, Andrew Critch, and @svlevine, we introduce Unsupervised Environment Design (UED) as a framework which encompasses prior approaches such as self-play, domain randomization, and minimax adversarial training. 2/6 <LINK> We show that UED is the dual problem to decisions under ignorance in decision theory, and we show that many of these prior approaches to UED correspond to classical approaches in decision theory. We show that our approach, PAIRED, corresponds to the classic minimax regret. 3/6 <LINK> Turns out that minimax regret, unlike the other rules tried, has useful properties which prevent training from stalling. We find that PAIRED motivates the adversary to generate the simplest environment that it knows is solvable that the agent does not yet solve. 4/6 There is a lot I am excited about with this work, which I don't have space to get into here. To see more, please check out the paper or see us at #neurips2020 (<LINK>) tomorrow (Monday) at our oral presentation (18:30 PT) or poster session (21:00-23:00 PT) 5/6 You can also see a different side of this project in this video/thread: <LINK> 6/6",https://arxiv.org/abs/2012.02096,"A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments. ","Emergent Complexity and Zero-shot Transfer via Unsupervised Environment
  Design",6,"['Happy to present new work with @natashajaques, @EugeneVinitsky, et. al where we propose PAIRED, a simple training regime using insights from decision theory to generate a curriculum of increasingly complex environments in more settings than self-play <LINK> 1/6 <LINK>', 'In this joint work along with @alexandrebayen, Stuart Russell, Andrew Critch, and @svlevine, we introduce Unsupervised Environment Design (UED) as a framework which encompasses prior approaches such as self-play, domain randomization, and minimax adversarial training. 2/6 https://t.co/j75tmHchzE', 'We show that UED is the dual problem to decisions under ignorance in decision theory, and we show that many of these prior approaches to UED correspond to classical approaches in decision theory. We show that our approach, PAIRED, corresponds to the classic minimax regret. 3/6 https://t.co/jqnNPc5pF5', 'Turns out that minimax regret, unlike the other rules tried, has useful properties which prevent training from stalling. We find that PAIRED motivates the adversary to generate the simplest environment that it knows is solvable that the agent does not yet solve. 4/6', ""There is a lot I am excited about with this work, which I don't have space to get into here. To see more, please check out the paper or see us at #neurips2020 (https://t.co/atWyxexZXF) tomorrow (Monday) at our oral presentation (18:30 PT) or poster session (21:00-23:00 PT) 5/6"", 'You can also see a different side of this project in this video/thread: https://t.co/CLKDmFVgbd  6/6']",20,12,1444
14,79,1039885487715037186,2191799629,Sebastian Gehrmann,"A little late to the party, but check out our #emnlp2018 paper on bottom-up abstractive summarization! <LINK> We find that constraining copy-attention to predetermined words and phrases greatly improves results, with potential application to low-resource domains!",https://arxiv.org/abs/1808.10792,"Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but which can be poor at content selection. This work proposes a simple technique for addressing this issue: use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain. ",Bottom-Up Abstractive Summarization,1,"['A little late to the party, but check out our #emnlp2018 paper on bottom-up abstractive summarization! <LINK>\nWe find that constraining copy-attention to predetermined words and phrases greatly improves results, with potential application to low-resource domains!']",18,08,263
15,94,1201908838732926976,4013975440,Cinjon Resnick,"New paper from Joan, Zeping, and me - Probing the State of the Art: A Critical Look at Visual Representation Evaluation (<LINK>). Central thesis is that using linear probes to evaluate self-supervised models is insufficient to adjudicate progress. 1/6 We show that the ranking of models we attain when using linear probes for classification is different from the ranking we get when performing temporal activity localization (which needs a nonlinear transformation) on two datasets - Thumos14 and a new Gymnastics dataset. 2/6 The accepted ordering in the self-supervised literature is not conclusive, but nor is it here! Sometimes AMDIM is better than TimeCycle and vice versa. 3/6 We also showed that the self-supervised objective can create a representation space that isn't linearly separable, obviating this approach to discriminating models. We did this by finding one, TimeCycle, that did better when just initialized than after training. cc @xiaolonw 4/6 What we actually care about in self-supervised learning is producing representation spaces that transfer well. Our experiments suggest that the self-supervised representations are just as good as the supervised ones at transferring to a much harder task on a new distribution. 5/6 Arguably, a better approach is to pose evaluation not in terms of linear or nonlinear tasks, but instead as a few-shot learning problem. Measure not with probes but with the necessary sample complexity to yield a successful transfer. 6/6 (cc @hugo_larochelle @giffmana) Much obliged to @jamberto and @snakamori above all for making this even possible. Huge thanks to @curran922, @_willfalcon, @kchonyc, @wfwhitney, and Shubho Sengupta for contributions to the paper, and also to @douglas_eck, @ada_rob, @jesseengel, @iamandrewdai, and @Mo_Norouzi.",https://arxiv.org/abs/1912.00215,"Self-supervised research improved greatly over the past half decade, with much of the growth being driven by objectives that are hard to quantitatively compare. These techniques include colorization, cyclical consistency, and noise-contrastive estimation from image patches. Consequently, the field has settled on a handful of measurements that depend on linear probes to adjudicate which approaches are the best. Our first contribution is to show that this test is insufficient and that models which perform poorly (strongly) on linear classification can perform strongly (weakly) on more involved tasks like temporal activity localization. Our second contribution is to analyze the capabilities of five different representations. And our third contribution is a much needed new dataset for temporal activity localization. ","Probing the State of the Art: A Critical Look at Visual Representation
  Evaluation",7,"['New paper from Joan, Zeping, and me - Probing the State of the Art: A Critical Look at Visual Representation Evaluation (<LINK>). Central thesis is that using linear probes to evaluate self-supervised models is insufficient to adjudicate progress. 1/6', 'We show that the ranking of models we attain when using linear probes for classification is different from the ranking we get when performing temporal activity localization (which needs a nonlinear transformation) on two datasets - Thumos14 and a new Gymnastics dataset. 2/6', 'The accepted ordering in the self-supervised literature is not conclusive, but nor is it here! Sometimes AMDIM is better than TimeCycle and vice versa. 3/6', ""We also showed that the self-supervised objective can create a representation space that isn't linearly separable, obviating this approach to discriminating models. We did this by finding one, TimeCycle, that did better when just initialized than after training. cc @xiaolonw 4/6"", 'What we actually care about in self-supervised learning is producing representation spaces that transfer well. Our experiments suggest that the self-supervised representations are just as good as the supervised ones at transferring to a much harder task on a new distribution. 5/6', 'Arguably, a better approach is to pose evaluation not in terms of linear or nonlinear tasks, but instead as a few-shot learning problem. Measure not with probes but with the necessary sample complexity to yield a successful transfer. 6/6 (cc @hugo_larochelle @giffmana)', 'Much obliged to @jamberto and @snakamori above all for making this even possible. Huge thanks to @curran922, @_willfalcon, @kchonyc, @wfwhitney, and Shubho Sengupta for contributions to the paper, and also to @douglas_eck, @ada_rob, @jesseengel, @iamandrewdai, and @Mo_Norouzi.']",19,12,1791
16,21,1143860072361185281,974769773539155968,Daniel Baumann,"New paper with @nu_phases and Tom Hartman: <LINK> We relate inflation to a special type of RG flow, and then use causality and unitarity to derive new constraints on the low-energy couplings of the theory. @nu_phases The main result is a sum rule relating the speed at which a certain operator spreads along the RG flow to the UV completion of the theory. In inflation this translates into a constraint on the speed of propagation of inflationary fluctuations. @nu_phases Relating IR observables to the UV will play an important role in connecting cosmological observables to the microscopic origin of inflation. @nu_phases All credit to my fantastic collaborators Dan and Tom for making this work. It has been a lot of fun to learn from them and explore the connection between fundamental questions in cosmology and advanced techniques in quantum field theory.",https://arxiv.org/abs/1906.10226,"Sum rules connecting low-energy observables to high-energy physics are an interesting way to probe the mechanism of inflation and its ultraviolet origin. Unfortunately, such sum rules have proven difficult to study in a cosmological setting. Motivated by this problem, we investigate a precise analogue of inflation in anti-de Sitter spacetime, where it becomes dual to a slow renormalization group flow in the boundary quantum field theory. This dual description provides a firm footing for exploring the constraints of unitarity, analyticity, and causality on the bulk effective field theory. We derive a sum rule that constrains the bulk coupling constants in this theory. In the bulk, the sum rule is related to the speed of radial propagation, while on the boundary, it governs the spreading of nonlocal operators. When the spreading speed approaches the speed of light, the sum rule is saturated, suggesting that the theory becomes free in this limit. We also discuss whether similar results apply to inflation, where an analogous sum rule exists for the propagation speed of inflationary fluctuations. ",Dynamical Constraints on RG Flows and Cosmology,4,"['New paper with @nu_phases and Tom Hartman: <LINK>\nWe relate inflation to a special type of RG flow, and then use causality and unitarity to derive new constraints on the low-energy couplings of the theory.', '@nu_phases The main result is a sum rule relating the speed at which a certain operator spreads along the RG flow to the UV completion of the theory. In inflation this translates into a constraint on the speed of propagation of inflationary fluctuations.', '@nu_phases Relating IR observables to the UV will play an important role in connecting cosmological observables to the microscopic origin of inflation.', '@nu_phases All credit to my fantastic collaborators Dan and Tom for making this work. It has been a lot of fun to learn from them and explore the connection between fundamental questions in cosmology and advanced techniques in quantum field theory.']",19,06,861
17,82,1516226735087497222,223458855,Michael A. Fedderke,"New paper 🥳 <LINK> We explore how you could possibly do astrometric detection of low-frequency gravitational waves using a stellar interferometer in space, using as stable targets a small number of photometrically stable, distant, hot white dwarfs. 1/2 This is an alternative optimization of this measurement as compared to existing work on this subject that uses large-N surveys (e.g., Gaia) to search for this well-known signal. 2/2",https://arxiv.org/abs/2204.07677,"We evaluate the potential for gravitational-wave (GW) detection in the frequency band from 10 nHz to 1 $\mu$Hz using extremely high-precision astrometry of a small number of stars. In particular, we argue that non-magnetic, photometrically stable hot white dwarfs (WD) located at $\sim$ kpc distances may be optimal targets for this approach. Previous studies of astrometric GW detection have focused on the potential for less precise surveys of large numbers of stars; our work provides an alternative optimization approach to this problem. Interesting GW sources in this band are expected at characteristic strains around $h_c \sim 10^{-17} \times \left(\mu\text{Hz}/f_{\text{GW}}\right)$. The astrometric angular precision required to see these sources is $\Delta \theta \sim h_c$ after integrating for a time $T \sim 1/f_{\text{GW}}$. We show that jitter in the photometric center of WD of this type due to starspots is bounded to be small enough to permit this high-precision, small-$N$ approach. We discuss possible noise arising from stellar reflex motion induced by orbiting objects and show how it can be mitigated. The only plausible technology able to achieve the requisite astrometric precision is a space-based stellar interferometer. Such a future mission with few-meter-scale collecting dishes and baselines of $\mathcal{O}(100\text{ km})$ is sufficient to achieve the target precision. This collector size is broadly in line with the collectors proposed for some formation-flown, space-based astrometer or optical synthetic-aperature imaging-array concepts proposed for other science reasons. The proposed baseline is however somewhat larger than the km-scale baselines discussed for those concepts, but we see no fundamental technical obstacle to utilizing such baselines. A mission of this type thus also holds the promise of being one of the few ways to access interesting GW sources in this band. ",Astrometric Gravitational-Wave Detection via Stellar Interferometry,2,"['New paper 🥳 <LINK>\nWe explore how you could possibly do astrometric detection of low-frequency gravitational waves using a stellar interferometer in space, using as stable targets a small number of photometrically stable, distant, hot white dwarfs. 1/2', 'This is an alternative optimization of this measurement as compared to existing work on this subject that uses large-N surveys (e.g., Gaia) to search for this well-known signal. 2/2']",22,04,434
18,155,1483032604655992832,978179721157595136,Joana Fraxanet,"I am very happy to share our latest work, which can now be found on arXiv: <LINK> In this paper, we study the effect of long-range superconducting pairing on the localization properties of a quasi-periodic chain. 👇🧵 @ICFOnians @adauphin4 Understanding metal-insulator transitions has been one of the central questions in condensed matter physics. One of the most paradigmatic examples of such transitions is Anderson localization, in which a system becomes insulating in presence of disorder. <LINK> The study of Anderson localization in disordered systems needs at least two-dimensions and averaging over many disorder realizations. In contrast, quasi-periodic systems have gained a lot of attention as an alternative to explore localization and criticality. <LINK> Quasiperiodic systems are neither periodic nor disordered, but somewhere in between. One of the most simple examples is the Aubry-André-Harper (AAH) model, a one-dimensional model resulting from the superposition of two incommensurate lattices. <LINK> Turns out that the AAH model features a metal-insulator transition! For certain parameters all the states of the system suddently become localized. Moreover, exactly at the transition point, the states of the system are critical and show multifractal properties. <LINK> One can ask what happens when considering more complicated systems, since nature is usually not that simplistic. In particular, what happens when particles interact in such a context? Does the nature of these interactions affect the localization properties and the transition? <LINK> In our work, we answer these questions for the case of a model with long-range superconducting pairing. Indeed, we see that the nature of the transition changes remarkably, leading to extended multifractal regimes and hybridization of energy bands with different properties. <LINK> Finally, we also put together a toolbox of different methods to characterize localized, ergodic and multifractal states. The code to reproduce all the data and the figures can be found in <LINK>. (PS: In case anyone is interested in the topological properties of this same model, we published a study one year ago in Physical Review Research: <LINK>) @aBohrdt @ICFOnians @adauphin4 Thanks!😊",https://arxiv.org/abs/2201.05458,"In the presence of quasiperiodic potentials, the celebrated Kitaev chain presents an intriguing phase diagram with ergodic, localized and and multifractal states. In this work, we generalize these results by studying the localization properties of the Aubry-Andr\'e-Harper model in the presence of long-range hopping and superconducting pairing amplitudes. These amplitudes decay with power-law exponents $\xi$ and $\alpha$ respectively. To this end, we review and compare a toolbox of global and local characterization methods in order to investigate different types of transitions between ergodic, localized and multifractal states. We report energy-dependent transitions from ergodic to multifractal states for pairing terms with $\alpha<1$ and energy-dependent transitions from ergodic to localized states with an intermediate multifractal region for $\alpha>1$. The size of the intermediate multifractal region depends not only on the value of the superconducting pairing term $\Delta$, but also on the energy band. The transitions are not described by a mobility edge, but instead we report hybridization of bands with different types of localization properties. This leads to coexisting multifractal regimes where fractal dimensions follow different distributions. ","Localization and multifractal properties of the long-range Kitaev chain
  in the presence of an Aubry-Andr\'e-Harper modulation",10,"['I am very happy to share our latest work, which can now be found on arXiv:\n\n<LINK>\n\nIn this paper, we study the effect of long-range superconducting pairing on the localization properties of a quasi-periodic chain. 👇🧵 \n@ICFOnians @adauphin4', 'Understanding metal-insulator transitions has been one of the central questions in condensed matter physics. One of the most paradigmatic examples of such transitions is Anderson localization, in which a system becomes insulating in presence of disorder. https://t.co/JrQl1wGrfu', 'The study of Anderson localization in disordered systems needs at least two-dimensions and averaging over many disorder realizations. In contrast, quasi-periodic systems have gained a lot of attention as an alternative to explore localization and criticality. https://t.co/FkZv9a4Ud6', 'Quasiperiodic systems are neither periodic nor disordered, but somewhere in between. One of the most simple examples is the Aubry-André-Harper (AAH) model, a one-dimensional model resulting from the superposition of two incommensurate lattices. https://t.co/1uyjZA8zMQ', 'Turns out that the AAH model features a metal-insulator transition! For certain parameters all the states of the system suddently become localized. Moreover, exactly at the transition point, the states of the system are critical and show multifractal properties. https://t.co/eys0gVUEUQ', 'One can ask what happens when considering more complicated systems, since nature is usually not that simplistic. In particular, what happens when particles interact in such a context? Does the nature of these interactions affect the localization properties and the transition? https://t.co/MXuJQKhG6c', 'In our work, we answer these questions for the case of a model with long-range superconducting pairing. Indeed, we see that the nature of the transition changes remarkably, leading to extended multifractal regimes and hybridization of energy bands with different properties. https://t.co/wLHSOIpmru', 'Finally, we also put together a toolbox of different methods to characterize localized, ergodic and multifractal states. The code to reproduce all the data and the figures can be found in  https://t.co/Qw2Bc78Mr6.', '(PS: In case anyone is interested in the topological properties of this same model, we published a study one year ago in Physical Review Research: https://t.co/GKcNSZdw8A)', '@aBohrdt @ICFOnians @adauphin4 Thanks!😊']",22,01,2245
19,63,1083410795407372293,70874545,Josh Lothringer,"Check out a new paper (<LINK>) led by Ian Crossfield, where we detect 3(!) isotopologues of CO in both stars of an M-dwarf binary. This gives us C12/C13 and O16/O18 ratios, telling us this binary was enriched by a massive core-collapse SN. You can hear Ian talk about this in Session 420 this morning at 11:20 at #aas233, 10 minutes after I'm done giving my dissertation talk in Session 404! @StellarTayar That's a good question- I don't have a good intuition when it comes to stellar abundances so I would probably chalk it up to uncertainty. Maybe that is good reason to get add'l observations though.",https://arxiv.org/abs/1901.02607,"Low-mass M dwarfs represent the most common outcome of star formation, but their complex emergent spectra hinder detailed studies of their composition and initial formation. The measurement of isotopic ratios is a key tool that has been used to unlock the formation of our Solar System, the Sun, and the nuclear processes within more massive stars. We observed GJ 745AB, two M dwarfs orbiting in a wide binary, with the IRTF/iSHELL spectrograph. Our spectroscopy of CO in these stars at the 4.7 micron fundamental and 2.3 micron first-overtone rovibrational bandheads reveals 12C16O, 13C16O, and 12C18O in their photospheres. Since the stars are fully convective, the atomic constituents of these isotopologues should be uniformly mixed throughout the stars' interiors. We find that in these M dwarfs, both 12C/13C and 16O/18O greatly exceed the Solar values. These measurements cannot be explained solely by models of Galactic chemical evolution, but require that the stars formed from an ISM significantly enriched by material ejected from an exploding core-collape supernova. These isotopic measurements complement the elemental abundances provided by large-scale spectroscopic surveys, and open a new window onto studies of Galactic evolution, stellar populations, and individual systems. ",Unusual Isotopic Abundances in a Fully-Convective Stellar Binary,3,"['Check out a new paper (<LINK>) led by Ian Crossfield, where we detect 3(!) isotopologues of CO in both stars of an M-dwarf binary. This gives us C12/C13 and O16/O18 ratios, telling us this binary was enriched by a massive core-collapse SN.', ""You can hear Ian talk about this in Session 420 this morning at 11:20 at #aas233, 10 minutes after I'm done giving my dissertation talk in Session 404!"", ""@StellarTayar That's a good question- I don't have a good intuition when it comes to stellar abundances so I would probably chalk it up to uncertainty. Maybe that is good reason to get add'l observations though.""]",19,01,603
20,74,1260921356306317314,1834931742,Yifan Qian,"Can we leverage Graph Convolutional Networks (@thomaskipf) to the data where no graph structure exists explicitly in empirical domains? In this new paper, we explore if graphs extracted from the features themselves can aid classification performance. <LINK> <LINK> We show that constructing optimal geometric graphs directly from data features can aid classification tasks on both synthetic and real-world data sets from different domains, ranging from text to music track features to single-cell transcriptomics. <LINK> Second, we introduce two metrics to characterize optimal graphs: i) by measuring the alignment between the subspaces spanned by the features convolved with the graph and the ground truth; <LINK> and ii) ratio of class separation in the output activations of Graph Convolutional Networks: this shows that the optimal graph maximally separates classes. <LINK> Finally, we find that sparsifying the optimal graph can potentially improve classification performance. <LINK> Joint work with Paul Expert (@ExpertPol), Pietro Panzarasa and Mauricio Barahona.",https://arxiv.org/abs/2005.04081,"Traditional classification tasks learn to assign samples to given classes based solely on sample features. This paradigm is evolving to include other sources of information, such as known relations between samples. Here we show that, even if additional relational information is not available in the data set, one can improve classification by constructing geometric graphs from the features themselves, and using them within a Graph Convolutional Network. The improvement in classification accuracy is maximized by graphs that capture sample similarity with relatively low edge density. We show that such feature-derived graphs increase the alignment of the data to the ground truth while improving class separation. We also demonstrate that the graphs can be made more efficient using spectral sparsification, which reduces the number of edges while still improving classification performance. We illustrate our findings using synthetic and real-world data sets from various scientific domains. ","Geometric graphs from data to aid classification tasks with graph
  convolutional networks",6,"['Can we leverage Graph Convolutional Networks (@thomaskipf) to the data where no graph structure exists explicitly in empirical domains? In this new paper, we explore if graphs extracted from the features themselves can aid classification performance.\n<LINK> <LINK>', 'We show that constructing optimal geometric graphs directly from data features can aid classification tasks on both synthetic and real-world data sets from different domains, ranging from text to music track features to single-cell transcriptomics. https://t.co/y94jXyugSm', 'Second, we introduce two metrics to characterize optimal graphs: i) by measuring the alignment between the subspaces spanned by the features convolved with the graph and the ground truth; https://t.co/hxYzYpF0AE', 'and ii) ratio of class separation in the output activations of Graph Convolutional Networks: this shows that the optimal graph maximally separates classes. https://t.co/ayZPqt2u40', 'Finally, we find that sparsifying the optimal graph can potentially improve classification performance. https://t.co/BnNdPow35E', 'Joint work with Paul Expert (@ExpertPol), Pietro Panzarasa and Mauricio Barahona.']",20,05,1071
21,19,1354837390188109831,1515424688,Armen Aghajanyan,"I'm happy to present our new paper MUPPET (<LINK>), arguing for an additional stage between pre-training and fine-tuning, called pre-finetuning which uses massively multi-task learning (~50 tasks) to further refine representations. Recent work has shown gains from MTL/multi-stage fine-tuning, but it can be hard to know which intermediate tasks will best transfer. We show that multi-task supervised tuning is effective if done at scale (# tasks), removing the need to pre-select the best intermediate tasks. Our multi-task set up consists of 46 datasets across 4 task types with close to 5 million supervised samples, using a classification head for each classification dataset, and unified heads for MRC and CommonSense tasks. <LINK> After solving practical problems (loss scaling, heterogeneous batches, etc) we pre-finetune RoBERTa variants and BART. We first look at fine-tuning MUPPET over datasets available in the pre-finetuning regime. MUPPET unanimously improves over it’s base model for GLUE/SQuAD. <LINK> MUPPET also improves over it’s base model for sentence prediction, commonsense, and summarization tasks. <LINK> To show that MUPPET representations are more generalizable, we also measure MUPPET performance over datasets not available in the pre-finetuning regime. MUPPET once again improves consistently over it’s only pre-trained counterparts. <LINK> MTL historically has given inconclusive results. So why does MUPPET work? Turns out scale is fundamental for MTL. There exists a critical # of tasks (~15) under which pre-finetuning degrades representations. But over this critical point linearly improves representations. <LINK> As another side-effect, MUPPET variants of pre-trained models show much better data-efficiency for downstream fine-tuning. This was joint work with great authors Anchit Gupta, @AkshatS07, Xilun Chen, @LukeZettlemoyer, @sonalsgupta",https://arxiv.org/abs/2101.11038,"We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.~RoBERTa) and generation models (e.g.~BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks. ",Muppet: Massive Multi-task Representations with Pre-Finetuning,9,"[""I'm happy to present our new paper MUPPET (<LINK>), arguing for an additional stage between pre-training and fine-tuning, called pre-finetuning which uses massively multi-task learning (~50 tasks) to further refine representations."", 'Recent work has shown gains from MTL/multi-stage fine-tuning, but it can be hard to know which intermediate tasks will best transfer. We show that multi-task supervised tuning is effective if done at scale (# tasks), removing the need to pre-select the best intermediate tasks.', 'Our multi-task set up consists of 46 datasets across 4 task types with close to 5 million supervised samples, using a classification head for each classification dataset, and unified heads for MRC and CommonSense tasks. https://t.co/4Nw4oQlbRo', 'After solving practical problems (loss scaling, heterogeneous batches, etc) we pre-finetune RoBERTa variants and BART. We first look at fine-tuning MUPPET over datasets available in the pre-finetuning regime. MUPPET unanimously improves over it’s base model for GLUE/SQuAD. https://t.co/UtrMe18vnW', 'MUPPET also improves over it’s base model for sentence prediction, commonsense, and summarization tasks. https://t.co/wTNwum3BB5', 'To show that MUPPET representations are more generalizable, we also measure MUPPET performance over datasets not available in the pre-finetuning regime. MUPPET once again improves consistently over it’s only pre-trained counterparts. https://t.co/lrYvGnZtBD', 'MTL historically has given inconclusive results. So why does MUPPET work? Turns out scale is fundamental for MTL. There exists a critical # of tasks (~15) under which pre-finetuning degrades representations. But over this critical point linearly improves representations. https://t.co/AJmjtfHvxp', 'As another side-effect, MUPPET variants of pre-trained models show much better data-efficiency for downstream fine-tuning.', 'This was joint work with great authors Anchit Gupta, @AkshatS07, Xilun Chen, @LukeZettlemoyer, @sonalsgupta']",21,01,1880
22,56,1062638170993500161,356676252,John Regan,"Check out our new paper (<LINK>) with colleagues from the @astroIAP. Massive black hole seeds can initially have super-Eddington accretion rates but jet outflows quickly regulate the accretion to below Eddington. Conclusion: Black holes are hard to grow quickly! Co-authors: @TurloughDownes, @maximetrebitsch, @RicardaBeckmann, Marta Volonteri, Alessandro Lupi & Yohan Dubois @cfar_dcu @DCUMaths @DCUFSH @MSCActions <LINK>",https://arxiv.org/abs/1811.04953,"Super-Eddington accretion onto massive black hole seeds may be commonplace in the early Universe, where the conditions exist for rapid accretion. Direct collapse black holes are often invoked as a possible solution to the observation of super massive black holes (SMBHs) in the pre-reionisation Universe. We investigate here how feedback, mainly in the form of bipolar jets, from super-Eddington accreting seed black holes will affect their subsequent growth. We find that, nearly independent of the mass loading of the bipolar jets, the violent outflows generated by the jets evacuate a region of approximately 0.1 pc surrounding the black hole seed. However, the jet outflows are unable to break free of the halo and their impact is limited to the immediate vicinity of the black hole. The outflows suppress any accretion for approximately a dynamical time. The gas then cools, recombines and falls back to the centre where high accretion rates are again observed. The overall effect is to create an effective accretion rate with values of between 0.1 and 0.5 times the Eddington rate. If this episodic accretion rate is maintained for order 500 million years then the black hole will increase in mass by a factor of between 3 and 300 but far short of the factor of $10^4$ required for the seeds to become the SMBHs observed at $z>6$. Therefore, direct collapse black holes born into atomic cooling haloes and which experience strong negative mechanical feedback will require external influences (e.g. rapid major mergers with other haloes) to promote efficient accretion and reach SMBH masses within a few hundred million years. ","Super-Eddington Accretion and Feedback from the First Massive Seed Black
  Holes",3,"['Check out our new paper (<LINK>) with colleagues from the @astroIAP. Massive black hole seeds can initially have super-Eddington accretion rates but jet outflows quickly regulate the accretion to below Eddington. Conclusion: Black holes are hard to grow quickly!', 'Co-authors: @TurloughDownes, @maximetrebitsch, @RicardaBeckmann, Marta Volonteri, Alessandro Lupi &amp; Yohan Dubois\n@cfar_dcu @DCUMaths @DCUFSH @MSCActions', 'https://t.co/hhAyFeR6Ag']",18,11,422
23,331,1312120926985576449,446694758,Julian Eisenschlos,"Entailment has been studied in depth for textual premises, but the case with structured data like tables or even HTML can have many applications in the wild.  We tackle this in our latest #EMNLP2020 Findings paper <LINK> with Syrine Krichene and @muelletm 1/5 We extend TAPAS (Herzig et al, 2020), originally pretrained with MLM, to predict if a table entails or refutes a sentence and eval on TabFact (Chen et al, 2020). We introduce 2 novel pretraining binary-classification tasks called Counterfactual and Synthetic, shown in image. 2/5 <LINK> Counterfactual examples are created by swapping entities that appear in both a table and a sentence for a plausible alternative: they are realistic but simple. Synthetic ones are sampled from a small pCFG based on the values of a real table: they improve numerical reasoning. 3/5 <LINK> Pretraining with these 2 tasks, we improve SOTA by ~10pts on TabFact and, interestingly, also get a new SOTA on the table QA task SQA (Iyyer et al, 2017). This results hold even with fraction of the data, and is only 2 points below a strong baseline with no data at all! 4/5 <LINK> Finally, we investigate how to deal with large tables by selecting which parts of the input to pass through the model using simple heuristics. We can can get 2x speed-ups with ~1pt acc drop, or 4x still above prior art. Code and models coming soon at <LINK> 5/5",http://arxiv.org/abs/2010.00571,"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets. ",Understanding tables with intermediate pre-training,5,"['Entailment has been studied in depth for textual premises, but the case with structured data like tables or even HTML can have many applications in the wild. \n\nWe tackle this in our latest #EMNLP2020 Findings paper <LINK> with Syrine Krichene and @muelletm\n\n1/5', 'We extend TAPAS (Herzig et al, 2020), originally pretrained with MLM, to predict if a table entails or refutes a sentence and eval on TabFact (Chen et al, 2020). We introduce 2 novel pretraining binary-classification tasks called Counterfactual and Synthetic, shown in image.\n\n2/5 https://t.co/4s84vzcut1', 'Counterfactual examples are created by swapping entities that appear in both a table and a sentence for a plausible alternative: they are realistic but simple. Synthetic ones are sampled from a small pCFG based on the values of a real table: they improve numerical reasoning.\n\n3/5 https://t.co/f17pVpWqiu', 'Pretraining with these 2 tasks, we improve SOTA by ~10pts on TabFact and, interestingly, also get a new SOTA on the table QA task SQA (Iyyer et al, 2017). This results hold even with fraction of the data, and is only 2 points below a strong baseline with no data at all!\n\n4/5 https://t.co/RAakCIRKGn', 'Finally, we investigate how to deal with large tables by selecting which parts of the input to pass through the model using simple heuristics. We can can get 2x speed-ups with ~1pt acc drop, or 4x still above prior art. Code and models coming soon at https://t.co/1VxCSrSunr\n\n5/5']",20,10,1377
24,141,1317091718664159232,1055880097595564034,Rohini Giles,"Our new paper about phosphine in Venus' atmosphere, led by Thérèse Encrenaz, has been accepted in A&A and is available on arXiv: <LINK> <LINK> We use infrared observations from TEXES/IRTF and don't find any evidence of phosphine absorption, giving an upper limit of 5 ppbv. The difference from the Greaves et al. paper could be due to probing slightly different levels in the atmosphere or PH3 variability.",https://arxiv.org/abs/2010.07817,"Following the announcement of the detection of phosphine (PH$_3$) in the cloud deck of Venus at millimeter wavelengths, we have searched for other possible signatures of this molecule in the infrared range. Since 2012, we have been observing Venus in the thermal infrared at various wavelengths to monitor the behavior of SO$_2$ and H$_2$O at the cloud top. We have identified a spectral interval recorded in March 2015 around 950 cm$^{-1}$ where a PH$_3$ transition is present. From the absence of any feature at this frequency, we derive, on the disk-integrated spectrum, a 3-$\sigma$ upper limit of 5 ppbv for the PH$_3$ mixing ratio, assumed to be constant throughout the atmosphere. This limit is 4 times lower than the disk-integrated mixing ratio derived at millimeter wavelengths. Our result brings a strong constraint on the maximum PH$_3$ abundance at the cloud top and in the lower mesosphere of Venus. ","A stringent upper limit of the PH$_3$ abundance at the cloud top of
  Venus",2,"[""Our new paper about phosphine in Venus' atmosphere, led by Thérèse Encrenaz, has been accepted in A&amp;A and is available on arXiv: <LINK> <LINK>"", ""We use infrared observations from TEXES/IRTF and don't find any evidence of phosphine absorption, giving an upper limit of 5 ppbv. The difference from the Greaves et al. paper could be due to probing slightly different levels in the atmosphere or PH3 variability.""]",20,10,406
25,55,1230644029311901703,494134136,Krzysztof Geras,"We just released a new paper on deep learning for screening mammography! “An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization”. <LINK> 1/7 <LINK> Despite learning only with image-level labels, the model achieves an AUC of 0.93, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, it is 4.1x faster while using 78.4% less memory. In a reader study, it surpasses radiologist-level AUC by a margin of 0.11. 2/7 <LINK> Our model works in 3 stages. (1) Looking at the entire image with a network of a relatively low capacity to identify the most informative patches. (2) Looking at these patches with a network of a higher capacity. (3) Integrating information obtained in stages (1) and (2). 3/7 <LINK> We conducted extensive experiments on the importance of various design choices in the model, the type of pooling aggregating saliency maps, the number of patches extracted to examine further, ... 4/7 <LINK> We are expecting that our model will be especially useful when applied to data for which it is difficult or impossible to collect pixel-level annotations. We also hypothesize that this or similar models could be useful to discover new biomarkers. 5/7 <LINK> We made the code and the model public at <LINK>. We are hoping to enable others to experiment with and build upon our model! Please let us know if you find it useful. 6/7 This paper is another product of a collaboration between @cai2r and @NYUDataScience. It was led by @ArtieShen, supported by @NanWu__, @zhansheng, @jpatrickpark, Kangning Liu, @TyagiSudarshini, Laura Heacock, S. Gene Kim, @DrLindaMoy, @kchonyc and myself. 7/7",http://arxiv.org/abs/2002.07613,"Medical images differ from natural images in significantly higher resolutions and smaller regions of interest. Because of these differences, neural network architectures that work well for natural images might not be applicable to medical image analysis. In this work, we extend the globally-aware multiple instance classifier, a framework we proposed to address these unique properties of medical images. This model first uses a low-capacity, yet memory-efficient, network on the whole image to identify the most informative regions. It then applies another higher-capacity network to collect details from chosen regions. Finally, it employs a fusion module that aggregates global and local information to make a final prediction. While existing methods often require lesion segmentation during training, our model is trained with only image-level labels and can generate pixel-level saliency maps indicating possible malignant findings. We apply the model to screening mammography interpretation: predicting the presence or absence of benign and malignant lesions. On the NYU Breast Cancer Screening Dataset, consisting of more than one million images, our model achieves an AUC of 0.93 in classifying breasts with malignant findings, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, our model is 4.1x faster for inference while using 78.4% less GPU memory. Furthermore, we demonstrate, in a reader study, that our model surpasses radiologist-level AUC by a margin of 0.11. The proposed model is available online: this https URL ","An interpretable classifier for high-resolution breast cancer screening
  images utilizing weakly supervised localization",7,"['We just released a new paper on deep learning for screening mammography!\n\n“An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization”.\n\n<LINK>\n\n1/7 <LINK>', 'Despite learning only with image-level labels, the model achieves an AUC of 0.93, outperforming ResNet-34 and Faster R-CNN. Compared to ResNet-34, it is 4.1x faster while using 78.4% less memory. In a reader study, it surpasses radiologist-level AUC by a margin of 0.11.\n\n2/7 https://t.co/d0PdA0ZBg9', 'Our model works in 3 stages. (1) Looking at the entire image with a network of a relatively low capacity to identify the most informative patches. (2) Looking at these patches with a network of a higher capacity. (3) Integrating information obtained in stages (1) and (2).\n\n3/7 https://t.co/YsoNpVmhjs', 'We conducted extensive experiments on the importance of various design choices in the model, the type of pooling aggregating saliency maps, the number of patches extracted to examine further, ...\n\n4/7 https://t.co/4uZr23aEcC', 'We are expecting that our model will be especially useful when applied to data for which it is difficult or impossible to collect pixel-level annotations. We also hypothesize that this or similar models could be useful to discover new biomarkers.\n\n5/7 https://t.co/I4S9N87lcX', 'We made the code and the model public at https://t.co/xATXueLLWw. We are hoping to enable others to experiment with and build upon our model! Please let us know if you find it useful.\n\n6/7', 'This paper is another product of a collaboration between @cai2r and @NYUDataScience. It was led by @ArtieShen, supported by @NanWu__, @zhansheng, @jpatrickpark, Kangning Liu, @TyagiSudarshini, Laura Heacock, S. Gene Kim, @DrLindaMoy, @kchonyc and myself.\n\n7/7']",20,02,1674
26,86,1252649152158355462,2818695390,Sasho Nikolov,"New paper with Vivek Madan, @mohitsinghr, and Tao Tantipongpipat, the result of an awesome visit to Atlanta last August (which feels approximately a century ago). ""Maximizing Determinants under Matroid Constraints"" <LINK> The problem: given n rank-1 d-by-d matrices, and a matroid of rank k over them, find a basis B of the matroid so that the sum of the matrices in B has the largest determinant. This shows up in many settings: optimal design, network design, allocation of goods, ML. We give the first algorithms with approximation factor that only depends on the dimension d, and not on the rank k. The main idea is to show that a convex relaxation of the problem has an optimal solution with only d^2 fractional variables. Surprising given the non-linearity. We also leverage known and cool connections with real stable & completely log-concave polynomials. We show that a relaxation studied by myself and Mohit, and also by Straszak and @NisheethVishnoi, is at least as strong as one by @nimaanari and @oveisgharan.",https://arxiv.org/abs/2004.07886,"Given vectors $v_1,\dots,v_n\in\mathbb{R}^d$ and a matroid $M=([n],I)$, we study the problem of finding a basis $S$ of $M$ such that $\det(\sum_{i \in S}v_i v_i^\top)$ is maximized. This problem appears in a diverse set of areas such as experimental design, fair allocation of goods, network design, and machine learning. The current best results include an $e^{2k}$-estimation for any matroid of rank $k$ and a $(1+\epsilon)^d$-approximation for a uniform matroid of rank $k\ge d+\frac d\epsilon$, where the rank $k\ge d$ denotes the desired size of the optimal set. Our main result is a new approximation algorithm with an approximation guarantee that depends only on the dimension $d$ of the vectors and not on the size $k$ of the output set. In particular, we show an $(O(d))^{d}$-estimation and an $(O(d))^{d^3}$-approximation for any matroid, giving a significant improvement over prior work when $k\gg d$. Our result relies on the existence of an optimal solution to a convex programming relaxation for the problem which has sparse support; in particular, no more than $O(d^2)$ variables of the solution have fractional values. The sparsity results rely on the interplay between the first-order optimality conditions for the convex program and matroid theory. We believe that the techniques introduced to show sparsity of optimal solutions to convex programs will be of independent interest. We also give a randomized algorithm that rounds a sparse fractional solution to a feasible integral solution to the original problem. To show the approximation guarantee, we utilize recent works on strongly log-concave polynomials and show new relationships between different convex programs studied for the problem. Finally, we use the estimation algorithm and sparsity results to give an efficient deterministic approximation algorithm with an approximation guarantee that depends solely on the dimension $d$. ",Maximizing Determinants under Matroid Constraints,4,"['New paper with Vivek Madan, @mohitsinghr, and Tao Tantipongpipat, the result of an awesome visit to Atlanta last August (which feels approximately a century ago). ""Maximizing Determinants under Matroid Constraints"" <LINK>', 'The problem: given n rank-1 d-by-d matrices, and a matroid of rank k over them, find a basis B of the matroid so that the sum of the matrices in B has the largest determinant. This shows up in many settings: optimal design, network design, allocation of goods, ML.', 'We give the first algorithms with approximation factor that only depends on the dimension d, and not on the rank k. The main idea is to show that a convex relaxation of the problem has an optimal solution with only d^2 fractional variables. Surprising given the non-linearity.', 'We also leverage known and cool connections with real stable &amp; completely log-concave polynomials. We show that a relaxation studied by myself and Mohit, and also by Straszak and @NisheethVishnoi, is at least as strong as one by @nimaanari and @oveisgharan.']",20,04,1021
27,21,1509590666979930116,774170436057731073,Alexis Conneau,"🚨[🗣️🔊💻💬]🚨 Excited to share our new benchmark ""XTREME-S"" to accelerate speech technologies for all XTREME-S evaluates recognition, translation, classification and retrieval in 100+ languages. Paper: <LINK> Code: <LINK> We have worked hard to try & make things simple for practitioners (easy download of public data, single fine-tuning for multilingual datasets) We hope this will catalyze research in data-efficient approaches like self-supervised speech representation learning for all languages <LINK> This is the result of work done by many amazing folks across @GoogleAI, @huggingface, and @MetaAI.  Please consider using XTREME-S so that as a community, we can accelerate progress on speech technology for the benefit of all. <LINK>",https://arxiv.org/abs/2203.10752,"We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102 languages from 10+ language families, 3 different domains and 4 task families, XTREME-S aims to simplify multilingual speech representation evaluation, as well as catalyze research in ""universal"" speech representation learning. This paper describes the new benchmark and establishes the first speech-only and speech-text baselines using XLS-R and mSLAM on all downstream tasks. We motivate the design choices and detail how to use the benchmark. Datasets and fine-tuning scripts are made easily accessible at this https URL ",XTREME-S: Evaluating Cross-lingual Speech Representations,3,"['🚨[🗣️🔊💻💬]🚨 Excited to share our new benchmark ""XTREME-S"" to accelerate speech technologies for all\n\nXTREME-S evaluates recognition, translation, classification and retrieval in 100+ languages.\n\nPaper: <LINK>\nCode: <LINK>', 'We have worked hard to try &amp; make things simple for practitioners (easy download of public data, single fine-tuning for multilingual datasets)\n\nWe hope this will catalyze research in data-efficient approaches like self-supervised speech representation learning for all languages https://t.co/pBYvpjHQhU', 'This is the result of work done by many amazing folks across @GoogleAI, @huggingface, and @MetaAI. \n\nPlease consider using XTREME-S so that as a community, we can accelerate progress on speech technology for the benefit of all.\n\nhttps://t.co/Pq7IIyO6HG']",22,03,736
28,56,1471138693771673604,373885410,Athul Paul Jacob,"⭐New paper⭐ How do you build AI agents that are both strong and human-like? Regularize search towards a human policy! In chess, Go and no-press Diplomacy, we get SOTA human prediction accuracy while being much stronger than imitation learning. <LINK> (1/9)🧵👇 <LINK> Even though self-play AI algorithms based on search exceed top humans in several games, the resulting policies are often inhuman. While imitation learning is effective at predicting human actions, it does not match the strength of expert humans. (2/9) We show that KL-regularizing search towards a human imitation-learned policy results in strong and human-like gameplay in perfect-information games like chess and Go as well as in imperfect-information games like no-press Diplomacy. (3/9) In chess and Go, we show that standard MCTS (green) with a human imitation learned policy prior and value function surpasses prior state-of-the-art results (blue) for human prediction accuracy, while also being substantially stronger than the imitation learned policies. (4/9) <LINK> MCTS is ineffective in imperfect-information games like Poker and Diplomacy. In these games, algorithms based on regret minimization are the leading approaches. (5/9) We introduce piKL-hedge, the first regret minimization algorithm to incorporate a cost term proportional to the KL divergence between the search policy and a human-imitation learned policy. We study piKL-hedge in no-press Diplomacy. (6/9) piKL-Hedge (green) can produce policies that predict human play with the same accuracy as imitation learning (blue) while being stronger by a factor of 1.4; or alternately a policy that outperforms unregularized search (yellow) while achieving greater prediction accuracy. (7/9) <LINK> Huge thanks to all my awesome co-authors, @lightvector1*, @gabrfarina*, @adamlerer, @anton_bakhtin, @jacobandreas, and @polynoamial! (8/9) This work was done during my internship with the FAIR multi-agent learning group (who are hiring interns!). Special thanks to @polynoamial for hosting me! (9/9) @egrefen Certainly related! KL-based objectives have been used extensively in RL. Our focus in this paper has been towards studying how KL can be leveraged in different inference-time search-based methods to allow for stronger agents while also improving their ""human-likeness"". @egrefen Exactly!",https://arxiv.org/abs/2112.07544,"We consider the task of building strong but human-like policies in multi-agent decision-making problems, given examples of human behavior. Imitation learning is effective at predicting human actions but may not match the strength of expert humans, while self-play learning and search techniques (e.g. AlphaZero) lead to strong performance but may produce policies that are difficult for humans to understand and coordinate with. We show in chess and Go that regularizing search based on the KL divergence from an imitation-learned policy results in higher human prediction accuracy and stronger performance than imitation learning alone. We then introduce a novel regret minimization algorithm that is regularized based on the KL divergence from an imitation-learned policy, and show that using this algorithm for search in no-press Diplomacy yields a policy that matches the human prediction accuracy of imitation learning while being substantially stronger. ",Modeling Strong and Human-Like Gameplay with KL-Regularized Search,11,"['⭐New paper⭐\nHow do you build AI agents that are both strong and human-like? Regularize search towards a human policy! In chess, Go and no-press Diplomacy, we get SOTA human prediction accuracy while being much stronger than imitation learning.\n<LINK>\n(1/9)🧵👇 <LINK>', 'Even though self-play AI algorithms based on search exceed top humans in several games, the resulting policies are often inhuman. While imitation learning is effective at predicting human actions, it does not match the strength of expert humans.\n(2/9)', 'We show that KL-regularizing search towards a human imitation-learned policy results in strong and human-like gameplay in perfect-information games like chess and Go as well as in imperfect-information games like no-press Diplomacy.\n(3/9)', 'In chess and Go, we show that standard MCTS (green) with a human imitation learned policy prior and value function surpasses prior state-of-the-art results (blue) for human prediction accuracy, while also being substantially stronger than the imitation learned policies.\n(4/9) https://t.co/6bZ9JTVyCu', 'MCTS is ineffective in imperfect-information games like Poker and Diplomacy. In these games, algorithms based on regret minimization are the leading approaches.\n(5/9)', 'We introduce piKL-hedge, the first regret minimization algorithm to incorporate a cost term proportional to the KL divergence between the search policy and a human-imitation learned policy. We study piKL-hedge in no-press Diplomacy.\n(6/9)', 'piKL-Hedge (green) can produce policies that predict human play with the same accuracy as imitation learning (blue) while being stronger by a factor of 1.4;  or alternately a policy that outperforms unregularized search (yellow) while achieving greater prediction accuracy.\n(7/9) https://t.co/M1Zf9nGjKN', 'Huge thanks to all my awesome co-authors, @lightvector1*, @gabrfarina*, @adamlerer, @anton_bakhtin, @jacobandreas, and @polynoamial!\n(8/9)', 'This work was done during my internship with the FAIR multi-agent learning group (who are hiring interns!). Special thanks to @polynoamial for hosting me!\n(9/9)', '@egrefen Certainly related! KL-based objectives have been used extensively in RL. Our focus in this paper has been towards studying how KL can be leveraged in different inference-time search-based methods to allow for stronger agents while also improving their ""human-likeness"".', '@egrefen Exactly!']",21,12,2329
29,78,1191695915700117504,216729597,Marcel S. Pawlowski,"Our paper lead by Pengfei Li, @lellifede & @DudeDarkmatter on a new measurement of the dark matter halo mass function from HI kinematics has been accepted! <LINK> @lellifede has you covered with the twitter summary of this work’s context and results. <LINK>",https://arxiv.org/abs/1911.00517v1,"We present an empirical method to measure the halo mass function (HMF) of galaxies. We determine the relation between the \hi\ line-width from single-dish observations and the dark matter halo mass ($M_{200}$) inferred from rotation curve fits in the SPARC database, then we apply this relation to galaxies from the \hi\ Parkes All Sky Survey (HIPASS) to derive the HMF. This empirical HMF is well fit by a Schecther function, and matches that expected in $\Lambda$CDM over the range $10^{10.5} < M_{200} < 10^{12}\;\mathrm{M}_{\odot}$. More massive halos must be poor in neutral gas to maintain consistency with the power law predicted by $\Lambda$CDM. We detect no discrepancy at low masses. The lowest halo mass probed by HIPASS, however, is just greater than the mass scale where the Local Group missing satellite problem sets in. The integrated mass density associated with the dark matter halos of \hi-detected galaxies sums to $\Omega_{\rm m,gal} \approx 0.03$ over the probed mass range. ",] The halo mass function of late-type galaxies from HI kinematics,1,"['Our paper lead by Pengfei Li, @lellifede &amp; @DudeDarkmatter on a new measurement of the dark matter halo mass function from HI kinematics has been accepted! <LINK>\n\n@lellifede has you covered with the twitter summary of this work’s context and results. <LINK>']",19,11,257
30,104,1358699269310607362,738769492122214400,Johannes Lischner,"In our new paper, we calculate the band structures of all twisted homo- and heterobilayers composed of MoS2, WS2, MoSe2 and WSe2: <LINK>. So many interesting findings - pls see thread below. #compchem #2dmaterials <LINK> Relaxations matter...A LOT! In homobilayers relaxations change the symmetry of the highest valence states from hexagonal to triangular. In heterobilayers, relaxations can change the ordering of states suggesting that these materials are highly sensitive to pressure. In many twisted bilayer materials flat bands are found...but not in all: in some heterobilayers, the highest valence band remains dispersive even at small twist angles. Such valence bands originate from K/K'-valleys of the monolayer. Such K/K'-derived dispersive bands are spin-polarized, but the flat bands are not. In some heterobilayers, we observe a weird phenomenon: spin-split bands (almost) without spin polarization. Finally, we present a new and efficient tight-binding model to describe twisted bilayer transition-metal dichalcogenides and give a complete set of parameters for all possible combinations.",https://arxiv.org/abs/2102.03259,"Twisted bilayers of two-dimensional materials, such as twisted bilayer graphene, often feature flat electronic bands that enable the observation of electron correlation effects. In this work, we study the electronic structure of twisted transition metal dichalcogenide (TMD) homo- and heterobilayers that are obtained by combining MoS$_2$, WS$_2$, MoSe$_2$ and WSe$_2$ monolayers, and show how flat band properties depend on the chemical composition of the bilayer as well as its twist angle. We determine the relaxed atomic structure of the twisted bilayers using classical force fields and calculate the electronic band structure using a tight-binding model parametrized from first-principles density-functional theory. We find that the highest valence bands in these systems can derive either from $\Gamma$-point or $K$/$K'$-point states of the constituent monolayers. For homobilayers, the two highest valence bands are composed of monolayer $\Gamma$-point states, exhibit a graphene-like dispersion and become flat as the twist angle is reduced. The situation is more complicated for heterobilayers where the ordering of $\Gamma$-derived and $K$/$K'$-derived states depends both on the material composition and also the twist angle. In all systems, qualitatively different band structures are obtained when atomic relaxations are neglected. ","Flat band properties of twisted transition metal dichalcogenide homo-
  and heterobilayers of MoS$_2$, MoSe$_2$, WS$_2$ and WSe$_2$",5,"['In our new paper, we calculate the band structures of all twisted homo- and heterobilayers composed of MoS2, WS2, MoSe2 and WSe2: <LINK>. So many interesting findings - pls see thread below.  #compchem #2dmaterials <LINK>', 'Relaxations matter...A LOT! In homobilayers relaxations change the symmetry of the highest valence states from hexagonal to triangular. In heterobilayers, relaxations can change the ordering of states suggesting that these materials are highly sensitive to pressure.', ""In many twisted bilayer materials flat bands are found...but not in all: in some heterobilayers, the highest valence band remains dispersive even at small twist angles. Such valence bands originate from K/K'-valleys of the monolayer."", ""Such K/K'-derived dispersive bands are spin-polarized, but the flat bands are not. In some heterobilayers, we observe a weird phenomenon: spin-split bands (almost) without spin polarization."", 'Finally, we present a new and efficient tight-binding model to describe twisted bilayer transition-metal dichalcogenides and give a complete set of parameters for all possible combinations.']",21,02,1102
31,32,1364485567468208128,734484164070772736,Guy Tennenholtz,"GELATO: How do we leverage proximity and uncertainty to improve offline reinforcement learning (RL) algorithms? In our new paper we answer this question through variational pullback metrics of proximity and uncertainty. <LINK> GELATO: We construct Riemannian metrics on submanifolds induced by a variational forward model. These metrics, capturing both proximity and uncertainty w.r.t the data, are leveraged in a model based offline RL framework (MOPO) <LINK> GELATO is capable of capturing intrinsic characteristics of the data manifold, trading off proximity and uncertainty in order to enjoy the benefits of both worlds. Read more in our paper! <LINK>",https://arxiv.org/abs/2102.11327,"Offline reinforcement learning approaches can generally be divided to proximal and uncertainty-aware methods. In this work, we demonstrate the benefit of combining the two in a latent variational model. We impose a latent representation of states and actions and leverage its intrinsic Riemannian geometry to measure distance of latent samples to the data. Our proposed metrics measure both the quality of out of distribution samples as well as the discrepancy of examples in the data. We integrate our metrics in a model-based offline optimization framework, in which proximity and uncertainty can be carefully controlled. We illustrate the geodesics on a simple grid-like environment, depicting its natural inherent topology. Finally, we analyze our approach and improve upon contemporary offline RL benchmarks. ","GELATO: Geometrically Enriched Latent Model for Offline Reinforcement
  Learning",3,"['GELATO: How do we leverage proximity and uncertainty to improve offline reinforcement learning (RL) algorithms? In our new paper we answer this question through variational pullback metrics of proximity and uncertainty. <LINK>', 'GELATO: We construct Riemannian metrics on submanifolds induced by a variational forward model. These metrics, capturing both proximity and uncertainty w.r.t the data, are leveraged in a model based offline RL framework (MOPO) https://t.co/PkbJ69F4oa', 'GELATO is capable of capturing intrinsic characteristics of the data manifold, trading off proximity and uncertainty in order to enjoy the benefits of both worlds. Read more in our paper! https://t.co/PkbJ69F4oa']",21,02,655
32,67,1383073302697099264,1204291434842595328,Clément Rebuffel,"Happy to have contributed to QuestEval, with a new paper in collaboration with @ThomasScialom:  Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation! Paper is available on ArXiv: <LINK> 1/2 Your goal: measure Semantic Matching between generated text and structured input using QuestEval Your issue: in-domain Question Generation/Answering copora are needed to train QuestEval Our solution: build synthetic QG/QA datasets for any Data-to-Text Generation task! 2/2 <LINK>",https://arxiv.org/abs/2104.07555,"QuestEval is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to Data-to-Text tasks is not straightforward, as it requires multimodal Question Generation and Answering systems on the considered tasks, which are seldom available. To this purpose, we propose a method to build synthetic multimodal corpora enabling to train multimodal components for a data-QuestEval metric. The resulting metric is reference-less and multimodal; it obtains state-of-the-art correlations with human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's code and models available for reproducibility purpose, as part of the QuestEval project. ","Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic
  Evaluation",2,"['Happy to have contributed to QuestEval, with a new paper in collaboration with @ThomasScialom: \n\nData-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation!\n\nPaper is available on ArXiv: <LINK>\n\n1/2', 'Your goal: measure Semantic Matching between generated text and structured input using QuestEval\n\nYour issue: in-domain Question Generation/Answering copora are needed to train QuestEval\n\nOur solution: build synthetic QG/QA datasets for any Data-to-Text Generation task!\n\n2/2 https://t.co/NVfu7bMCIR']",21,04,492
33,199,1367646761494450176,1047899041311412224,Francois Grondin,"Here's the preprint of our new paper ""Audio scene monitoring using redundant un-localized microphone arrays"". I had the privilege to collaborate with researchers at University of California San Diego, including Pr. Peter Gerstoft and Pr. Yoav Freund. <LINK>",https://arxiv.org/abs/2103.01830,"We present a system for localizing sound sources in a room with several ad-hoc microphone arrays. Each circular array performs direction of arrival (DOA) estimation independently using commercial software. The DOAs are fed to a fusion center, concatenated, and used to perform the localization based on two proposed methods, which require only few labeled source locations (anchor points) for training. The first proposed method is based on principal component analysis (PCA) of the observed DOA and does not require any knowledge of anchor points. The array cluster can then perform localization on a manifold defined by the PCA of concatenated DOAs over time. The second proposed method performs localization using an affine transformation between the DOA vectors and the room manifold. The PCA has fewer requirements on the training sequence, but is less robust to missing DOAs from one of the arrays. The methods are demonstrated with five IoT 8-microphone circular arrays, placed at unspecified fixed locations in an office. Both the PCA and the affine method can easily map out a rectangle based on a few anchor points with similar accuracy. The proposed methods provide a step towards monitoring activities in a smart home and require little installation effort as the array locations are not needed. ",Audio scene monitoring using redundant ad-hoc microphone array networks,1,"['Here\'s the preprint of our new paper ""Audio scene monitoring using redundant un-localized microphone arrays"". I had the privilege to collaborate with researchers at University of California San Diego, including Pr. Peter Gerstoft and Pr. Yoav Freund.\n\n<LINK>']",21,03,257
34,7,756214204122603520,23165990,Seán Bartz 🏁🏎,"It feels hot enough outside to restore chiral symmetry (which my new paper predicts is 151 MeV ~ a trillion degrees) <LINK> When you say ""about a trillion degrees,"" it doesn't matter if you use Fahrenheit or Celsius. Switching to Kelvin is a rounding error. Chirality literally refers to ""handedness"" of particles. Do they seem to be spinning clockwise or counterclockwise as they move toward you? Imagine a football thrown by a right-handed player vs thrown by a lefty. They both move forward, but spin opposite ways. Chirality. A video of a left-handed throw would look the same as video of a right-handed throw that had been reflected in a mirror. Chiral symmetry. Some particles made of quarks look the same as their reflection, and some look reversed. If they were like footballs, this wouldn't matter. But quarks are different. How they look under reflection affects how they interact w/ empty space. Thus, the particles have different masses This is chiral symmetry breaking. A symmetry (mirror reflection) is almost true, but not quite. A mirror universe is different from ours. Chiral symmetry breaking accounts for most of the mass of ordinary matter. Quark mass accounts for about 10%, rest from this interaction This has all been at zero temperature. At high temp and density (like when we collide gold ions together), chiral symmetry is restored. My paper looks at how chiral symmetry is restored as temperature and density increase. Share w/ your friends! <LINK> This paper written in collaboration with a @Macalester undergrad, Theo Jacobson! #heymac <LINK> Also, shout out to @TheOnlyMasSquad whose question inspired the chiral football analogy <LINK>",http://arxiv.org/abs/1607.05751,"We investigate the in-medium behavior of mesons at finite temperature and baryon chemical potential within a soft-wall model of AdS/QCD. We use a quartic scalar potential to obtain the correct form of chiral symmetry breaking. At zero quark mass the chiral phase transition is second-order, becoming a crossover at physical quark mass. At zero baryon chemical potential, we find a chiral transition temperature of 155 MeV in the chiral limit and a pseudo-transition temperature of 151 MeV at physical quark mass, consistent with lattice results. In the low-temperature limit, the second-order transition occurs at a baryon chemical potential of 566 MeV while the rapid crossover occurs at 559 MeV. A new parameterization of the dilaton profile results in improved meson spectra. Meson melting occurs at a lower temperature and chemical potential than the chiral phase transition, so the vector-axial vector mass splitting remains constant until the bound states melt. ",Chiral Phase Transition and Meson Melting from AdS/QCD,13,"['It feels hot enough outside to restore chiral symmetry (which my new paper predicts is 151 MeV ~ a trillion degrees) <LINK>', 'When you say ""about a trillion degrees,"" it doesn\'t matter if you use Fahrenheit or Celsius. Switching to Kelvin is a rounding error.', 'Chirality literally refers to ""handedness"" of particles. Do they seem to be spinning clockwise or counterclockwise as they move toward you?', 'Imagine a football thrown by a right-handed player vs thrown by a lefty. They both move forward, but spin opposite ways. Chirality.', 'A video of a left-handed throw would look the same as video of a right-handed throw that had been reflected in a mirror. Chiral symmetry.', ""Some particles made of quarks look the same as their reflection, and some look reversed. If they were like footballs, this wouldn't matter."", 'But quarks are different. How they look under reflection affects how they interact w/ empty space. Thus, the particles have different masses', 'This is chiral symmetry breaking. A symmetry (mirror reflection) is almost true, but not quite. A mirror universe is different from ours.', 'Chiral symmetry breaking accounts for most of the mass of ordinary matter. Quark mass accounts for about 10%, rest from this interaction', 'This has all been at zero temperature. At high temp and density (like when we collide gold ions together), chiral symmetry is restored.', 'My paper looks at how chiral symmetry is restored as temperature and density increase. Share w/ your friends! https://t.co/M1JkpJ5Rnv', 'This paper written in collaboration with a @Macalester undergrad, Theo Jacobson! #heymac https://t.co/M1JkpJ5Rnv', 'Also, shout out to @TheOnlyMasSquad whose question inspired the chiral football analogy https://t.co/IcRcb8vfSw']",16,07,1667
35,26,1509412029756547073,592862195,Jarvist Moore Frost,"<LINK> New paper! @Neutrino155 's main project so far on extending the Feynman variational polaron method to multiple phonon modes. Codes are open source, but on a development branch for the moment. We can now simulate freq dependent mobility / opt absorption!",http://arxiv.org/abs/2203.16472,"The Feynman path-integral variational solution to the polaron problem \cite{Feynman1955}, along with the associated FHIP linear-response mobility theory \cite{Feynman1962}, provides a computationally amenable method to predict the frequency-resolved temperature-dependent charge-carrier mobility, and other experimental observables in polar semi-conductors. We show that the FHIP mobility theory is capable of demonstrating non-Drude transport behaviour, and provides good agreement with the recent diagrammatic Monte-Carlo mobility simulations of Mishchenko et al. \cite{Mishchenko2019} for the abstract Fr\""ohlich Hamiltonian. We extend this method to multiple variational parameters in the model action, and to multiple phonon modes in the true action. This enables a slightly better variational solution, as inferred from the resulting energy. We carry forward this complexity into the mobility theory, where it enables richer structure in the frequency and temperature dependent mobility, due to the different phonon modes activating at different energies. ","Multiple phonon modes in Feynman path-integral variational polaron
  mobility",1,"[""<LINK>\nNew paper! @Neutrino155 's main project so far on extending the Feynman variational polaron method to multiple phonon modes.\n\nCodes are open source, but on a development branch for the moment.\n\nWe can now simulate freq dependent mobility / opt absorption!""]",22,03,260
36,131,1446276490514026498,973404788,Bei Zhou,"New paper! <LINK> with @ProfJohnBeacom. We study neutrino-induced dimuons, a phenomenon that has only been seen in accelerator neutrino experiments, as a new event class of neutrino telescopes like IceCube, IceCube-Gen2. @uw_icecube 1/6 <LINK> @ProfJohnBeacom The dimuons are mainly from neutrino-nucleus deep-inelastic scatter and W-boson production. We develop a calculational framework and show that for 10 years, IceCube can detect ≃400 dimuons and IceCube-Gen2 can detect ≃1200!  2/6 <LINK> @ProfJohnBeacom These dimuons have very important physics potentials, including probing high-energy QCD, enabling the first detection of W-boson production (a process that will be very important for high energy neutrinos but has never been identified), and new physics.  3/6 <LINK> @ProfJohnBeacom More excitingly, we find 19 dimuon candidates from analyzing IceCube public data! We are not totally sure if they are dimuon signals yet due to the reasons detailed in the paper, but all aspects of them match our prediction.  4/6 <LINK> @ProfJohnBeacom @uw_icecube Whether they are real dimuons or some new background (or signal!), it’s important to understand them by IceCube collaboration.  5/6 @ProfJohnBeacom @uw_icecube The continued success of neutrino physics & astrophysics depends on developing new tools to get the most out of the data. Developing new event classes is an important part. Our theory and observation contributions help open a valuable new direction for neutrino telescopes.  6/6",https://arxiv.org/abs/2110.02974,"Neutrino telescopes allow powerful probes of high-energy astrophysics and particle physics. Their power is increased when they can isolate different event classes, e.g., by flavor, though that is not the only possibility. Here we focus on a new event class for neutrino telescopes: dimuons, two energetic muons from one neutrino interaction. We make new theoretical and observational contributions. For the theoretical part, we calculate dimuon production cross sections and detection prospects via deep-inelastic scattering (DIS; where we greatly improve upon prior work) and $W$-boson production (WBP; where we present first results). We show that IceCube should have $\simeq 400$ dimuons ($\simeq 8$ from WBP) in its current data and that IceCube-Gen2, with a higher threshold but a larger exposure, could detect $\simeq 1200$ dimuons ($\simeq 30$ from WBP) in 10 years. These dimuons are almost all produced by atmospheric neutrinos. For the observational part, we perform a simple but conservative analysis of IceCube public data, finding 19 candidate dimuon events. Subsequent to our paper appearing, visual inspection of these events by the IceCube Collaboration reveals that they are not real dimuons, but instead arise from an internal reconstruction error that identifies some single muons crossing the dust layer as two separate muons. To help IceCube and the broader community with future dimuon searches, we include the updated full details of our analysis. Together, these theoretical and observational contributions help open a valuable new direction for neutrino telescopes, one especially important for probing high-energy QCD and new physics. ","Dimuons in Neutrino Telescopes: New Predictions and First Search in
  IceCube",6,"['New paper! <LINK> with @ProfJohnBeacom. We study neutrino-induced dimuons, a phenomenon that has only been seen in accelerator neutrino experiments, as a new event class of neutrino telescopes like IceCube, IceCube-Gen2.  @uw_icecube  1/6 <LINK>', '@ProfJohnBeacom The dimuons are mainly from neutrino-nucleus deep-inelastic scatter and W-boson production. We develop a calculational framework and show that for 10 years, IceCube can detect ≃400 dimuons and IceCube-Gen2 can detect ≃1200!   2/6 https://t.co/gYuPDTRGqC', '@ProfJohnBeacom These dimuons have very important physics potentials, including probing high-energy QCD, enabling the first detection of W-boson production (a process that will be very important for high energy neutrinos but has never been identified), and new physics.   3/6 https://t.co/ybjf3usXlZ', '@ProfJohnBeacom More excitingly, we find 19 dimuon candidates from analyzing IceCube public data! We are not totally sure if they are dimuon signals yet due to the reasons detailed in the paper, but all aspects of them match our prediction.    4/6 https://t.co/XdZbGbl2Hz', '@ProfJohnBeacom @uw_icecube Whether they are real dimuons or some new background (or signal!), it’s important to understand them by IceCube collaboration.    5/6', '@ProfJohnBeacom @uw_icecube The continued success of neutrino physics &amp; astrophysics depends on developing new tools to get the most out of the data. Developing new event classes is an important part. Our theory and observation contributions help open a valuable new direction for neutrino telescopes.    6/6']",21,10,1497
37,188,1292098113902202880,1137068512286007297,Monica Agrawal,"Clinical notes are hard to understand: for people and computers. In our paper (<LINK>) presented at #MLHC2020, we dive into the current performance of clinical entity extraction algorithms (hint: they could be better) and propose a path forward for clinical NLP. <LINK> Joint work with @david_sontag and several wonderful Twitter-less coauthors at @MIT_CSAIL and @MGHMedicine. Learn more here (<LINK>), and reach out if you're interested in helping create an open-sourced dataset!",http://arxiv.org/abs/2007.16127,"Clinical studies often require understanding elements of a patient's narrative that exist only in free text clinical notes. To transform notes into structured data for downstream use, these elements are commonly extracted and normalized to medical vocabularies. In this work, we audit the performance of and indicate areas of improvement for state-of-the-art systems. We find that high task accuracies for clinical entity normalization systems on the 2019 n2c2 Shared Task are misleading, and underlying performance is still brittle. Normalization accuracy is high for common concepts (95.3%), but much lower for concepts unseen in training data (69.3%). We demonstrate that current approaches are hindered in part by inconsistencies in medical vocabularies, limitations of existing labeling schemas, and narrow evaluation techniques. We reformulate the annotation framework for clinical entity extraction to factor in these issues to allow for robust end-to-end system benchmarking. We evaluate concordance of annotations from our new framework between two annotators and achieve a Jaccard similarity of 0.73 for entity recognition and an agreement of 0.83 for entity normalization. We propose a path forward to address the demonstrated need for the creation of a reference standard to spur method development in entity recognition and normalization. ",Robust Benchmarking for Machine Learning of Clinical Entity Extraction,2,"['Clinical notes are hard to understand: for people and computers. In our paper (<LINK>) presented at #MLHC2020,  we dive into the current performance of clinical entity extraction algorithms (hint: they could be better) and propose a path forward for clinical NLP. <LINK>', ""Joint work with @david_sontag and several wonderful Twitter-less coauthors at @MIT_CSAIL and @MGHMedicine. Learn more here (https://t.co/hhHiZQDCMP), and reach out if you're interested in helping create an open-sourced dataset!""]",20,07,480
38,93,1480746665690632194,1196266674954985472,Nirmal Raj,"1/n New paper with @HostertMatheus, @davemckeen, and Maxim Pospelov. <LINK> ""Dark sectors in neutron-shining-through-a-wall and nuclear absorption signals"" Thread follows. <LINK> 2/n Particle species w/ the same quantum numbers can ""mix"", e.g. you can emit a photon and measure it elsewhere as a Z boson. We explored how to discover feeble quantum mixings b/w the neutron and a hypothetical ""dark neutron"", a species that could resolve many puzzles in Nature. 3/n Quantum mixing lets you ""shine neutrons through a wall"". Throw a neutron at a wall, and it cd detect it as a dark neutron and let it thru, which cd then regenerate as a neutron on the other side. See attached cartoon where a batter faces a neutron bowler through a wall. <LINK> 4/n We show that this process can be exploited at IsoDAR, an imminent experiment consisting of a very intense proton beam paired to a very large detector, to be situated deep underground at (most likely) Yemilab, South Korea. 5/n While the original design of IsoDAR is to do important physics with neutrinos, the shielding of the beam target cd be a ""wall"" thru which beam-produced neutrons could shine & then show up in the detector. Thus, for free, IsoDAR will be a world-leading hunter of dark neutrons. <LINK> 6/n Next, dark neutrons could make up the #darkmatter in our galaxy (and the universe beyond). They could then slip through the Earth's layers, touch underground detectors and convert to neutrons -- promptly eaten up by detector nuclei with flashes of light spilling over. <LINK> 7/n (And that can be used to constrain even more feeble mixings between the neutron and dark neutron.) 8/n Finally, we explored a few other promising ways to find dark neutrons -- how to reinterpret searches for ultracold neutrons disappearing from their traps, how to catch dark neutrons at spallation sources, and so on.",https://arxiv.org/abs/2201.02603,"We propose new searches for $n^\prime$, a dark baryon that can mix with the Standard Model neutron. We show that IsoDAR, a proposal to place an intense cyclotron near a large-volume neutrino detector deep underground, can look for $n\to n^\prime \to n$ transitions with much lower backgrounds than surface experiments. This neutron-shining-through-a-wall search would be possible without any modifications to the experiment and would provide the strongest laboratory constraints on the $n$-$n^\prime$ mixing for a wide range of mass splittings. We also consider dark neutrons as dark matter and show that their nuclear absorption at deep-underground detectors such as SNO and Borexino places some of the strongest limits in parameter space. Finally, we describe other $n^\prime$ signatures, such as neutrons shining through walls at spallation sources, reactors, and the disappearance of ultracold neutrons. ","Dark sectors in neutron-shining-through-a-wall and nuclear absorption
  signals",8,"['1/n New paper with @HostertMatheus, @davemckeen, and Maxim Pospelov.\n<LINK>\n""Dark sectors in neutron-shining-through-a-wall and nuclear absorption signals""\nThread follows. <LINK>', '2/n Particle species w/ the same quantum numbers can ""mix"", e.g. you can emit a photon and measure it elsewhere as a Z boson. We explored how to discover feeble quantum mixings b/w the neutron and a hypothetical ""dark neutron"", a species that could resolve many puzzles in Nature.', '3/n Quantum mixing lets you ""shine neutrons through a wall"".  Throw a neutron at a wall, and it cd detect it as a dark neutron and let it thru, which cd then regenerate as a neutron on the other side. See attached cartoon where a batter faces a neutron bowler through a wall. https://t.co/jXO1aESFlh', '4/n We show that this process can be exploited at IsoDAR, an imminent experiment consisting of a very intense proton beam paired to a very large detector, to be situated deep underground at (most likely) Yemilab, South Korea.', '5/n While the original design of IsoDAR is to do important physics with neutrinos, the shielding of the beam target cd be a ""wall"" thru which beam-produced neutrons could shine &amp; then show up in the detector. Thus, for free, IsoDAR will be a world-leading hunter of dark neutrons. https://t.co/7IHiXtG3RQ', ""6/n Next, dark neutrons could make up the #darkmatter in our galaxy (and the universe beyond). They could then slip through the Earth's layers, touch underground detectors and convert to neutrons -- promptly eaten up by detector nuclei with flashes of light spilling over. https://t.co/Lqh66Ceodq"", '7/n (And that can be used to constrain even more feeble mixings between the neutron and dark neutron.)', '8/n Finally, we explored a few other promising ways to find dark neutrons -- how to reinterpret searches for ultracold neutrons disappearing from their traps, how to catch dark neutrons at spallation sources, and so on.']",22,01,1858
39,20,1100619058649550848,1030693296,Nicolas Martin,"New paper: we (Ibata, Bellazzini, @kmalhan07, Bianchini and myself) show that the Fimbulthul stream is produced by the Omega Centauri stream. <LINK> 1/3 <LINK> Omega Cen is the most massive Milky Way globular cluster and looks quite odd. It's possibly the nuclei of an accreted dwarf galaxy. We don't shed light on this but we show where stripped Omega Cen stars are and were to look for the leftovers of its progenitor if it exists. 2/3 This stream was found as part of our effort searching for nearby stellar streams in #GaiaDR2. That search, led by @khayati and Rodrigo Ibata has been quite successful so far (<LINK>)! <LINK>",https://arxiv.org/abs/1902.09544,"Omega Centauri is the most massive globular cluster of the Milky Way, and possesses many peculiar properties. In particular, the cluster contains distinct multiple stellar populations, with a large spread in metallicity and different kinematics as a function of light elements abundance, implying that it formed over an extended period of time. This has lead to the suggestion that $\omega$ Cen is the remnant core of an accreted dwarf galaxy. If this scenario is correct, $\omega$ Cen should be tidally limited, and one should expect to find tidal debris spread along its orbit. Here we use N-body simulations to show that the recently-discovered `Fimbulthul' structure, identified in the second data release (DR2) of the Gaia mission, is the long sought-for tidal stream of $\omega$ Cen, extending up to $28\deg$ from the cluster. Follow-up high-resolution spectroscopy of 5 stars in the stream show that they are closely-grouped in velocity, and have metallicities consistent with having originated in that cluster. Guided by our N-body simulations, we devise a selection filter that we apply to Gaia data to also uncover the portion of the stream in the highly-contaminated and crowded field within $10\deg$ of $\omega$ Cen. Further modelling of the stream may help to constrain the dynamical history of the dwarf galaxy progenitor of this disrupting system and guide future searches for its remnant stars in the Milky Way. ","Identification of the Long Stellar Stream of the Prototypical Massive
  Globular Cluster $\omega$ Centauri",3,"['New paper: we (Ibata, Bellazzini, @kmalhan07, Bianchini and myself) show that the Fimbulthul stream is produced by the Omega Centauri stream. <LINK> 1/3 <LINK>', ""Omega Cen is the most massive Milky Way globular cluster and looks quite odd. It's possibly the nuclei of an accreted dwarf galaxy. We don't shed light on this but we show where stripped Omega Cen stars are and were to look for the leftovers of its progenitor if it exists. 2/3"", 'This stream was found as part of our effort searching for nearby stellar streams in #GaiaDR2. That search, led by @khayati and Rodrigo Ibata has been quite successful so far (https://t.co/jLLrImcNGD)! https://t.co/JvBYE7i6rv']",19,02,628
40,47,974077485892583424,3242991169,Bharath Ramsundar,"Check out our new paper on ""Spatial Graph Convolutions for Drug Discovery."" Converts a 3D macro molecular structure into a graph structure that it feeds into a graph convolutional deep network. Matches state-of-art with end-to-end learning. <LINK> The work is written up in a great Medium post by lead author @enfeinberg and corresponding author @vijaypande <LINK>",https://arxiv.org/abs/1803.04465,"The arc of drug discovery entails a multiparameter optimization problem spanning vast length scales. They key parameters range from solubility (angstroms) to protein-ligand binding (nanometers) to in vivo toxicity (meters). Through feature learning---instead of feature engineering---deep neural networks promise to outperform both traditional physics-based and knowledge-based machine learning models for predicting molecular properties pertinent to drug discovery. To this end, we present the PotentialNet family of graph convolutions. These models are specifically designed for and achieve state-of-the-art performance for protein-ligand binding affinity. We further validate these deep neural networks by setting new standards of performance in several ligand-based tasks. In parallel, we introduce a new metric, the Regression Enrichment Factor $EF_\chi^{(R)}$, to measure the early enrichment of computational models for chemical data. Finally, we introduce a cross-validation strategy based on structural homology clustering that can more accurately measure model generalizability, which crucially distinguishes the aims of machine learning for drug discovery from standard machine learning tasks. ",PotentialNet for Molecular Property Prediction,2,"['Check out our new paper on ""Spatial Graph Convolutions for Drug Discovery."" Converts a 3D macro molecular structure into a graph structure that it feeds into a graph convolutional deep network. Matches state-of-art with end-to-end learning. <LINK>', 'The work is written up in a great Medium post by lead author @enfeinberg and corresponding author @vijaypande https://t.co/UwegCC8ni7']",18,03,364
41,261,1316215116753301505,892993855670431744,Xikun Zhang,"Do Language Embeddings Capture Scales? e.g. Can embeddings learned from pretrained language models predict the price of a ring or the weight of an elephant? New paper available at <LINK> which was done during my internship at @GoogleAI! To be published ... <LINK> at Findings of #emnlp2020 and to be presented at #BlackboxNLP. Done together with my wonderful collaborators Deepak Ramachandran, @iftenney, @yanaiela and @dannydanr! @yanaiela has a nice brief summary about the paper above! 😄",https://arxiv.org/abs/2010.05345,"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense, and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance and show that a simple method of canonicalizing numbers can have a significant effect on the results. ",Do Language Embeddings Capture Scales?,2,"['Do Language Embeddings Capture Scales?\n\ne.g. Can embeddings learned from pretrained language models predict the price of a ring or the weight of an elephant?\n\nNew paper available at <LINK> which was done during my internship at @GoogleAI! To be published ... <LINK>', 'at Findings of #emnlp2020 and to be presented at #BlackboxNLP. Done together with my wonderful collaborators Deepak Ramachandran, @iftenney, @yanaiela  and @dannydanr! @yanaiela has a nice brief summary about the paper above! 😄']",20,10,490
42,130,1436237195074015233,16389141,Massimo Nicosia,📄 Our new EMNLP paper is on arXiv! 📄 1⃣ Train an mT5 filler model to reconstruct full parses from English utterances + parse signatures 2⃣ Run it on translations and parse signatures to obtain high quality i18n synthetic data! More here:👉 <LINK> 👈  @Google,https://arxiv.org/abs/2109.04319,"While multilingual pretrained language models (LMs) fine-tuned on a single language have shown substantial cross-lingual task transfer capabilities, there is still a wide performance gap in semantic parsing tasks when target language supervision is available. In this paper, we propose a novel Translate-and-Fill (TaF) method to produce silver training data for a multilingual semantic parser. This method simplifies the popular Translate-Align-Project (TAP) pipeline and consists of a sequence-to-sequence filler model that constructs a full parse conditioned on an utterance and a view of the same parse. Our filler is trained on English data only but can accurately complete instances in other languages (i.e., translations of the English training utterances), in a zero-shot fashion. Experimental results on three multilingual semantic parsing datasets show that data augmentation with TaF reaches accuracies competitive with similar systems which rely on traditional alignment techniques. ","Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with
  Synthetic Data",1,['📄 Our new EMNLP paper is on arXiv! 📄\n\n1⃣ Train an mT5 filler model to reconstruct full parses from English utterances + parse signatures\n2⃣ Run it on translations and parse signatures to obtain high quality i18n synthetic data!\n\nMore here:👉 <LINK> 👈\n\n @Google'],21,09,256
43,179,1494676841126416393,1327363823817412608,Maura Lally,"Browsing arXiv? Check out my first first-author paper, with @amvanderburg: we reassess the claimed detection of atmospheric variability on exoplanet HAT-P-7b, & find that the apparent variations could instead be caused by supergranulation on the host star. <LINK>",https://arxiv.org/abs/2202.08279,"We reassess the claimed detection of variability in the atmosphere of the hot Jupiter HAT-P-7 b, reported by Armstrong et al. (2016). Although astronomers expect hot Jupiters to have changing atmospheres, variability is challenging to detect. We looked for time variation in the phase curves of HAT-P-7 b in Kepler data using similar methods to Armstrong et al. (2016), and identified apparently significant variations similar to what they found. Numerous tests show the variations to be mostly robust to different analysis strategies. However, when we injected unchanging phase curve signals into the light curves of other stars and searched for variability, we often saw similar levels of variations as in the HAT-P-7 light curve. Fourier analysis of the HAT-P-7 light curve revealed background red noise from stellar supergranulation on timescales similar to the planet's orbital period. Tests of simulated light curves with the same level of noise as HAT-P-7's supergranulation show that this effect alone can cause the amplitude and phase offset variability we detect for HAT-P-7 b. Therefore, the apparent variations in HAT-P-7 b's atmosphere could instead be caused by non-planetary sources, most likely photometric variability due to supergranulation on the host star. ","Reassessing the Evidence for Time Variability in the Atmosphere of the
  Exoplanet HAT-P-7 b",1,"['Browsing arXiv? Check out my first first-author paper, with @amvanderburg: we reassess the claimed detection of atmospheric variability on exoplanet HAT-P-7b, &amp; find that the apparent variations could instead be caused by supergranulation on the host star. <LINK>']",22,02,263
44,198,1392714026455535617,422672164,Dr Michael Reidinger,"Neutrino constraints to scotogenic dark matter interacting in the Sun Here we present a study of neutrino signals from the annihilation of dark matter particles which have been gravitationally captured in the Sun, in the framework of the scotogenic model. <LINK>",https://arxiv.org/abs/2105.05613,"Radiative seesaw models have the attractive property of providing dark matter candidates in addition to generation of neutrino masses. Here we present a study of neutrino signals from the annihilation of dark matter particles which have been gravitationally captured in the Sun, in the framework of the scotogenic model. We compute expected event rates in the IceCube detector in its 86-string configuration. As fermionic dark matter does not accumulate in the Sun, we study the case of scalar dark matter, with a scan over the parameter space. Due to a naturally small mass splitting between the two neutral scalar components, inelastic scattering processes with nucleons can occur. We find that for small mass splittings, the model yields very high event rates. If a detailed analysis at IceCube can exclude these parameter points, our findings can be translated into a lower limit on one of the scalar couplings in the model. For larger mass splittings only the elastic case needs to be considered. We find that in this scenario the XENON1T limits exclude all points with sufficiently large event rates. ",Neutrino constraints to scotogenic dark matter interacting in the Sun,1,"['Neutrino constraints to scotogenic dark matter interacting in the Sun\n\nHere we present a study of neutrino signals from the annihilation of dark matter particles which have been gravitationally captured in the Sun, in the framework of the scotogenic model.\n<LINK>']",21,05,262
45,130,1501482717761916929,521154902,Massimiliano Luca,New preprint out🎉  Can next-location predictors generalize? Not really 👉Many test trajectories are seen during training 👉Mobility laws can be used to support predictors and improve generalization  paper: <LINK> wt @brulepri @GianniBarlacchi @lucpappalard <LINK>,https://arxiv.org/abs/2203.03208,"Next-location prediction, consisting of forecasting a user's location given their historical trajectories, has important implications in several fields, such as urban planning, geo-marketing, and disease spreading. Several predictors have been proposed in the last few years to address it, including last-generation ones based on deep learning. This paper tests the generalization capability of these predictors on public mobility datasets, stratifying the datasets by whether the trajectories in the test set also appear fully or partially in the training set. We consistently discover a severe problem of trajectory overlapping in all analyzed datasets, highlighting that predictors memorize trajectories while having limited generalization capacities. We thus propose a methodology to rerank the outputs of the next-location predictors based on spatial mobility patterns. With these techniques, we significantly improve the predictors' generalization capability, with a relative improvement on the accuracy up to 96.15% on the trajectories that cannot be memorized (i.e., low overlap with the training set). ",Trajectory Test-Train Overlap in Next-Location Prediction Datasets,1,['New preprint out🎉 \n\nCan next-location predictors generalize? Not really\n\n👉Many test trajectories are  seen during training\n\n👉Mobility laws can be used to support predictors and improve generalization \n\npaper: <LINK>\nwt @brulepri @GianniBarlacchi @lucpappalard <LINK>'],22,03,261
46,109,1503392824494071815,795318493675712512,Maura Pintor,"📌New preprint available! ""ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches."" paper: <LINK> code: <LINK> w/ @DAngioni97 @biggiobattista @zangobot @ambrademontis @sotgiu_angelo <LINK> #mlsec #machinelearning #adversarial #ml #advml #cybersecurity #benchmark @adversarial_ML @mlsec_lab",https://arxiv.org/abs/2203.04412,"Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations. We showcase the usefulness of this dataset by testing the effectiveness of the computed patches against 127 models. We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at this https URL ","ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness
  against Adversarial Patches",2,"['📌New preprint available!\n\n""ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches.""\n\npaper: <LINK>\ncode: <LINK>\n\nw/ @DAngioni97  @biggiobattista @zangobot @ambrademontis @sotgiu_angelo <LINK>', '#mlsec #machinelearning #adversarial #ml #advml #cybersecurity #benchmark\n@adversarial_ML @mlsec_lab']",22,03,332
47,150,1425355170901266434,578977577,Javier Argüello Luengo,"Now online, the latest work with Darrick! We show that a photon scattered by a single-atom in a cavity can highly entangle with the subsequent atomic motion We observe a much larger heating than expected in free space and propose experimental signatures👇 <LINK> Focusing on current platforms, we derive an optimal strong-coupling configuration where the probability that a photon entering the cavity gets reflected highly depends on the position of the atom inside the cavity. <LINK> Therefore, if we see that a photon gets reflected, we gain information about where the atom is (or is not) in that moment.  The “conditioned” atomic wavefunction changes accordingly: a “hole” appears in the positions where it was unlikely to have been (shaded in blue). <LINK> A similar change also occurs when the photon is not reflected. As a consequence, even if we were not actively monitoring where the photon ends up, a trapped atom inside the cavity heats up as its wavefunction entangles with the scattered photons. <LINK> Interestingly, we see that this heating mechanism can be much larger than the one caused by a single photon in free-space.  In particular, heating gets enhanced by the number of photon round-trips inside the cavity (the cooperativity). <LINK> But what if we were actually monitoring the cavity and detected a reflected photon?  For weak driving, the resulting “holed” atomic wavefunction would start oscillating in the trap before a next photon arrives. <LINK> As the atom explores different regions of the cavity, the probability that a second photon reflects will change accordingly: <LINK> This can then be captured by oscillations of second-order correlations in time g(t), showing a periodicity dictated by the trap. When the atom populates positions compatible with reflection (white) one measures g(t)&gt;1 (bunching), in the opposite case (shaded region), g(t)&lt;1. <LINK> For this analysis we have developed a minimal scattering matrix formalism that capture the leading features of the problem, derived an optimal configuration and showed that these effects can be observed in realistic systems, even for a non-zero initial motional temperature. <LINK> As always, feel very welcome to send us any comment or feedback, and special thanks to our colleagues and funding agencies @ERC_Research @CienciaGob @CaixaResearch @ICFOnians",https://arxiv.org/abs/2108.03526,"Single atoms coupled to a cavity offer unique opportunities as quantum optomechanical devices because of their small mass and strong interaction with light. A particular regime of interest in optomechanics is that of ""single-photon strong coupling,"" where motional displacements on the order of the zero-point uncertainty are sufficient to shift the cavity resonance frequency by more than its linewidth. In many cavity QED platforms, however, this is unfeasible due to the large cavity linewidth. Here, we propose an alternative route in such systems, which instead relies on the coupling of atomic motion to the much narrower cavity-dressed atomic resonance frequency. We discuss and optimize the conditions in which the scattering properties of single photons from the atom-cavity system become highly entangled with the atomic motional wave function. We also analyze the prominent observable features of this optomechanical strong coupling, which include a per-photon motional heating that is significantly larger than the single-photon recoil energy, as well as mechanically-induced oscillations in time of the second-order correlation function of the emitted light. This physics should be realizable in current experimental setups, such as trapped atoms coupled to photonic crystal cavities, and more broadly opens the door to realizing qualitatively different phenomena beyond what has been observed in optomechanical systems thus far. ","Optomechanical strong coupling between a single cavity photon and a
  single atom",10,"['Now online, the latest work with Darrick!\n\nWe show that a photon scattered by a single-atom in a cavity can highly entangle with the subsequent atomic motion We observe a much larger heating than expected in free space and propose experimental signatures👇\n<LINK>', 'Focusing on current platforms, we derive an optimal strong-coupling configuration where the probability that a photon entering the cavity gets reflected highly depends on the position of the atom inside the cavity. https://t.co/5QCEZgOTAJ', 'Therefore, if we see that a photon gets reflected, we gain information about where the atom is (or is not) in that moment. \n\nThe “conditioned” atomic wavefunction changes accordingly: a “hole” appears in the positions where it was unlikely to have been (shaded in blue). https://t.co/0uYOKjLWm2', 'A similar change also occurs when the photon is not reflected. As a consequence, even if we were not actively monitoring where the photon ends up, a trapped atom inside the cavity heats up as its wavefunction entangles with the scattered photons. https://t.co/5a8YXqW2a1', 'Interestingly, we see that this heating mechanism can be much larger than the one caused by a single photon in free-space. \n\nIn particular, heating gets enhanced by the number of photon round-trips inside the cavity (the cooperativity). https://t.co/STLf0oud1Q', 'But what if we were actually monitoring the cavity and detected a reflected photon? \n\nFor weak driving, the resulting “holed” atomic wavefunction would start oscillating in the trap before a next photon arrives. https://t.co/ZOST7xWEDU', 'As the atom explores different regions of the cavity, the probability that a second photon reflects will change accordingly: https://t.co/VXkNX1BX47', 'This can then be captured by oscillations of second-order correlations in time g(t), showing a periodicity dictated by the trap.\n\nWhen the atom populates positions compatible with reflection (white) one measures g(t)&gt;1 (bunching), in the opposite case (shaded region), g(t)&lt;1. https://t.co/PmLVCjx0IH', 'For this analysis we have developed a minimal scattering matrix formalism that capture the leading features of the problem, derived an optimal configuration and showed that these effects can be observed in realistic systems, even for a non-zero initial motional temperature. https://t.co/6TaQInmEKR', 'As always, feel very welcome to send us any comment or feedback, and special thanks to our colleagues and funding agencies \n@ERC_Research @CienciaGob @CaixaResearch @ICFOnians']",21,08,2353
48,32,1431276942729392138,3083528599,Ashutosh Baheti,"New paper in EMNLP 2021: <LINK> Chatbots can insult anyone by agreeing with an offensive statement. To understand this contextually offensive behavior we create ToxiChat - A crowd-annotated dataset of 2000 Reddit Conversations. We augment each conversation with responses generated by DialoGPT and GPT-3 models and label each utterance with offensive language and stance. <LINK> Key finding: Chatbots are 2x more likely to agree with statements that are offensive! <LINK> We experiment with controllable text generation (CTG) methods to mitigate the contextual offensive behavior of dialogue models. Although our CTG model obtains a 19% reduction in agreement with offensive statement and 29% fewer offensive responses, there is scope for improvements. <LINK> Our experiments and results illustrate the need for future work in controllable text generation methods to mitigate chatbots' tendency to agree with offensive statements.",https://arxiv.org/abs/2108.11830,"Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer. Our code and corpus are available at this https URL . ","Just Say No: Analyzing the Stance of Neural Dialogue Generation in
  Offensive Contexts",5,"['New paper in EMNLP 2021: <LINK>\nChatbots can insult anyone by agreeing with an offensive statement. To understand this contextually offensive behavior we create ToxiChat - A crowd-annotated dataset of 2000 Reddit Conversations.', 'We augment each conversation with responses generated by DialoGPT and GPT-3 models and label each utterance with offensive language and stance. https://t.co/hpL1TTBlnw', 'Key finding: Chatbots are 2x more likely to agree with statements that are offensive! https://t.co/QfspxkXR2R', 'We experiment with controllable text generation (CTG) methods to mitigate the contextual offensive behavior of dialogue models. Although our CTG model obtains a 19% reduction in agreement with offensive statement and 29% fewer offensive responses, there is scope for improvements. https://t.co/jfN9ee9YfC', ""Our experiments and results illustrate the need for future work in controllable text generation methods to mitigate chatbots' tendency to agree with offensive statements.""]",21,08,930
49,229,1278980933329260545,1720813753,yappie,"Context Graphs for Legal Reasoning and Argumentation. (arXiv:2007.00732v1 [cs.LO]) <LINK> We propose a new, structured, logic-based framework for legal reasoning and argumentation: Instead of using a single, unstructured meaning space, theory graphs organize k…",http://arxiv.org/abs/2007.00732,"We propose a new, structured, logic-based framework for legal reasoning and argumentation: Instead of using a single, unstructured meaning space, theory graphs organize knowledge and inference into collections of modular meaning spaces organized by inheritance and interpretation. Context graphs extend theory graphs by attack relations and interpret theories as knowledge contexts of agents in argumentation. We introduce the context graph paradigm by modeling the well-studied case Popov v. Hayashi, concentrating on the role of analogical reasoning in context graphs. ",Context Graphs for Legal Reasoning and Argumentation,1,"['Context Graphs for Legal Reasoning and Argumentation. (arXiv:2007.00732v1 [cs.LO]) <LINK>\n\nWe propose a new, structured, logic-based framework for legal reasoning and argumentation: Instead of using a single, unstructured meaning space, theory graphs organize k…']",20,07,261
50,183,1471070813637984257,804069495253962752,David Martínez Delgado,"We have posted our new MEGARA @GTCtelescope study of the blue stellar stream of NGC 7241, possibly one of the lowest mass streams detected beyond the Local Group. And we find the stream's progenitor is suffering a star-formation burst! <LINK> (Credit: @ngc1535) <LINK>",https://arxiv.org/abs/2112.07029,"We study the striking case of a blue narrow stream with a possible globular cluster-like progenitor around the Milky Way-size galaxy NGC 7241 and its foreground dwarf companion. We present a follow-up spectroscopic study of this stream based on data taken with the MEGARA instrument at the 10.4-m Gran Telescopio Canarias using the integral field spectroscopy mode. Although our data suggest that this compact object in the stream is actually a foreground Milky Way halo star, we detect emission lines overlapping a less compact, bluer and fainter blob of the stream that is clearly visible in both ultra-violet and optical deep images. From its heliocentric systemic radial velocity derived from the [OIII] 5007A lines (V_syst= 1548.58+/-1.80 km\s^-1) and new UV and optical broad-band photometry, we conclude that this over-density could be the actual core of the stream, with an absolute magnitude of Mg~ -10 and a g-r = 0.08+/- 0.11, consistent with a remnant of a low-mass dwarf satellite undergoing a current episode of star formation. From the width of the stream, we calculate that the progenitor mass is between 6.4 x 10^6 Mo -2.7 x 10^7 Mo, which is typical of a dwarf galaxy. These estimates suggest that this is one of the lowest mass streams detected so far beyond the Local Group. We find that blue stellar streams containing star formation regions are commonly predicted by high-resolution cosmological simulations of galaxies lighter than the Milky Way. This scenario is consistent with the processes explaining the bursty star formation history of some dwarf satellites, which are followed by a gas depletion and a fast quenching once they enter within the virial radius of their host galaxies. Thus, it is likely that the stream's progenitor is suffering a star-formation burst comparable to those that have shaped the star-formation history of several Local Group dwarfs in the last few Gigayears. ","Once in a blue stream: Detection of recent star formation in the NGC
  7241 stellar stream with MEGARA",1,"[""We have posted our new MEGARA @GTCtelescope study of the blue stellar stream of NGC 7241, possibly one of  the lowest mass streams detected beyond the Local Group. And we find the stream's progenitor is suffering a star-formation burst!\n<LINK>  (Credit: @ngc1535) <LINK>""]",21,12,268
51,42,1298018355513659392,536866317,Hernan Garcia,"New paper with Nick Lammers, @yangjoonkim @jiaxi_zhao94! Transcriptional bursts occur on times scales of minutes to hours, but transcription factor binding lasts &lt;5s. We review several molecular models that can reconcile these dissimilar timescales. <LINK> <LINK>",http://arxiv.org/abs/2008.09225,"Eukaryotic transcription generally occurs in bursts of activity lasting minutes to hours; however, state-of-the-art measurements have revealed that many of the molecular processes that underlie bursting, such as transcription factor binding to DNA, unfold on timescales of seconds. This temporal disconnect lies at the heart of a broader challenge in physical biology of predicting transcriptional outcomes and cellular decision-making from the dynamics of underlying molecular processes. Here, we review how new dynamical information about the processes underlying transcriptional control can be combined with theoretical models that predict not only averaged transcriptional dynamics, but also their variability, to formulate testable hypotheses about the molecular mechanisms underlying transcriptional bursting and control. ","A matter of time: Using dynamics and theory to uncover mechanisms of
  transcriptional bursting",1,"['New paper with Nick Lammers, @yangjoonkim @jiaxi_zhao94!\nTranscriptional bursts occur on times scales of minutes to hours, but transcription factor binding lasts &lt;5s. We review several molecular models that can reconcile these dissimilar timescales.\n<LINK> <LINK>']",20,08,266
52,152,1425054995334971394,1345856121660178433,Annabelle Bohrdt,"In my PhD, I spent a lot of time figuring out what a single hole 🕳 in a quantum antiferromagnet ⬇️⬆️ does. Now, as a postdoc, I’ve moved on to 2 holes 🕳 🕳. We introduce a general pairing mechanism and find super high binding energies — check it out: <LINK> <LINK>",https://arxiv.org/abs/2108.04118,"Interacting many-body systems combining confined and extended dimensions, such as ladders and few layer systems are characterized by enhanced quantum fluctuations, which often result in interesting collective properties. Recently two-dimensional bilayer systems, such as twisted bilayer graphene or ultracold atoms, have sparked a lot of interest because they can host rich phase diagrams, including unconventional superconductivity. Here we present a theoretical proposal for realizing high temperature pairing of fermions in a class of bilayer Hubbard models. We introduce a general, highly efficient pairing mechanism for mobile dopants in antiferromagnetic Mott insulators, which leads to binding energies proportional to $t^{1/3}$, where $t$ is the hopping amplitude of the charge carriers. The pairing is caused by the energy that one charge gains when retracing a string of frustrated bonds created by another charge. Concretely, we show that this mechanism leads to the formation of highly mobile, but tightly bound pairs in the case of mixed-dimensional Fermi-Hubbard bilayer systems. This setting is closely related to the Fermi-Hubbard model believed to capture the physics of copper oxides, and can be realized by currently available ultracold atom experiments. ","Strong pairing in mixed dimensional bilayer antiferromagnetic Mott
  insulators",1,"['In my PhD, I spent a lot of time figuring out what a single hole 🕳 in a quantum antiferromagnet ⬇️⬆️ does. Now, as a postdoc, I’ve moved on to 2 holes 🕳 🕳. We introduce a general pairing mechanism and find super high binding energies — check it out: <LINK> <LINK>']",21,08,263
53,127,1431284942605066248,366380609,Evan Rosenman,"New paper from me, @rina_friedberg, and @BaiocchiMike: ""Robust Designs for Prospective Randomized Trials Surveying Sensitive Topics"". <LINK> This work emerged out of our experience analyzing a cluster-randomized trial of an empowerment training program deployed to adolescent girls in Nairobi, Kenya. The treatment was intended to reduce the incidence of gender-based violence. When surveying sensitive topics, reporting biases -- e.g. the possibility of underreporting troubling outcomes -- pose a threat to causal inference. We approach the problem under the potential outcomes framework, assuming a binary outcome. We suppose reporting behavior is fixed given the choice of survey and show the joint distribution of ""reporting classes"" (e.g. underreporter, overreporter, truth-teller) and ""response classes"" (e.g. outcome increases, decreases, stays the same) determines the bias exactly. <LINK> Then, we propose a sensitivity model and an optimization procedure to determine the required sample size for achieving a desired power level, given the worst-case configuration of misreporters. <LINK> This is a challenging area! Insights from social scientists + local stakeholders are crucial to design the best survey instruments. Rigorous practices must be followed to preserve comfort and anonymity. Then, statisticians can design procedures to help address residual biases. We hope folks find these results interesting and welcome any feedback!",https://arxiv.org/abs/2108.08944,"We consider the problem of designing a prospective randomized trial in which the outcome data will be self-reported, and will involve sensitive topics. Our interest is in misreporting behavior, and how respondents' tendency to under- or overreport a binary outcome might affect the power of the experiment. We model the problem by assuming each individual in our study is a member of one ""reporting class"": a truth-teller, underreporter, overreporter, or false-teller. We show that the joint distribution of reporting classes and ""response classes"" (characterizing individuals' response to the treatment) will exactly define the bias and variance of the causal estimate in our experiment. Then, we propose a novel procedure for deriving sample sizes under the worst-case power corresponding to a given level of misreporting. Our problem is motivated by prior experience implementing a randomized controlled trial of a sexual violence prevention program among adolescent girls in Nairobi, Kenya. ","Robust Designs for Prospective Randomized Trials Surveying Sensitive
  Topics",7,"['New paper from me, @rina_friedberg, and @BaiocchiMike: ""Robust Designs for Prospective Randomized Trials Surveying Sensitive Topics"". <LINK>', 'This work emerged out of our experience analyzing a cluster-randomized trial of an empowerment training program deployed to adolescent girls in Nairobi, Kenya. The treatment was intended to reduce the incidence of gender-based violence.', 'When surveying sensitive topics, reporting biases -- e.g. the possibility of underreporting troubling outcomes -- pose a threat to causal inference. We approach the problem under the potential outcomes framework, assuming a binary outcome.', 'We suppose reporting behavior is fixed given the choice of survey and show the joint distribution of ""reporting classes"" (e.g. underreporter, overreporter, truth-teller) and ""response classes"" (e.g. outcome increases, decreases, stays the same) determines the bias exactly. https://t.co/saUrrRg92u', 'Then, we propose a sensitivity model and an optimization procedure to determine the required sample size for achieving a desired power level, given the worst-case configuration of misreporters. https://t.co/mCsRmEWxcK', 'This is a challenging area! Insights from social scientists + local stakeholders are crucial to design the best survey instruments. Rigorous practices must be followed to preserve comfort and anonymity. Then, statisticians can design procedures to help address residual biases.', 'We hope folks find these results interesting and welcome any feedback!']",21,08,1448
54,18,1509694077285146630,863828060600139776,Dr. Deep Anand,"Presenting a paper that's been in the works for about 3 years.  here, we provide some important insights into the #HubbleTension in #Cosmology, using a brand new local Universe measurement technique. <LINK> <LINK> so very happy to finally get to publish a paper with my friends @AstroClayt and @onetweettwomany. also so very happy to be able to say that we think the tension is (close to) dead. game over folks. find a new gig.",https://arxiv.org/abs/2203.16551,"Using sedimentary and eclipse-based measurements of the lunar recession velocity, we derive a new local-Universe measurement of the Hubble constant ($H_0$) from the recession rate of Earth's Moon. Taking into account the effects of tides, we find a value of $H_{0}$ = 63.01 $\pm$ 1.79 km s$^{-1}$ Mpc$^{-1}$, which is in approximate agreement with the Planck space mission's measurement using the cosmic microwave background (CMB) and base $\Lambda$CDM. Our new measurement represents the first ever model-independent, single-step measurement of the Universe's current expansion rate. This is also the first major local Universe measurement of $H_0$ which is below the measurement from Planck. Importantly, it is robust to the systematic errors that may be present in other $H_0$ measurements using other cosmological probes such as type Ia supernovae, baryon acoustic oscillations, or lensed quasars. Our work provides key evidence towards the notion that the existing Hubble tension may indeed be a result of systematic uncertainties in the local distance ladder. ","Worry No More, The Hubble Tension is Relieved: A Truly Direct
  Measurement of the Hubble Constant from Mooniversal Expansion",3,"[""Presenting a paper that's been in the works for about 3 years. \n\nhere, we provide some important insights into the #HubbleTension in #Cosmology, using a brand new local Universe measurement technique.\n\n<LINK> <LINK>"", 'so very happy to finally get to publish a paper with my friends @AstroClayt and @onetweettwomany.', 'also so very happy to be able to say that we think the tension is (close to) dead. game over folks. find a new gig.']",22,03,427
55,153,1347142720326950912,57571700,Taha Yasseri,New Preprint We find a positive relationship between neuroticism & obsessive work passion in enterprising environments. <LINK> One of last papers based on myPersonality data @david_stillwell & @michalkosinski generated & shared securely & ethically for many years <LINK>,https://arxiv.org/abs/2101.01270,"Passionate employees are essential for organisational success as they foster higher performance and exhibit lower turnover or absenteeism. While a large body of research has investigated the consequences of passion, we know only little about its antecedents. Integrating trait interaction theory with trait activation theory, this paper examines how personality traits, i.e. conscientiousness, agreeableness, and neuroticism impact passion at work across different job situations. Passion has been conceptualized as a two-dimensional construct, consisting of harmonious work passion (HWP) and obsessive work passion (OWP). Our study is based on a sample of N = 824 participants from the myPersonality project. We find a positive relationship between neuroticism and OWP in enterprising environments. Further, we find a three-way interaction between conscientiousness, agreeableness, and enterprising environment in predicting OWP. Our findings imply that the impact of personality configurations on different forms of passion is contingent on the job environment. Moreover, in line with self-regulation theory, the results reveal agreeableness as a ""cool influencer"" and neuroticism as a ""hot influencer"" of the relationship between conscientiousness and work passion. We derive practical implications for organisations on how to foster work passion, particularly HWP, in organisations. ","What drives passion? An empirical examination on the impact of
  personality trait interactions and job environments on work passion",1,['New Preprint\nWe find a positive relationship between neuroticism &amp; obsessive work passion in enterprising environments.\n<LINK>\nOne of last papers based on myPersonality data @david_stillwell &amp; @michalkosinski generated &amp; shared securely &amp; ethically for many years <LINK>'],21,01,270
56,83,1148320485886873600,19510090,Julian Togelius,"How can you use AI methods to balance a game? In particular, a complex game such as Hearthstone, with hundreds of cards? Through multiobjective evolutionary algorithms. In our new paper, we show how to rebalance the game while making minimal card changes. <LINK> <LINK> In our experiments, we use an existing game-playing agent, which has different performance for different classes. We then nerf and buff cards so the agent plays all classes equally well. This can be seen as a form of handicapping. The key is to change the cards as little as possible, so the player does not have to relearn the game. That is why we use multiobjective evolution, with one of the objectives being to minimize changes. <LINK> This procedure could, of course, be applied to many other games, as well as to non-game processes. We chose Hearthstone because it is so complex - most games would probably be easier to balance. The paper, which is written by @EuMyself, @rocanaan, @scotchkorean27, Matthew Fontaine, myself, and @amykhoover, will be presented at @cog2019ieee. Looking forward to it! @jacobmbuckman @hardmaru Quite possible. In general, the competition between players, other players, and game configurations is what forms the meta. So if we rebalanced the meta for a particular player or set of players, it makes sense that we should also be able to find a new player to break the meta.",https://arxiv.org/abs/1907.01623,"Balancing an ever growing strategic game of high complexity, such as Hearthstone is a complex task. The target of making strategies diverse and customizable results in a delicate intricate system. Tuning over 2000 cards to generate the desired outcome without disrupting the existing environment becomes a laborious challenge. In this paper, we discuss the impacts that changes to existing cards can have on strategy in Hearthstone. By analyzing the win rate on match-ups across different decks, being played by different strategies, we propose to compare their performance before and after changes are made to improve or worsen different cards. Then, using an evolutionary algorithm, we search for a combination of changes to the card attributes that cause the decks to approach equal, 50% win rates. We then expand our evolutionary algorithm to a multi-objective solution to search for this result, while making the minimum amount of changes, and as a consequence disruption, to the existing cards. Lastly, we propose and evaluate metrics to serve as heuristics with which to decide which cards to target with balance changes. ",Evolving the Hearthstone Meta,6,"['How can you use AI methods to balance a game? In particular, a complex game such as Hearthstone, with hundreds of cards? Through multiobjective evolutionary algorithms. In our new paper, we show how to rebalance the game while making minimal card changes.\n<LINK> <LINK>', 'In our experiments, we use an existing game-playing agent, which has different performance for different classes. We then nerf and buff cards so the agent plays all classes equally well. This can be seen as a form of handicapping.', 'The key is to change the cards as little as possible, so the player does not have to relearn the game. That is why we use multiobjective evolution, with one of the objectives being to minimize changes. https://t.co/vhCLxqhizY', 'This procedure could, of course, be applied to many other games, as well as to non-game processes. We chose Hearthstone because it is so complex - most games would probably be easier to balance.', 'The paper, which is written by @EuMyself, @rocanaan, @scotchkorean27, Matthew Fontaine, myself, and @amykhoover, will be presented at @cog2019ieee. Looking forward to it!', '@jacobmbuckman @hardmaru Quite possible. In general, the competition between players, other players, and game configurations is what forms the meta. So if we rebalanced the meta for a particular player or set of players, it makes sense that we should also be able to find a new player to break the meta.']",19,07,1379
57,36,1376727888079257603,2956121356,Russ Salakhutdinov,"New #ICLR2021 paper on Self-supervised Learning with Relative Predictive Coding: A new contrastive learning objective that maintains a good balance between training stability, minibatch size sensitivity, & downstream task performance. <LINK> w/ H. Tsai et al. <LINK>",https://arxiv.org/abs/2103.11275,"This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance. ",Self-supervised Representation Learning with Relative Predictive Coding,1,"['New #ICLR2021 paper on Self-supervised Learning with Relative Predictive Coding: A new contrastive learning objective that maintains a good balance between training stability, minibatch size sensitivity, &amp; downstream task performance.\n\n<LINK>\n\nw/ H. Tsai et al. <LINK>']",21,03,266
58,171,1364292898351681540,3045030351,Wesley Cota,Our new preprint is out! We propose a theoretical framework that allows accommodating the heterogeneity of human contacts and the complexity of mobility patterns in infectious disease dynamics. <LINK> w/ @sorianopanos @silviojrufv @_AlexArenas @gomezgardenes <LINK> Desenvolvemos um ferramental teórico que permite incorporar a heterogeneidade de contatos humanos e a complexidade dos padrões de mobilidade em dinâmicas de propagação de epidemias. O preprint é fruto do Programa de Doutorado Sanduíche no Exterior da @CAPES_Oficial de 2019/2020. <LINK>,https://arxiv.org/abs/2102.10614,"Human mobility, contact patterns, and their interplay are key aspects of our social behavior that shape the spread of infectious diseases across different regions. In the light of new evidence and data sets about these two elements, epidemic models should be refined to incorporate both the heterogeneity of human contacts and the complexity of mobility patterns. Here, we propose a theoretical framework that allows accommodating these two aspects in the form of a set of Markovian equations. We validate these equations with extensive mechanistic simulations and derive analytically the epidemic threshold. The expression of this critical value allows us to evaluate its dependence on the specific demographic distribution, the structure of mobility flows, and the heterogeneity of contact patterns, thus shedding light on the microscopic mechanisms responsible for the epidemic detriment driven by recurrent mobility patterns reported in the literature. ","Infectious disease dynamics in metapopulations with heterogeneous
  transmission and recurrent mobility",2,"['Our new preprint is out! We propose a theoretical framework that allows accommodating the heterogeneity of human contacts and the complexity of mobility patterns in infectious disease dynamics.\n\n<LINK>\n\nw/ @sorianopanos @silviojrufv @_AlexArenas @gomezgardenes <LINK>', 'Desenvolvemos um ferramental teórico que permite incorporar a heterogeneidade de contatos humanos e a complexidade dos padrões de mobilidade em dinâmicas de propagação de epidemias.\n\nO preprint é fruto do Programa de Doutorado Sanduíche no Exterior da @CAPES_Oficial de 2019/2020. https://t.co/GElqCzk8Ni']",21,02,552
59,35,1385396035195858944,3377160202,Djuna Croon,"New paper!  Non-perturbative methods for false vacuum decay <LINK> with the amazing Eleanor Hall (@quarkygirl) and Hitoshi Muruyama (@sleptogenesis)  We propose a new (non-perturbative!) technique to calculate false vacuum decay rates. Important, because... ...accurate false vacuum decay calculations are needed to predict the resulting gravitational wave spectra.  I'll try to give a brief explanation below, but I also highly recommend Nell's excellent slides on the topic: <LINK> at #DarkSectorRainbow last month... ...So, accurately calculating the gravitational wave spectrum from a first order phase transition is a big challenge. Existing methods struggle particularly with strong coupling. Why? The usual false vacuum decay formalism is well-defined for tree-level bounces, but... ...for radiatively induced phase-transitions, it needs modifications. A related issue is that an all-orders calculation of the effective action is manifestly convex.  What does that mean? A convex potential doesn't have more than one minimum -&gt; no first order phase transition... ...""coarse-graining"" in momentum scale is a useful solution. However, the accuracy of coarse graining usually depends on a large ratio of scales or a weak coupling.  We propose an alternative, where we enforce locality in field space (see fig) rather than in momentum space... <LINK> ...that can be done in the language of the functional renormalization group, as we show.  Moreover, we work out a simple example, which we compare to the result found in other methods. As expected, the difference creep in at stronger coupling... <LINK> ...We hope to develop this method further, and eventually study things like confinement / chiral symmetry breaking and the resulting gravitational wave spectra. I can't wait to learn more.  A big thank you to my wonderful collaborators! It has been an absolute joy ❤️ Oops, MurAyama, apologies!!",https://arxiv.org/abs/2104.10687,"We propose a simple non-perturbative formalism for false vacuum decay using functional methods. We introduce the quasi-stationary effective action, a bounce action that non-perturbatively incorporates radiative corrections and is robust to strong couplings. The quasi-stationary effective action obeys an exact flow equation in a modified functional renormalization group with a motivated regulator functional. We demonstrate the use of this formalism in a simple toy model and compare our result with that obtained in perturbation theory. ",Non-perturbative methods for false vacuum decay,8,"['New paper! \n\nNon-perturbative methods for false vacuum decay\n<LINK>\nwith the amazing Eleanor Hall (@quarkygirl) and Hitoshi Muruyama (@sleptogenesis) \n\nWe propose a new (non-perturbative!) technique to calculate false vacuum decay rates. Important, because...', ""...accurate false vacuum decay calculations are needed to predict the resulting gravitational wave spectra. \n\nI'll try to give a brief explanation below, but I also highly recommend Nell's excellent slides on the topic: https://t.co/PlOhCeZI4H at #DarkSectorRainbow last month..."", '...So, accurately calculating the gravitational wave spectrum from a first order phase transition is a big challenge.\n\nExisting methods struggle particularly with strong coupling. Why?\n\nThe usual false vacuum decay formalism is well-defined for tree-level bounces, but...', ""...for radiatively induced phase-transitions, it needs modifications. A related issue is that an all-orders calculation of the effective action is manifestly convex. \n\nWhat does that mean? A convex potential doesn't have more than one minimum -&gt; no first order phase transition..."", '...""coarse-graining"" in momentum scale is a useful solution. However, the accuracy of coarse graining usually depends on a large ratio of scales or a weak coupling. \n\nWe propose an alternative, where we enforce locality in field space (see fig) rather than in momentum space... https://t.co/peSC3QKVDP', '...that can be done in the language of the functional renormalization group, as we show. \n\nMoreover, we work out a simple example, which we compare to the result found in other methods. As expected, the difference creep in at stronger coupling... https://t.co/tY9KJU24Mo', ""...We hope to develop this method further, and eventually study things like confinement / chiral symmetry breaking and the resulting gravitational wave spectra. I can't wait to learn more. \n\nA big thank you to my wonderful collaborators! It has been an absolute joy ❤️"", 'Oops, MurAyama, apologies!!']",21,04,1905
60,137,1350547329028419585,257986573,Khalid Saqr,#COVID19 spreads via #airborne transmission of expiratory aerosols. #CFD is used to simulate the #aerodynamics of #SARS_CoV_2. Read my #SystematicReview of CFD studies of COVID-19 and find out how can we evaluate the reproducibility of CFD simulations: <LINK>,https://arxiv.org/abs/2101.04874,"There is overwhelming evidence on SARS-CoV-2 Airborne Transmission (AT) in the ongoing COVID-19 outbreak. It is extraordinarily difficult, however, to deduce a generalized framework to assess the relative airborne transmission risk with respect to other modes. This is due to the complex biophysics entailed in such phenomena. Since the SARS outbreak in 2002, Computational Fluid Dynamics (CFD) has been one of the main tools scientists used to investigate AT of respiratory viruses. Now, CFD simulations produce intuitive and physically plausible colour-coded results that help scientists understand SARS-CoV-2 airborne transmission patterns. In addition to validation requirements, for any CFD model to be of epistemic value to the scientific community; it must be reproducible. In 2020, more than 45 published studies investigated SARS-CoV-2 airborne transmission in different scenarios using CFD. Here, I systematically review the published CFD studies of COVID-19 and discuss their reproducibility criteria with respect to the CFD modeling process. Using a Weighted Scoring Model (WSM), I propose a novel reproducibility index for CFD simulations of SARS-CoV-2 AT. The proposed index $(0 \leq R^{CFD}_j \leq 1)$ relies on three reproducibility criteria comprising 10 elements that represent the possibility of a CFD study (j) to be reproduced. Frustratingly, only 3 of 23 studies (13%) achieved full reproducibility index $(R^{CFD}_j\geq 0.9)$ while the remaining 87% were found generally irreproducible $(R^{CFD}_j<0.9)$. Without reproducible models, the scientific benefit of CFD simulations will remain hindered, fragmented and limited. In conclusion, I call the scientific community to apply more rigorous measures on reporting and publishing CFD simulations in COVID-19 research. ","Amicus Plato, sed magis amica veritas: There is a reproducibility crisis
  in COVID-19 Computational Fluid Dynamics studies",1,['#COVID19 spreads via #airborne transmission of expiratory aerosols. #CFD is used to simulate the #aerodynamics of #SARS_CoV_2. Read my #SystematicReview of CFD studies of COVID-19 and find out how can we evaluate the reproducibility of CFD simulations: <LINK>'],21,01,259
61,15,1387568794650902529,2337598033,Geraint F. Lewis,"Paper day! New results from the S5 survey on the demise of a dwarf galaxy in the Galactic halo! Led by Terese Hansen, authors include @alexanderpji @sazabi_li @andycaseydesign @deniserkal @kwkuehn @dougalmackey @FadAstra @norashipp @JossBlandHawtho <LINK> <LINK>",https://arxiv.org/abs/2104.13883,"The recently discovered Indus stellar stream exhibits a diverse chemical signature compared to what is found for most other streams due to the abundances of two outlier stars, Indus$\_$0 and Indus$\_$13. Indus$\_$13, exhibits an extreme enhancement in rapid neutron-capture ($r$-)process elements with $\mathrm{[Eu/Fe]} = +1.81$. It thus provides direct evidence of the accreted nature of $r$-process enhanced stars. In this paper we present a detailed chemical analysis of the neutron-capture elements in Indus$\_$13, revealing the star to be slightly actinide poor. The other outlier, Indus$\_0$, displays a globular cluster-like signature with high N, Na, and Al abundances, while the rest of the Indus stars show abundances compatible with a dwarf galaxy origin. Hence, Indus$\_0$ provides the first chemical evidence of a fully disrupted dwarf containing a globular cluster. We use the chemical signature of the Indus stars to discuss the nature of the stream progenitor which was likely a chemically evolved system, with a mass somewhere in the range from Ursa Minor to Fornax. ","${S}^5$: The destruction of a bright dwarf galaxy as revealed by the
  chemistry of the Indus stellar stream",1,"['Paper day! New results from the S5 survey on the demise of a dwarf galaxy in the Galactic halo!\n\nLed by Terese Hansen, authors include @alexanderpji @sazabi_li @andycaseydesign\n@deniserkal @kwkuehn @dougalmackey @FadAstra @norashipp @JossBlandHawtho\n\n<LINK> <LINK>']",21,04,262
62,112,1302868135633211392,904084910163537920,Stefan Szeider (Hiring PhD students),Two teams compete.  Team A uses modern SAT solvers on a 20-year-old computer. Team B uses 20-year-old SAT solvers on a modern computer.  Who solves more problem instances?  New @cp2020conf paper ➡️<LINK> ➡️<LINK>  Funded by @FWF_at @WWTF <LINK> @DvH24375691 @cp2020conf @FWF_at @WWTF Don't forget that we are dealing here with an NP-complete problem (SAT). The search space managed by new hw/sw is of orders of magnitude larger.,https://arxiv.org/abs/2008.02215,"We compare the impact of hardware advancement and algorithm advancement for SAT solving over the last two decades. In particular, we compare 20-year-old SAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old hardware. Our findings show that the progress on the algorithmic side has at least as much impact as the progress on the hardware side. ",A Time Leap Challenge for SAT Solving,2,"['Two teams compete. \n\nTeam A uses modern SAT solvers on a 20-year-old computer.\n\nTeam B uses 20-year-old SAT solvers on a modern computer.\n \nWho solves more problem instances? \n\nNew @cp2020conf paper ➡️<LINK>\n➡️<LINK> \n\nFunded by @FWF_at @WWTF <LINK>', ""@DvH24375691 @cp2020conf @FWF_at @WWTF Don't forget that we are dealing here with an NP-complete problem (SAT). The search space managed by new hw/sw is of orders of magnitude larger.""]",20,08,428
63,138,1428336031078756353,1164202716,Magnus Jonsson,"Thank you @KAWstiftelsen for the nice description of our project, and for enabling our research. Our latest study just got available online, where we demonstrate electrical tuning of the nanoantennas. <LINK> <LINK> ...and here is the previous publication that is mentioned in the text. <LINK>",https://arxiv.org/abs/2108.04045,"Nanostructures of conventional metals offer manipulation of light at the nanoscale but are limited to static behavior due to their fixed material properties. To develop the next frontier of dynamic nanooptics and metasurfaces, we utilize the redox-tunable optical properties of conducting polymers, which were recently shown to be capable of sustaining plasmons in their most conducting oxidized state. Using nanodisks of poly(3,4-ethylenedioxythiophene:sulfate) (PEDOT:Sulf) as a model system, we present the first electrically tunable conducting polymer nanooptical antennas. In addition to repeated on/off switching of the polymeric nanoantennas, we demonstrate the possibility for gradual electrical tuning of their nanooptical response, which was found to be related to the modulation of both density and mobility of the mobile polaronic charge carriers in the polymer. The presented concept takes important steps towards electrically tunable metasurfaces with truly dynamic optical nanoantenna pixels, with not only varying farfield but also tunable nearfield. The work paves the way for applications ranging from tunable flat metaoptics to adaptable smart windows. ",Electrical Tuning of Plasmonic Conducting Polymer Nanoantennas,2,"['Thank you @KAWstiftelsen for the nice description of our project, and for enabling our research. Our latest study just got available online, where we demonstrate electrical tuning of the nanoantennas. <LINK> <LINK>', '...and here is the previous publication that is mentioned in the text. https://t.co/0MVp22eSE6']",21,08,292
64,74,1382968473953918978,328430286,Jad C. Halimeh,New paper <LINK>. We show analytically and numerically that #GaugeInvariance can be enforced at volume-independent protection strength in the thermodynamic limit and for large link spin lengths in ultracold-atom setups of #QLM #LatticeGaugeTheories. @HaukeGroup <LINK>,https://arxiv.org/abs/2104.07040,"Although gauge invariance is a postulate in fundamental theories of nature such as quantum electrodynamics, in quantum-simulation implementations of gauge theories it is compromised by experimental imperfections. In a recent work [Halimeh and Hauke, \href{this https URL}{Phys. Rev. Lett. \textbf{125}, 030503 (2020)}], it has been shown in finite-size spin-$1/2$ quantum link lattice gauge theories that upon introducing an energy-penalty term of sufficiently large strength $V$, unitary gauge-breaking errors at strength $\lambda$ are suppressed $\propto\lambda^2/V^2$ up to all accessible evolution times. Here, we show numerically that this result extends to quantum link models in the thermodynamic limit and with larger spin-$S$. As we show analytically, the dynamics at short times is described by an \textit{adjusted} gauge theory up to a timescale that is at earliest $\tau_\text{adj}\propto\sqrt{V/V_0^3}$, with $V_0$ an energy factor. Moreover, our analytics predicts that a renormalized gauge theory dominates at intermediate times up to a timescale $\tau_\text{ren}\propto\exp(V/V_0)/V_0$. In both emergent gauge theories, $V$ is volume-independent and scales at worst $\sim S^2$. Furthermore, we numerically demonstrate that robust gauge invariance is also retained through a single-body gauge-protection term, which is experimentally straightforward to implement in ultracold-atom setups and NISQ devices. ",Reliability of lattice gauge theories in the thermodynamic limit,1,['New paper <LINK>. We show analytically and numerically that #GaugeInvariance can be enforced at volume-independent protection strength in the thermodynamic limit and for large link spin lengths in ultracold-atom setups of #QLM #LatticeGaugeTheories. @HaukeGroup <LINK>'],21,04,268
65,85,1470217150606426113,1273467805283659777,Pang Wei Koh,"We're excited to announce WILDS v2.0, which adds unlabeled data to 8 datasets! This lets us benchmark methods for domain adaptation & representation learning. All labeled data & evaluations are unchanged.  (New) paper: <LINK> Website: <LINK> 🧵 <LINK> Unlabeled data can be a powerful source of leverage. It comes from a mixture of: - source domains (same as the labeled training data) - target domains (same as the labeled test data) - extra domains with no labeled data.  We illustrate this for the GlobalWheat dataset: <LINK> We evaluated domain adaptation, self-training, & self-supervised methods on these datasets. Unfortunately, many methods did not do better than standard supervised training, despite using additional unlabeled data. This table shows OOD test performance; higher numbers are better. <LINK> In contrast, prior work has shown these methods to be successful on standard domain adaptation tasks such as DomainNet, which we replicate below. This underscores the importance of developing and evaluating methods on a broad variety of distribution shifts. <LINK> We've added the unlabeled data loaders + method implementations to our Python package: <LINK>. They're easy to use: check out the code snippet below!  We've also updated our leaderboards to accept submissions with and without unlabeled data. <LINK> We've uploaded the exact commands and hyperparameters used in our paper, as well as trained model checkpoints, to <LINK>. This is thanks to @tonyh_lee, who oversaw all of the experimental infrastructure and made it fully reproducible on @CodaLabWS. We're grateful to everyone who helped us with WILDS and the v2.0 update: <LINK>.  We'd also like to thank Jiang et al. for <LINK> and Zhang et al. for <LINK>, which were very helpful references for our method implementations. This was joint work with @shiorisagawa* @tonyh_lee* IrenaGao*, and @sangmichaelxie @kendrick_shen @ananyaku @weihua916 @michiyasunaga HenrikMarklund @sarameghanbeery @EtienneDavid @IanStavness @guowei_net @jure @kate_saenko_ @tatsu_hashimoto @svlevine @chelseabfinn @percyliang. We'll be presenting this at the DistShift workshop at NeurIPS. Find us at our poster on Dec 13, 1-3pm Pacific Time: <LINK>  Read our paper for more details and analysis: <LINK>",http://arxiv.org/abs/2112.05090,"Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as the evaluation metrics. On these datasets, we systematically benchmark state-of-the-art methods that leverage unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development and evaluation, we provide an open-source package that automates data loading and contains all of the model architectures and methods used in this paper. Code and leaderboards are available at this https URL ",Extending the WILDS Benchmark for Unsupervised Adaptation,9,"[""We're excited to announce WILDS v2.0, which adds unlabeled data to 8 datasets! This lets us benchmark methods for domain adaptation &amp; representation learning. All labeled data &amp; evaluations are unchanged.\n \n(New) paper: <LINK>\nWebsite: <LINK>\n\n🧵 <LINK>"", 'Unlabeled data can be a powerful source of leverage. It comes from a mixture of:\n- source domains (same as the labeled training data)\n- target domains (same as the labeled test data) \n- extra domains with no labeled data. \n\nWe illustrate this for the GlobalWheat dataset: https://t.co/kT11wMPDaa', 'We evaluated domain adaptation, self-training, &amp; self-supervised methods on these datasets. Unfortunately, many methods did not do better than standard supervised training, despite using additional unlabeled data.\n\nThis table shows OOD test performance; higher numbers are better. https://t.co/WgjVMZ79mG', 'In contrast, prior work has shown these methods to be successful on standard domain adaptation tasks such as DomainNet, which we replicate below. This underscores the importance of developing and evaluating methods on a broad variety of distribution shifts. https://t.co/dPPPeJZYCT', ""We've added the unlabeled data loaders + method implementations to our Python package: https://t.co/S73kjDxMis. They're easy to use: check out the code snippet below! \n\nWe've also updated our leaderboards to accept submissions with and without unlabeled data. https://t.co/Buy612P2IX"", ""We've uploaded the exact commands and hyperparameters used in our paper, as well as trained model checkpoints, to https://t.co/qI7yvTWGsT. This is thanks to @tonyh_lee, who oversaw all of the experimental infrastructure and made it fully reproducible on @CodaLabWS."", ""We're grateful to everyone who helped us with WILDS and the v2.0 update: https://t.co/1CAsr8JV99. \n\nWe'd also like to thank Jiang et al. for https://t.co/CSIYF8gcFT and Zhang et al. for https://t.co/Kla5i4C9Y9, which were very helpful references for our method implementations."", 'This was joint work with @shiorisagawa* @tonyh_lee* IrenaGao*, and @sangmichaelxie @kendrick_shen @ananyaku @weihua916 @michiyasunaga HenrikMarklund @sarameghanbeery @EtienneDavid @IanStavness @guowei_net @jure @kate_saenko_ @tatsu_hashimoto @svlevine @chelseabfinn @percyliang.', ""We'll be presenting this at the DistShift workshop at NeurIPS. Find us at our poster on Dec 13, 1-3pm Pacific Time: https://t.co/gid3wBSqb6\n \nRead our paper for more details and analysis: https://t.co/m95JSY9LbJ""]",21,12,2259
66,212,1514007913970421763,874897164,Michael Green,"Ever play a level and felt ""tied up""? Like it was too punishing or maybe not punishing enough? Introducing PDSM: Persona-driven Dominant/Submissive Map Generation for Tutorials. We use quality-diversity methods to find #tutorial levels that suit personas <LINK> <LINK> Tutorial levels are scenarios in which the player can explore and discover different rules and #game #mechanics. Procedural personas can guide generators to create content which encourages or discourages certain #playstyle behaviors. In this system, we use procedural personas to calculate the behavioral characteristics of levels which are evolved using the quality-diversity algorithm known as Constrained MAP-Elites. An evolved map's quality is determined by its simplicity: the simpler it is, the better it is. <LINK> We measure #persona health for our behavioral characteristics. The maps are therefore measured by how high or low HP is for different personas. Some personas dominate certain maps but are submissive on others. <LINK> We show that our generated maps can strongly encourage or discourage different persona-like behaviors and range from simple solutions to complex puzzle-levels, making them perfect candidates for a tutorial generative system. <LINK> Our intention is to incorporate this system into a persona-adaptive tutorial sequencer so that players may learn to adapt to different playstyles. I hope that one day this could be present in commercial, so that players could play #personalized levels just for them! <LINK> Big thanks to my collaborators @Amidos2006 from the University of Malta and @MasterMilkX and @togelius from the Game Innovation Lab at NYU for their help on this research! @Amidos2006 @MasterMilkX @togelius Heres a link to the paper if you want to check it out: <LINK>",https://arxiv.org/abs/2204.05217,"In this paper, we present a method for automated persona-driven video game tutorial level generation. Tutorial levels are scenarios in which the player can explore and discover different rules and game mechanics. Procedural personas can guide generators to create content which encourages or discourages certain playstyle behaviors. In this system, we use procedural personas to calculate the behavioral characteristics of levels which are evolved using the quality-diversity algorithm known as Constrained MAP-Elites. An evolved map's quality is determined by its simplicity: the simpler it is, the better it is. Within this work, we show that the generated maps can strongly encourage or discourage different persona-like behaviors and range from simple solutions to complex puzzle-levels, making them perfect candidates for a tutorial generative system. ",Persona-driven Dominant/Submissive Map (PDSM) Generation for Tutorials,8,"['Ever play a level and felt ""tied up""? Like it was too punishing or maybe not punishing enough? Introducing PDSM: Persona-driven Dominant/Submissive Map Generation for Tutorials. We use quality-diversity methods to find #tutorial levels that suit personas\n<LINK> <LINK>', 'Tutorial levels are scenarios in which the player can explore and discover different rules and #game #mechanics. Procedural personas can guide generators to create content which encourages or discourages certain #playstyle behaviors.', ""In this system, we use procedural personas to calculate the behavioral characteristics of levels which are evolved using the quality-diversity algorithm known as Constrained MAP-Elites. An evolved map's quality is determined by its simplicity: the simpler it is, the better it is. https://t.co/Z84zknMnz1"", 'We measure #persona health for our behavioral characteristics. The maps are therefore measured by how high or low HP is for different personas. Some personas dominate certain maps but are submissive on others. https://t.co/KZ9XaKmWF2', 'We show that our generated maps can strongly encourage or discourage different persona-like behaviors and range from simple solutions to complex puzzle-levels, making them perfect candidates for a tutorial generative system. https://t.co/pH1s5JJfFr', 'Our intention is to incorporate this system into a persona-adaptive tutorial sequencer so that players may learn to adapt to different playstyles. I hope that one day this could be present in commercial, so that players could play #personalized levels just for them! https://t.co/kWUAM8uVJq', 'Big thanks to my collaborators @Amidos2006 from the University of Malta and @MasterMilkX and @togelius from the Game Innovation Lab at NYU for their help on this research!', '@Amidos2006 @MasterMilkX @togelius Heres a link to the paper if you want to check it out: https://t.co/gP13HY25Zx']",22,04,1782
67,73,1260939443139153923,476582730,Shakir Mohamed,"Nowcasting was a new and fun field for us, and after a while we thought to write a paper we wished we had when we started. Thankful for hard work by @RachelPrudden & team 🎉 This areas exposes many core questions in ML so a great for advancing research <LINK> <LINK>",https://arxiv.org/abs/2005.04988,"A 'nowcast' is a type of weather forecast which makes predictions in the very short term, typically less than two hours - a period in which traditional numerical weather prediction can be limited. This type of weather prediction has important applications for commercial aviation; public and outdoor events; and the construction industry, power utilities, and ground transportation services that conduct much of their work outdoors. Importantly, one of the key needs for nowcasting systems is in the provision of accurate warnings of adverse weather events, such as heavy rain and flooding, for the protection of life and property in such situations. Typical nowcasting approaches are based on simple extrapolation models applied to observations, primarily rainfall radar. In this paper we review existing techniques to radar-based nowcasting from environmental sciences, as well as the statistical approaches that are applicable from the field of machine learning. Nowcasting continues to be an important component of operational systems and we believe new advances are possible with new partnerships between the environmental science and machine learning communities. ","A review of radar-based nowcasting of precipitation and applicable
  machine learning techniques",1,"['Nowcasting was a new and fun field for us, and after a while we thought to write a paper we wished we had when we started. Thankful for hard work by @RachelPrudden &amp; team 🎉 This areas exposes many core questions in ML so a great for advancing research <LINK> <LINK>']",20,05,265
68,20,1075310613222572032,954415596,J. Trayford,"🚨 New paper 🚨 - we investigate the spatially resolved main sequence, mass-metallicity relations and their emergence in the EAGLE simulations. These are a neat way to probe the internal structure and population statistics of galaxies simultaneously <LINK>",https://arxiv.org/abs/1812.06984v1,"We explore scaling relations between the physical properties of spatially resolved regions within the galaxies that emerge in the Evolution and Assembly of GaLaxies and their Environments (EAGLE) hydrodynamical, cosmological simulations. Using 1 kpc-scale spaxels, we compute the relationships between the star formation rate and stellar mass surface densities, i.e. the spatially resolved star-forming main sequence (rSFMS), and between the gas metallicity and the stellar mass surface density, i.e. the spatially resolved mass-metallicity relation (rMZR). We compare to observed relations derived from integral field unit surveys and imaging of galaxies. EAGLE reproduces the slope of the local ($z\approx0.1$) rSFMS well, but with a $\approx-0.15$ dex offset, close to that found for the galaxy-integrated relation. The shape of the rMZR agrees reasonably well with observations, replicating the characteristic turnover at high surface density, which we show is due to AGN feedback. The residuals of the rSFMS and rMZR are negatively (positively) correlated at low (high) surface density. The rSFMS becomes shallower as the simulation evolves from $z=2$ to 0.1, a manifestation of inside-out galaxy formation. The shape of the rMZR also exhibits dramatic evolution, from a convex profile at $z=2$ to the observed concave profile at $z=0.1$, such that the gas in regions of high stellar density is more enriched at higher redshift. The redshift independence of the relationship between the galaxy-wide gas fraction and metallicity in EAGLE galaxies is not preserved on 1 kpc scales, implying that chemical evolution is non-local due to the transport of gas and metals within galaxies. ","] Resolved galaxy scaling relations in the EAGLE simulation: star
  formation, metallicity and stellar mass on kpc scales",1,"['🚨 New paper 🚨 - we investigate the spatially resolved main sequence, mass-metallicity relations and their emergence in the EAGLE simulations. These are a neat way to probe the internal structure and population statistics of galaxies simultaneously <LINK>']",18,12,254
69,121,1283060417237934081,2166703328,Dr. Cℏarles D. Brown II,"Changing fields from PhD to PD has been a wild ride but I love it. In my subgroup's new paper, we load a Bose-Einstein condensate into a special lattice made of light, and study how atom-atom interactions affect the lattice-trapped atoms' motional energy <LINK> @PhysMossman Thanks! @cosmoloony Very cool. We should definitely talk about BECs in different environments some time",https://arxiv.org/abs/2007.05928,"Geometric frustration of particle motion in a kagome lattice causes the single-particle band structure to have a flat s-orbital band. We probe this band structure by exciting a Bose-Einstein condensate into excited Bloch states of an optical kagome lattice, and then measuring the group velocity through the atomic momentum distribution. We find that interactions renormalize the band structure of the kagome lattice, greatly increasing the dispersion of the third band that, according to non-interacting band theory, should be nearly non-dispersing. Measurements at various lattice depths and gas densities agree quantitatively with predictions of the lattice Gross-Pitaevskii equation, indicating that the observed distortion of band structure is caused by the disortion of the overall lattice potential away from the kagome geometry by interactions. ","Interaction-Enhanced Group Velocity of Bosons in the Flat Band of an
  Optical Kagome Lattice",3,"[""Changing fields from PhD to PD has been a wild ride but I love it. In my subgroup's new paper, we load a Bose-Einstein condensate into a special lattice made of light, and study how atom-atom interactions affect the lattice-trapped atoms' motional energy <LINK>"", '@PhysMossman Thanks!', '@cosmoloony Very cool. We should definitely talk about BECs in different environments some time']",20,07,378
70,4,1003625222812131329,1900087399,Warwick Astro Group,New research from our own Elizabeth Stanway from @warwickuni and @astro_jje from @UoA_Physics! Reevaluating Old Stellar Populations with the @astroBPASS models - Read the press release here <LINK> and the paper on arxiv here <LINK> <LINK> (Image credit: Mark Garlick/University of Warwick),https://arxiv.org/abs/1805.08784,"Determining the properties of old stellar populations (those with age >1 Gyr) has long involved the comparison of their integrated light, either in the form of photometry or spectroscopic indexes, with empirical or synthetic templates. Here we reevaluate the properties of old stellar populations using a new set of stellar population synthesis models, designed to incorporate the effects of binary stellar evolution pathways as a function of stellar mass and age. We find that single-aged stellar population models incorporating binary stars, as well as new stellar evolution and atmosphere models, can reproduce the colours and spectral indices observed in both globular clusters and quiescent galaxies. The best fitting model populations are often younger than those derived from older spectral synthesis models, and may also lie at slightly higher metallicities. ",Reevaluating Old Stellar Populations,2,"['New research from our own Elizabeth Stanway from @warwickuni and @astro_jje from @UoA_Physics!  Reevaluating Old Stellar Populations with the @astroBPASS models - Read the press release here <LINK> and the paper on arxiv here <LINK> <LINK>', '(Image credit:  Mark Garlick/University of Warwick)']",18,05,289
71,68,1027037843045023744,315718949,Clément Canonne,"New paper out, with J. Acharya, C. Freitag, and H. Tyagi: Locally private algorithms for hypothesis testing. I'm pretty excited about the underlying technique: dinosaurs! <LINK>  (picture from <LINK>) <LINK> Blimey, I do suck at Twitter handles. w/ @AcharyaJayadev, tagging #privacy #statistics #bambiraptor",https://arxiv.org/abs/1808.02174,"We study the problem of distribution testing when the samples can only be accessed using a locally differentially private mechanism and focus on two representative testing questions of identity (goodness-of-fit) and independence testing for discrete distributions. We are concerned with two settings: First, when we insist on using an already deployed, general-purpose locally differentially private mechanism such as the popular RAPPOR or the recently introduced Hadamard Response for collecting data, and must build our tests based on the data collected via this mechanism; and second, when no such restriction is imposed, and we can design a bespoke mechanism specifically for testing. For the latter purpose, we introduce the Randomized Aggregated Private Testing Optimal Response (RAPTOR) mechanism which is remarkably simple and requires only one bit of communication per sample. We propose tests based on these mechanisms and analyze their sample complexities. Each proposed test can be implemented efficiently. In each case (barring one), we complement our performance bounds for algorithms with information-theoretic lower bounds and establish sample optimality of our proposed algorithm. A peculiar feature that emerges is that our sample-optimal algorithm based on RAPTOR uses public-coins, and any test based on RAPPOR or Hadamard Response, which are both private-coin mechanisms, requires significantly more samples. ",Test without Trust: Optimal Locally Private Distribution Testing,2,"[""New paper out, with J. Acharya, C. Freitag, and H. Tyagi: Locally private algorithms for hypothesis testing. I'm pretty excited about the underlying technique: dinosaurs! \n<LINK> \n\n(picture from <LINK>) <LINK>"", 'Blimey, I do suck at Twitter handles. w/ @AcharyaJayadev, tagging #privacy #statistics #bambiraptor']",18,08,307
72,116,1489529011462156288,3358932880,Joschka Roffe,"New paper on the arXiv: Bias-tailored quantum LDPC codes, w/ Larry Cohen, @armanda_oq, @daryuschandra and @earltcampbell <LINK> 🧵👇(1/n) Often, in QEC theory, we like to study things in terms of depolarising noise where px=py=pz. However, in the real world, it will usually be the case that the strength of one error types dominates over the others. (2/n) Last year Ataides et al. showed that a modified form of the surface code, the XZZX code, can take advantage of biased-noise to significantly improve code performance <LINK> (3/n) In this project, we set out to extend the idea of bias-tailoring to more general quantum LDPC codes (which promise much lower overheads than the surface code). To do this, we first looked to unpick exactly what allows the XZZX code to respond so well to biased noise... (4/n) Essentially, this boils down to two modifications over the surface code: 1) A redefinition of the stabilisers that ensures the XZZX code decouples to sets of repetition codes in the infinite-bias limit; 2) Twisted boundary checks that lead to longer logical operators... (5/n) We discovered that both characteristics of the XZZX code -- stabiliser redefinition and boundary twists -- arise naturally from a modified form of Pantaleev and Kalachev's lifted product (of recent asymptotically good d~n scaling fame)... (6/n) This modified form, which we've coined the bias-tailored lifted product, is obtained from the standard lifted product via a Clifford transformation on a subset of the qubits. The bias-tailored lifted product can be applied to any pair of quasi-cyclic LDPC codes to... (7/n) construct quantum LDPC codes that inherit the high-bias performance of the XZZX code. Pictured is a decoding plot of a bias-tailored lifted product code with parameters [[416,16,d~20]] under increasing X-bias... (8/n) <LINK> The code construction scripts and decoding simulation routines can be found on the Github repo for this project. <LINK>",https://arxiv.org/abs/2202.01702,"Bias-tailoring allows quantum error correction codes to exploit qubit noise asymmetry. Recently, it was shown that a modified form of the surface code, the XZZX code, exhibits considerably improved performance under biased noise. In this work, we demonstrate that quantum low density parity check codes can be similarly bias-tailored. We introduce a bias-tailored lifted product code construction that provides the framework to expand bias-tailoring methods beyond the family of 2D topological codes. We present examples of bias-tailored lifted product codes based on classical quasi-cyclic codes and numerically assess their performance using a belief propagation plus ordered statistics decoder. Our Monte Carlo simulations, performed under asymmetric noise, show that bias-tailored codes achieve several orders of magnitude improvement in their error suppression relative to depolarising noise. ",Bias-tailored quantum LDPC codes,9,"['New paper on the arXiv: Bias-tailored quantum LDPC codes, w/ Larry Cohen, @armanda_oq, @daryuschandra and @earltcampbell <LINK> 🧵👇(1/n)', 'Often, in QEC theory, we like to study things in terms of depolarising noise where px=py=pz. However, in the real world, it will usually be the case that the strength of one error types dominates over the others. (2/n)', 'Last year Ataides et al. showed that a modified form of the surface code, the XZZX code, can take advantage of biased-noise to significantly improve code performance https://t.co/tKeA2ktjO3 (3/n)', 'In this project, we set out to extend the idea of bias-tailoring to more general quantum LDPC codes (which promise much lower overheads than the surface code). To do this, we first looked to unpick exactly what allows the XZZX code to respond so well to biased noise... (4/n)', 'Essentially, this boils down to two modifications over the surface code: 1) A redefinition of the stabilisers that ensures the XZZX code decouples to sets of repetition codes in the infinite-bias limit; 2) Twisted boundary checks that lead to longer logical operators... (5/n)', ""We discovered that both characteristics of the XZZX code -- stabiliser redefinition and boundary twists -- arise naturally from a modified form of Pantaleev and Kalachev's lifted product (of recent asymptotically good d~n scaling fame)... (6/n)"", ""This modified form, which we've coined the bias-tailored lifted product, is obtained from the standard lifted product via a Clifford transformation on a subset of the qubits. The bias-tailored lifted product can be applied to any pair of quasi-cyclic LDPC codes to... (7/n)"", 'construct quantum LDPC codes that inherit the high-bias performance of the XZZX code. Pictured is a decoding plot of a bias-tailored lifted product code with parameters [[416,16,d~20]] under increasing X-bias... (8/n) https://t.co/ZvL02A91hL', 'The code construction scripts and decoding simulation routines can be found on the Github repo for this project.\nhttps://t.co/xNJw8sUyYH']",22,02,1950
73,54,1296501230605406209,14551614,Jason Weston,"(1/3) New paper!  Instead of a *static* train/valid/test setup, ML systems should become more useful as they interact with people & the world. As a step in that direction, we deploy dialogue as a game and show that models improve by talking to humans! <LINK> (2/3) ...and the more models improve, the more humans want to talk to them! Virtuous circle! Intrinsically motivated players provide a better distbn & collection is more efficient than crowdsourcing.  We collect ~461k utterances over ~13k players, and release the data. (3/3)  with @kurt_shuster* @JackUrbs* @em_dinan Arthur Szlam @jaseweston (* joint first) Built in ParlAI @parlai_parley. See: <LINK>",https://arxiv.org/abs/2008.08076,"Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect. ",Deploying Lifelong Open-Domain Dialogue Learning,3,"['(1/3) New paper! \n\nInstead of a *static* train/valid/test setup, ML systems should become more useful as they interact with people &amp; the world.\n\nAs a step in that direction, we deploy dialogue as a game and show that models improve by talking to humans!\n\n<LINK>', '(2/3)\n\n...and the more models improve, the more humans want to talk to them! Virtuous circle!\n\nIntrinsically motivated players provide a better distbn &amp; collection is more efficient than crowdsourcing. \n\nWe collect ~461k utterances over ~13k players, and release the data.', '(3/3) \n\nwith @kurt_shuster* @JackUrbs*  @em_dinan Arthur Szlam @jaseweston  (* joint first)\n\nBuilt in ParlAI @parlai_parley.\nSee: https://t.co/xVAaHVEwmc']",20,08,661
74,118,1456175144683048961,805477439648440321,Rob Kavanagh,"Delighted to have our new paper accepted by @RAS_Journals! We explored the variability of CMEs and associated radio emission with the changing surface magnetic field of Epsilon Eridani. Congrats to @dualta93 for leading this great work! <LINK> <LINK> @RAS_Journals @dualta93 Force of habit, I meant @AAS_Publishing! 🤠",https://arxiv.org/abs/2111.02284,"We simulate possible stellar coronal mass ejection (CME) scenarios over the magnetic cycle of $\epsilon$ Eridani (18 Eridani; HD 22049). We use three separate epochs from 2008, 2011, and 2013, and estimate the radio emission frequencies associated with these events. These stellar eruptions have proven to be elusive, although a promising approach to detect and characterise these phenomena are low-frequency radio observations of potential type II bursts as CME induced shocks propagate through the stellar corona. Stellar type II radio bursts are expected to emit below 450 MHz, similarly to their solar counterparts. We show that the length of time these events remain above the ionospheric cutoff is not necessarily dependent on the stellar magnetic cycle, but more on the eruption location relative to the stellar magnetic field. We find that these type II bursts would remain within the frequency range of LOFAR for a maximum of 20-30 minutes post-eruption for the polar CMEs, (50 minutes for 2nd harmonics). We find evidence of slower equatorial CMEs, which result in slightly longer observable windows for the 2008 and 2013 simulations. Stellar magnetic geometry and strength has a significant effect on the detectability of these events. We place the CMEs in the context of the stellar mass-loss rate (27-48 $\times$ solar mass-loss rate), showing that they can amount to 3-50% of the stellar wind mass-loss rate for $\epsilon$ Eridani. Continuous monitoring of likely stellar CME candidates with low-frequency radio telescopes will be required to detect these transient events. ","Coronal mass ejections and type II radio emission variability during a
  magnetic cycle on the solar-type star $\epsilon$ Eridani",2,"['Delighted to have our new paper accepted by @RAS_Journals! We explored the variability of CMEs and associated radio emission with the changing surface magnetic field of Epsilon Eridani. Congrats to @dualta93 for leading this great work!\n\n<LINK> <LINK>', '@RAS_Journals @dualta93 Force of habit, I meant @AAS_Publishing! 🤠']",21,11,317
75,126,1204228552586981376,1057550940331540480,Jakub Otwinowski,"New work with Colin Lamont at MPIDS where we propose a new evolutionary optimization algorithm inspired by quantitative genetics. <LINK> cc @togelius -- summary to follow First we point out that natural selection performs a natural gradient step (known but under-appreciated). This is similar to what CMA-ES and NES do, but without any linear algebra, all you need are exponential weights. In other words there's a lot of similarity between classical multivariate quantitative genetics and information geometric optimization algorithms (chiefly NES). Then we discuss how new variants are generated. Mutations can knock you off a ridge in a fitness landscape. In quantitative genetics, recombination can sort of (not quite) make a variant that is aligned with the population, which is in turn aligned with the landscape We invent a phenotype recombination operator that preserves the phenotype covariance of the population, so new variants are aligned with the landscape. This recombination is like drawing from a normal distribution in CMA-ES, without having to store a covariance matrix! We put it all together to make a Quantitative Genetic Algorithm (QGA). We implement an adaptive scheme that increases selection based on adjusting the exponential weights to reach a target population entropy. It naturally incorporates all past information, and adapts to curvature QGA works pretty well, more or less like CMA-ES in benchmarks... but only after scanning over the single hyperparameter first. So figuring out other adaptive schemes is something to think about. We're new to this, and hope QGA inspires more work in evolutionary optimization. Maybe someone can try QGA on a large problem. The advantage is that the algorithm is ridiculously simple and you don't need to store a covariance matrix.",https://arxiv.org/abs/1912.03395,"Evolutionary algorithms, inspired by natural evolution, aim to optimize difficult objective functions without computing derivatives. Here we detail the relationship between population genetics and evolutionary optimization and formulate a new evolutionary algorithm. Optimization of a continuous objective function is analogous to searching for high fitness phenotypes on a fitness landscape. We summarize how natural selection moves a population along the non-euclidean gradient that is induced by the population on the fitness landscape (the natural gradient). Under normal approximations common in quantitative genetics, we show how selection is related to Newton's method in optimization. We find that intermediate selection is most informative of the fitness landscape. We describe the generation of new phenotypes and introduce an operator that recombines the whole population to generate variants that preserve normal statistics. Finally, we introduce a proof-of-principle algorithm that combines natural selection, our recombination operator, and an adaptive method to increase selection. Our algorithm is similar to covariance matrix adaptation and natural evolutionary strategies in optimization, and has similar performance. The algorithm is extremely simple in implementation with no matrix inversion or factorization, does not require storing a covariance matrix, and may form the basis of more general model-based optimization algorithms with natural gradient updates. ",Information-geometric optimization with natural selection,8,"['New work with Colin Lamont at MPIDS where we propose a new evolutionary optimization algorithm inspired by quantitative genetics. <LINK>  cc @togelius -- summary to follow', 'First we point out that natural selection performs a natural gradient step (known but under-appreciated). This is similar to what CMA-ES and NES do, but without any linear algebra, all you need are exponential weights.', ""In other words there's a lot of similarity between classical multivariate quantitative genetics and information geometric optimization algorithms (chiefly NES)."", 'Then we discuss how new variants are generated. Mutations can knock you off a ridge in a fitness landscape. In quantitative genetics, recombination can sort of (not quite) make a variant that is aligned with the population, which is in turn aligned with the landscape', 'We invent a phenotype recombination operator that preserves the phenotype covariance of the population, so new variants are aligned with the landscape. This recombination is like drawing from a normal distribution in CMA-ES, without having to store a covariance matrix!', 'We put it all together to make a Quantitative Genetic Algorithm (QGA). We implement an adaptive scheme that increases selection based on adjusting the exponential weights to reach a target population entropy. It naturally incorporates all past information, and adapts to curvature', ""QGA works pretty well, more or less like CMA-ES in benchmarks... but only after scanning over the single hyperparameter first. So figuring out other adaptive schemes is something to think about. We're new to this, and hope QGA inspires more work in evolutionary optimization."", ""Maybe someone can try QGA on a large problem. The advantage is that the algorithm is ridiculously simple and you don't need to store a covariance matrix.""]",19,12,1799
76,34,1443044522380779525,1015190157845258240,Yuji Matsumoto,Our new paper! <LINK> We calculate the size evolution of protoplanets through giant impacts and photoevaporation. We show the size distribution and the size ratios of planets as functions of the initial envelope fraction and mass of protoplanets. <LINK>,https://arxiv.org/abs/2109.13487,"The Kepler transit survey with follow-up spectroscopic observations has discovered numerous super-Earth sized planets and revealed intriguing features of their sizes, orbital periods, and their relations between adjacent planets. For the first time, we investigate the size evolution of planets via both giant impacts and photoevaporation to compare with these observed features. We calculate the size of a protoplanet, which is the sum of its core and envelope sizes, by analytical models. $N$-body simulations are performed to evolve planet sizes during the giant impact phase with envelope stripping via impact shocks. We consider the initial radial profile of the core mass and the initial envelope mass fractions as parameters. Inner planets can lose their whole envelopes via giant impacts, while outer planets can keep their initial envelopes since they do not experience giant impacts. Photoevaporation is simulated to evolve planet sizes afterward. Our results suggest that the period-radius distribution of the observed planets would be reproduced if we perform simulations in which the initial radial profile of the core mass follows a wide range of power-law distributions and the initial envelope mass fractions are $\sim0.1$. Moreover, our model shows that the adjacent planetary pairs have similar sizes and regular spacings, with slight differences from detailed observational results such as the radius gap. ","Size evolution of close-in super-Earths through giant impacts and
  photoevaporation",1,['Our new paper! <LINK> \nWe calculate the size evolution of protoplanets through giant impacts and photoevaporation. We show the size distribution and the size ratios of planets as functions of the initial envelope fraction and mass of protoplanets. <LINK>'],21,09,253
77,8,1245385591824347137,171674815,Mark Marley,"Nice new paper by Phillips et al. presenting a new cloudless grid of substellar models. <LINK> If you'd like to compare with our model photometry and evolution you can get them here <LINK> and spectra here <LINK> Differences: Phillips et al. employ the very latest alkali line profiles by Nikole Allard. Our online models used the previous iteration of her profiles. We both use updated equations of state of the evolution. I need to dig deeper to see exactly the differences. Comparing our evolution Luminosity(time) will elucidate how much uncertainty remains. Our disequilibrium models are essentially done but not yet online. We both use rainout, not pure equilibrium, chemistry but with slightly different approaches. Will need to compare carefully to see how much that matters. Having two independent groups make models is extremely valuable and I think we can learn a lot about where the field is by comparing/contrasting.",https://arxiv.org/abs/2003.13717,"We present a new set of solar metallicity atmosphere and evolutionary models for very cool brown dwarfs and self-luminous giant exoplanets, which we term ATMO 2020. Atmosphere models are generated with our state-of-the-art 1D radiative-convective equilibrium code ATMO, and are used as surface boundary conditions to calculate the interior structure and evolution of $0.001-0.075\,\mathrm{M_{\odot}}$ objects. Our models include several key improvements to the input physics used in previous models available in the literature. Most notably, the use of a new H-He equation of state including ab initio quantum molecular dynamics calculations has raised the mass by $\sim1-2\%$ at the stellar-substellar boundary and has altered the cooling tracks around the hydrogen and deuterium burning minimum masses. A second key improvement concerns updated molecular opacities in our atmosphere model ATMO, which now contains significantly more line transitions required to accurately capture the opacity in these hot atmospheres. This leads to warmer atmospheric temperature structures, further changing the cooling curves and predicted emission spectra of substellar objects. We present significant improvement for the treatment of the collisionally broadened potassium resonance doublet, and highlight the importance of these lines in shaping the red-optical and near-infrared spectrum of brown dwarfs. We generate three different grids of model simulations, one using equilibrium chemistry and two using non-equilibrium chemistry due to vertical mixing, all three computed self-consistently with the pressure-temperature structure of the atmosphere. We show the impact of vertical mixing on emission spectra and in colour-magnitude diagrams, highlighting how the $3.5-5.5\,\mathrm{\mu m}$ flux window can be used to calibrate vertical mixing in cool T-Y spectral type objects. ","A new set of atmosphere and evolution models for cool T-Y brown dwarfs
  and giant exoplanets",6,"[""Nice new paper by Phillips et al. presenting a new cloudless grid of substellar models. <LINK> \nIf you'd like to compare with our model photometry and evolution you can get them here\n<LINK>\nand spectra here\n<LINK>"", 'Differences: Phillips et al. employ the very latest alkali line profiles by Nikole Allard. Our online models used the previous iteration of her profiles.', 'We both use updated equations of state of the evolution. I need to dig deeper to see exactly the differences. Comparing our evolution Luminosity(time) will elucidate how much uncertainty remains.', 'Our disequilibrium models are essentially done but not yet online.', 'We both use rainout, not pure equilibrium, chemistry but with slightly different approaches. Will need to compare carefully to see how much that matters.', 'Having two independent groups make models is extremely valuable and I think we can learn a lot about where the field is by comparing/contrasting.']",20,03,929
78,78,1237302029166534656,561899047,Aki Vehtari,"New paper ""When are Bayesian model probabilities overconfident?"" by Oscar Oelrich, Shutong Ding, @MansMeg, me, and @matvil <LINK> (I had just a minor role, others did awesome work on this) <LINK> We demonstrate overconfidence in two high-profile applications in economics and neuroscience and derive the sampling variance in univariate and multivariate linear regression. <LINK> Overconfidence is likely when 1) the compared models give very different approximations of the data-gener. process, 2) the models are very flexible with large degrees of freedom that are not shared between the models, 3) the models underestimate the true variability in the data. <LINK> In the end we mention possible alternatives for model comparison and combination, but I realize we should have mentioned also that instead of trying to compare or combine bad models it's better to think harder and construct better models.",https://arxiv.org/abs/2003.04026,"Bayesian model comparison is often based on the posterior distribution over the set of compared models. This distribution is often observed to concentrate on a single model even when other measures of model fit or forecasting ability indicate no strong preference. Furthermore, a moderate change in the data sample can easily shift the posterior model probabilities to concentrate on another model. We document overconfidence in two high-profile applications in economics and neuroscience. To shed more light on the sources of overconfidence we derive the sampling variance of the Bayes factor in univariate and multivariate linear regression. The results show that overconfidence is likely to happen when i) the compared models give very different approximations of the data-generating process, ii) the models are very flexible with large degrees of freedom that are not shared between the models, and iii) the models underestimate the true variability in the data. ",When are Bayesian model probabilities overconfident?,4,"['New paper ""When are Bayesian model probabilities overconfident?"" by Oscar Oelrich, Shutong Ding, @MansMeg, me, and @matvil <LINK> (I had just a minor role, others did awesome work on this) <LINK>', 'We demonstrate overconfidence in two high-profile applications in economics and neuroscience and derive the sampling variance in univariate and multivariate linear regression. https://t.co/jjRBaMcRxf', 'Overconfidence is likely when 1) the compared models give very different approximations of the data-gener. process, 2) the models are very flexible with large degrees of freedom that are not shared between the models, 3) the models underestimate the true variability in the data. https://t.co/jD7zRbluxK', ""In the end we mention possible alternatives for model comparison and combination, but I realize we should have mentioned also that instead of trying to compare or combine bad models it's better to think harder and construct better models.""]",20,03,904
79,208,1278500500841721858,314395154,Tengyu Ma,"DL models tend to struggle with heteroskedastic and imbalanced datasets, where long-tailed labels have varying levels of uncertainty, partly bc it's hard to distinguish mislabeled, ambiguous, and rare examples. We propose a new regularization technique: <LINK> <LINK> The main principle is to regularize more strongly for those data that are rare and noisy. Joint work with @caokd8888, Yining Chen, @lu_junwei, Nikos Arechiga, @adnothing. Conceptually, this can be viewed as a follow-up work of our last year NeurIPS paper on learning imbalanced datasets <LINK>. In this paper, we need to deal with the interaction of the heteroskedasticity and imbalance more carefully. @huyhcmut1997 Thanks for the comments/questions. Yes, the idea can also be used for regression. Actually, the demonstrating examples in the intro are for the regression setting. To use it for regression, one can take equation (6) as the objective and choose $\tau_i=\simga_i^a/q_i^b$ @huyhcmut1997 where a,b &gt; 0 are constants and \sigma_i is the estimated standard deviation of the noise for that example (e.g., a=4/5, b=2/5 would the best choice predicted by the theory, but in fact, we think the exact choice does not matter that much as long as you tune lambda.) @huyhcmut1997 We will add a section on it in the next revision! Thanks! @chupvl @huyhcmut1997 Thanks for the comments. Hopefully my answer above to @huyhcmut also answers your question?",http://arxiv.org/abs/2006.15766,"Real-world large-scale datasets are heteroskedastic and imbalanced -- labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning. ","Heteroskedastic and Imbalanced Deep Learning with Adaptive
  Regularization",7,"[""DL models tend to struggle with heteroskedastic and imbalanced datasets, where long-tailed labels have varying levels of uncertainty, partly bc it's hard to distinguish mislabeled, ambiguous, and rare examples. We propose a new regularization technique: <LINK> <LINK>"", 'The main principle is to regularize more strongly for those data that are rare and noisy. Joint work with @caokd8888, Yining Chen, @lu_junwei, Nikos Arechiga, @adnothing.', 'Conceptually, this can be viewed as a follow-up work of our last year NeurIPS paper on learning imbalanced datasets  https://t.co/IHbopWrwCD. In this paper, we need to deal with the interaction of the heteroskedasticity and imbalance more carefully.', '@huyhcmut1997 Thanks for the comments/questions. Yes, the idea can also be used for regression. Actually, the demonstrating examples in the intro are for the regression setting. To use it for regression, one can take equation (6) as the objective and choose $\\tau_i=\\simga_i^a/q_i^b$', '@huyhcmut1997 where a,b &gt; 0 are constants and \\sigma_i is the estimated standard deviation of the noise for that example (e.g., a=4/5, b=2/5 would the best choice predicted by the theory, but in fact, we think the exact choice does not matter that much as long as you tune lambda.)', '@huyhcmut1997 We will add a section on it in the next revision! Thanks!', '@chupvl @huyhcmut1997 Thanks for the comments. Hopefully my answer above to @huyhcmut also answers your question?']",20,06,1425
80,88,1171326547569106944,1153187867897860096,Nikita Nikolaev,New Paper: I construct Levelt filtration for singularly perturbed linear systems of #ODE (in rank 2 at a regular singular point) maintaining very tight #asymptotic control by upper-triangularising such system in a singular #perturbation families. <LINK> I put this paper out whilst attending the @NCCRSwissMAP General Meeting in Villars-sur-Ollon. Hit submit with the Alps in my view.,https://arxiv.org/abs/1909.04011,"We study singularly perturbed linear systems of rank two of ordinary differential equations of the form $\hbar x\partial_x \psi (x, \hbar) + A (x, \hbar) \psi (x, \hbar) = 0$, with a regular singularity at $x = 0$, and with a fixed asymptotic regularity in the perturbation parameter $\hbar$ of Gevrey type in a fixed sector. We show that such systems can be put into an upper-triangular form by means of holomorphic gauge transformations which are also Gevrey in the perturbation parameter $\hbar$ in the same sector. We use this result to construct a family in $\hbar$ of Levelt filtrations which specialise to the usual Levelt filtration for every fixed nonzero value of $\hbar$; this family of filtrations recovers in the $\hbar \to 0$ limit the eigen-decomposition for the $\hbar$-leading-order of the matrix $A (x, \hbar)$, and also recovers in the $x \to 0$ limit the eigen-decomposition of the residue matrix $A (0, \hbar)$. ","Triangularisation of Singularly Perturbed Logarithmic Differential
  Systems of Rank 2",2,"['New Paper: I construct Levelt filtration for singularly perturbed linear systems of #ODE (in rank 2 at a regular singular point) maintaining very tight #asymptotic control by upper-triangularising such system in a singular #perturbation families.\n\n<LINK>', 'I put this paper out whilst attending the @NCCRSwissMAP General Meeting in Villars-sur-Ollon. Hit submit with the Alps in my view.']",19,09,384
81,87,1202068883001053184,3292006950,zhou Yu,"Our new AAAI paper <LINK> and the code is here <LINK> One dialog context can have multiple dialog responses that can be appropriate for finishing the goal. Existing datasets do not count in all the possible responses. We develop a method to augment the existing datasets to count in all the possible alternative responses. So the model trained will not biased towards one particular policy. We obtained state-of-the-art dialog generation results on MultiWOZ. @tianchezhao Thanks! Feel free to use our code Forget to mention, we also publish a better and cleaner version of MultiWoz. We corrected delexicalization errors in multi-domain tasks.Unified slots such as phone number from different domains. <LINK>",https://arxiv.org/abs/1911.10484,"Conversations have an intrinsic one-to-many property, which means that multiple responses can be appropriate for the same dialog context. In task-oriented dialogs, this property leads to different valid dialog policies towards task completion. However, none of the existing task-oriented dialog generation approaches takes this property into account. We propose a Multi-Action Data Augmentation (MADA) framework to utilize the one-to-many property to generate diverse appropriate dialog responses. Specifically, we first use dialog states to summarize the dialog history, and then discover all possible mappings from every dialog state to its different valid system actions. During dialog system training, we enable the current dialog state to map to all valid system actions discovered in the previous process to create additional state-action pairs. By incorporating these additional pairs, the dialog policy learns a balanced action distribution, which further guides the dialog model to generate diverse responses. Experimental results show that the proposed framework consistently improves dialog policy diversity, and results in improved response diversity and appropriateness. Our model obtains state-of-the-art results on MultiWOZ. ","Task-Oriented Dialog Systems that Consider Multiple Appropriate
  Responses under the Same Context",4,"['Our new AAAI paper <LINK> \nand the code is here <LINK>\nOne dialog context can have multiple dialog responses that can be appropriate for finishing the goal. Existing datasets do not count in all the possible responses.', 'We develop a method to augment the existing datasets to count in all the possible alternative responses. So the model trained will not biased towards one particular policy. We obtained state-of-the-art dialog generation results on MultiWOZ.', '@tianchezhao Thanks! Feel free to use our code', 'Forget to mention, we also publish a better and cleaner version of MultiWoz. We corrected delexicalization errors in multi-domain tasks.Unified slots such as phone number from different domains. https://t.co/vaLA3JmwSi']",19,11,707
82,76,1006336750300459008,759118366468481024,Decker French,"New paper today! <LINK> A thread: We used stellar population modeling to study the recent starbursts in post-starburst galaxies. Now that the starbursts have ended, we can characterize how much stellar mass was produced, whether there was one burst or two, and the duration of the starburst. But, the most interesting thing we can determine is the time elapsed since the starburst ended (the “post-starburst age”). Comparing the ages of post-starburst galaxies lets us put them on a timeline after the starburst. We used the ages of post-starburst galaxies with molecular gas measurements from my 2015 sample, @astrokatey’s sample, and @kerowlands’s sample. <LINK> We see a significant decline in the molecular gas fraction after the starburst. This is cool for two reasons: (1) the decline rate is too fast to be accounted for by star-formation, even assuming a heavy impact from stellar feedback. It looks like AGN feedback might be acting during this phase. Others have seen AGN driving outflows and winds before, but this is the first time we’ve seen the molecular gas being truly *depleted* from the galaxy as a whole! (2) The decline in gas fraction solves the problem of how post-starburst galaxies could look like early types in a few Gyr, which they otherwise will. Given the current gas depletion rates, the post-starbursts could resemble early type galaxies in their gas properties in 1-2 Gyr. Aside from the ages, we also find some neat trends in the starburst properties. Lower stellar mass post-starbursts have stronger starbursts and are more likely to have had multiple recent bursts. This might be because stellar feedback is stronger in lower mass galaxies, or that lower mass galaxies are less likely to have a bulge to stabilize the gas during a merger first passage. If you want to read more, the main results are in section 4 (the starburst properties) and section 5 (the molecular gas trends).",https://arxiv.org/abs/1806.03301,"Detailed modeling of the recent star formation histories (SFHs) of post-starburst (or ""E+A"") galaxies is impeded by the degeneracy between the time elapsed since the starburst ended (post-burst age), the fraction of stellar mass produced in the burst (burst strength), and the burst duration. To resolve this issue, we combine GALEX ultraviolet photometry, SDSS photometry and spectra, and new stellar population synthesis models to fit the SFHs of 532 post-starburst galaxies. In addition to an old stellar population and a recent starburst, 48% of the galaxies are best fit with a second recent burst. Lower stellar mass galaxies (log M$_\star$/M$_\odot<10.5$) are more likely to experience two recent bursts, and the fraction of their young stellar mass is more strongly anti-correlated with their total stellar mass. Applying our methodology to other, younger post-starburst samples, we identify likely progenitors to our sample and examine the evolutionary trends of molecular gas and dust content with post-burst age. We discover a significant (4$\sigma$) decline, with a 117-230 Myr characteristic depletion time, in the molecular gas to stellar mass fraction with the post-burst age. The implied rapid gas depletion rate of 2-150 M$_\odot$yr$^{-1}$ cannot be due to current star formation, given the upper limits on the current SFRs in these post-starbursts. Nor are stellar winds or SNe feedback likely to explain this decline. Instead, the decline points to the expulsion or destruction of molecular gas in outflows, a possible smoking gun for AGN feedback. ","Clocking the Evolution of Post-Starburst Galaxies: Methods and First
  Results",11,"['New paper today! <LINK> A thread:', 'We used stellar population modeling to study the recent starbursts in post-starburst galaxies. Now that the starbursts have ended, we can characterize how much stellar mass was produced, whether there was one burst or two, and the duration of the starburst.', 'But, the most interesting thing we can determine is the time elapsed since the starburst ended (the “post-starburst age”).', 'Comparing the ages of post-starburst galaxies lets us put them on a timeline after the starburst. We used the ages of post-starburst galaxies with molecular gas measurements from my 2015 sample, @astrokatey’s sample, and @kerowlands’s sample. https://t.co/kY4TXtrgI2', 'We see a significant decline in the molecular gas fraction after the starburst. This is cool for two reasons:', '(1) the decline rate is too fast to be accounted for by star-formation, even assuming a heavy impact from stellar feedback. It looks like AGN feedback might be acting during this phase.', 'Others have seen AGN driving outflows and winds before, but this is the first time we’ve seen the molecular gas being truly *depleted* from the galaxy as a whole!', '(2) The decline in gas fraction solves the problem of how post-starburst galaxies could look like early types in a few Gyr, which they otherwise will. Given the current gas depletion rates, the post-starbursts could resemble early type galaxies in their gas properties in 1-2 Gyr.', 'Aside from the ages, we also find some neat trends in the starburst properties. Lower stellar mass post-starbursts have stronger starbursts and are more likely to have had multiple recent bursts.', 'This might be because stellar feedback is stronger in lower mass galaxies, or that lower mass galaxies are less likely to have a bulge to stabilize the gas during a merger first passage.', 'If you want to read more, the main results are in section 4 (the starburst properties) and section 5 (the molecular gas trends).']",18,06,1916
83,192,1404473337670213645,32513057,Kristian Lum,"Do you use the COMPAS dataset to demonstrate fair ML methods? Do you want to do so responsibly? Then, ""It's COMPASlicated,"" my new paper w @TheMichelleBao @angelamczhou @samzottola @BrianBrubach @DrSLDesmarais @Aaron_Horowitz @geomblog is for you! 🧵 1/n <LINK> In this paper, we discuss properties inherent to CJ data that make strong claims about fairness in this context, well.... COMPASlicated.  Tl;dr You may be predicting not-what-you-want based on features/a population that are themselves the result of ""biased"" decisions. 2/n Algorithmic RAIs are not an automated, single-decision system, but rather 1 point of discretion in a complex multi-decisionmaker & multi-institution system.  As such, even if your model performs ""fairly"" in isolation, this may not translate to fair outcomes in practice. 3/n It's not possible to decontextualize risk assessment data from the politics of the criminal justice system. For example, accepting the objective of predicting future crime for use in this context tacitly endorses incapacitation over rehabilitation or retribution. 4/n ML researchers undermine efforts for CJ reform (e.g. eliminating bail) through their misunderstanding of the issues-- for example, being ""granted bail"" is often posed as the positive outcome. The many people currently detained on (even small amounts) of bail would disagree. 5/n Speaking of reform, ""fair ML"" often implicitly endorses RAIs as an avenue for criminal justice reform. The notion of structural reform, for policing, incarceration, or CJ in general, is itself value-laden and presents existing structures as worthy of reform. 6/n Many of these are issues that folks in other fields have already grappled with and have developed written guidelines and standards for implementing best practices when engaging in criminal justice work. So far, these have largely been ignored by the CS and fair ML community. 7/n Specifically, there are guidelines for language use for describing justice-involved people. Terms like ""prisoner,"" ""felon,"" ""inmate,"" etc. are dehumanizing & terms that acknowledge the person's humanity, such as ""person who has been convicted of a felony"" are now recommended 8/n I just gave you the rundown of the issues nearest and dearest to my heart. Others perspectives, including those of advocates and psychology/CJ researchers, are also included.  I hope this offers some useful context for ML researchers who use the COMPAS dataset!",https://arxiv.org/abs/2106.05498,"Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS dataset, are commonly used in algorithmic fairness papers due to benchmarking practices of comparing algorithms on datasets used in prior work. In many cases, this data is used as a benchmark to demonstrate good performance without accounting for the complexities of criminal justice (CJ) processes. However, we show that pretrial RAI datasets can contain numerous measurement biases and errors, and due to disparities in discretion and deployment, algorithmic fairness applied to RAI datasets is limited in making claims about real-world outcomes. These reasons make the datasets a poor fit for benchmarking under assumptions of ground truth and real-world impact. Furthermore, conventional practices of simply replicating previous data experiments may implicitly inherit or edify normative positions without explicitly interrogating value-laden assumptions. Without context of how interdisciplinary fields have engaged in CJ research and context of how RAIs operate upstream and downstream, algorithmic fairness practices are misaligned for meaningful contribution in the context of CJ, and would benefit from transparent engagement with normative considerations and values related to fairness, justice, and equality. These factors prompt questions about whether benchmarks for intrinsically socio-technical systems like the CJ system can exist in a beneficial and ethical way. ","It's COMPASlicated: The Messy Relationship between RAI Datasets and
  Algorithmic Fairness Benchmarks",9,"['Do you use the COMPAS dataset to demonstrate fair ML methods? Do you want to do so responsibly?\n\nThen, ""It\'s COMPASlicated,"" my new paper w @TheMichelleBao @angelamczhou @samzottola @BrianBrubach @DrSLDesmarais @Aaron_Horowitz @geomblog is for you! 🧵 1/n\n\n<LINK>', 'In this paper, we discuss properties inherent to CJ data that make strong claims about  fairness in this context, well.... COMPASlicated. \n\nTl;dr You may be predicting not-what-you-want based on features/a population that are themselves the result of ""biased"" decisions. 2/n', 'Algorithmic RAIs are not an automated, single-decision system, but rather 1 point of discretion in a complex multi-decisionmaker &amp; multi-institution system. \n\nAs such, even if your model performs ""fairly"" in isolation, this may not translate to fair outcomes in practice.  3/n', ""It's not possible to decontextualize risk assessment data from the politics of the criminal justice system. For example, accepting the objective of predicting future crime for use in this context tacitly endorses incapacitation over rehabilitation or retribution. 4/n"", 'ML researchers undermine efforts for CJ reform (e.g. eliminating bail) through their  misunderstanding of the issues-- for example, being ""granted bail"" is often posed as the positive outcome. The many people currently detained on (even small amounts) of bail would disagree. 5/n', 'Speaking of reform, ""fair ML"" often implicitly endorses RAIs as an avenue for criminal justice reform.  The notion of structural reform, for policing, incarceration, or CJ in general, is itself value-laden and presents existing structures as worthy of reform.  6/n', 'Many of these are issues that folks in other fields have already grappled with and have developed written guidelines and standards for implementing best practices when engaging in criminal justice work. So far, these have largely been ignored by the CS and fair ML community. 7/n', 'Specifically, there are guidelines for language use for describing justice-involved people. Terms like ""prisoner,"" ""felon,"" ""inmate,"" etc. are dehumanizing &amp; terms that acknowledge the person\'s humanity, such as ""person who has been convicted of a felony"" are now recommended 8/n', 'I just gave you the rundown of the issues nearest and dearest to my heart. Others perspectives, including those of  advocates and psychology/CJ researchers, are also included. \n\nI hope this offers some useful context for ML researchers who use the COMPAS dataset!']",21,06,2440
84,59,1321139111063814144,1074633382452051969,Kimin,"Excited to share our #NeurIPS2020 paper that introduces a new model-based RL method to learn the multi-modal transition distribution in an unsupervised manner 🎓<LINK> 💻<LINK> w/@younggyoseo @clavera_i @KurutachThanard @jinwoos0417 @pabbeel 1/N Learning an ensemble of dynamics models is a common practice in model-based RL. However, when the transition dynamics of environments change, dynamics models fail to provide accurate predictions because the target transition dynamics follow a multi-modal distribution. 2/N <LINK> To tackle this problem, we propose a trajectory-wise multiple choice learning (T-MCL) that is a novel combination of MCL and model-based RL. The main idea is to force each dynamics model to specialize in different environments by only updating the most accurate dynamics model 3/N <LINK> To utilize the specialized dynamics models more effectively, we propose adaptive planning that selects actions using the most accurate model over a recent experience, which can be interpreted as finding the nearest cluster to the current environment.  4/N <LINK> Can T-MCL make dynamics models be specialized for a certain subset of training environments with similar dynamics? Yes. T-MCL can separate trajectories from different environments in an unsupervised manner (i.e., without any additional information about environments). 5/N <LINK> We also found that learning specialized dynamics models can improve the generalization performance on unseen (yet related) environments with different transition dynamics. We show T-MCL outperforms model-based RL methods (CaDM, GrBAL) and model-free meta-RL method (PEARL). 6/N <LINK> More qualitative analysis: We first train T-MCL on CartPole environments with different masses and visualize the agents' behavior by manually assigning specialized dynamics models. We show agents act as if they are light-weight or heavy-weight according to assigned models. 7/N <LINK> For more details, please check out our paper: <LINK> open-source code: <LINK> webpage: <LINK>  Thank you for your attention!  8/N Special thanks to co-first author @younggyoseo and collaborators @clavera_i,@KurutachThanard, @jinwoos0417, @pabbeel 🙏",https://arxiv.org/abs/2010.13303,"Model-based reinforcement learning (RL) has shown great potential in various control tasks in terms of both sample-efficiency and final performance. However, learning a generalizable dynamics model robust to changes in dynamics remains a challenge since the target transition dynamics follow a multi-modal distribution. In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning, that learns a multi-headed dynamics model for dynamics generalization. The main idea is updating the most accurate prediction head to specialize each head in certain environments with similar dynamics, i.e., clustering environments. Moreover, we incorporate context learning, which encodes dynamics-specific information from past experiences into the context latent vector, enabling the model to perform online adaptation to unseen environments. Finally, to utilize the specialized prediction heads more effectively, we propose an adaptive planning method, which selects the most accurate prediction head over a recent experience. Our method exhibits superior zero-shot generalization performance across a variety of control tasks, compared to state-of-the-art RL methods. Source code and videos are available at this https URL ","Trajectory-wise Multiple Choice Learning for Dynamics Generalization in
  Reinforcement Learning",9,"['Excited to share our #NeurIPS2020 paper that introduces a new model-based RL method to learn the multi-modal transition distribution in an unsupervised manner\n🎓<LINK>\n💻<LINK>\nw/@younggyoseo @clavera_i @KurutachThanard @jinwoos0417 @pabbeel \n1/N', 'Learning an ensemble of dynamics models is a common practice in model-based RL. However, when the transition dynamics of environments change, dynamics models fail to provide accurate predictions because the target transition dynamics follow a multi-modal distribution.\n\n2/N https://t.co/yiNmVqTNUt', 'To tackle this problem, we propose a trajectory-wise multiple choice learning (T-MCL) that is a novel combination of MCL and model-based RL. The main idea is to force each dynamics model to specialize in different environments by only updating the most accurate dynamics model\n3/N https://t.co/5cJlGJBFqt', 'To utilize the specialized dynamics models more effectively, we propose adaptive planning that selects actions using the most accurate model over a recent experience, which can be interpreted as finding the nearest cluster to the current environment. \n\n4/N https://t.co/Pym57oaVVz', 'Can T-MCL make dynamics models be specialized for a certain subset of training environments with similar dynamics?\n\nYes. T-MCL can separate trajectories from different environments in an unsupervised manner (i.e., without any additional information about environments).\n\n5/N https://t.co/Cl4eGUvIGA', 'We also found that learning specialized dynamics models can improve the generalization performance on unseen (yet related) environments with different transition dynamics. We show T-MCL outperforms model-based RL methods (CaDM, GrBAL) and model-free meta-RL method (PEARL).\n6/N https://t.co/nhGFENVAuL', ""More qualitative analysis: We first train T-MCL on CartPole environments with different masses and visualize the agents' behavior by manually assigning specialized dynamics models. We show agents act as if they are light-weight or heavy-weight according to assigned models.\n\n7/N https://t.co/jY1T2oFzPl"", 'For more details, please check out \nour paper: https://t.co/nhKgJJJ9JW \nopen-source code: https://t.co/fRg25TSOdR \nwebpage: https://t.co/0u8oGOPPpj \n\nThank you for your attention! \n\n8/N', 'Special thanks to co-first author @younggyoseo and collaborators @clavera_i,@KurutachThanard, @jinwoos0417, @pabbeel 🙏']",20,10,2173
85,112,1371333166577844226,2377407248,Daniel Whiteson,"Who says you can’t use machine learning in theoretical physics? New paper: <LINK> Efficient sampling of constrained high-dimensional theoretical spaces with machine learning.  Led by Jake Hollingsworth, with @FlipTanedo and M. Ratz. @FlipTanedo Simple question: what fraction of supersymmetry theories have a Higgs that matches the one in our Universe?  WE DON’T KNOW because the space is too large to explore.  It has 100+ parameters! We can’t generate ~100^100 points. Theorists try to reduce the dimensions by making a bunch of assumptions.  But what’s missed? Are there islands that nobody has seen in that vast space?  How can we explore it? Machine learning is GREAT at exploring high-dim spaces. We used ML to learn where the Higgs-compatible spots are, letting us sample the space orders of magnitude more quickly. <LINK> It's much faster AND not biased. <LINK> We hope to use this tool to search for unexplored islands in theory space. Stay tuned!",https://arxiv.org/abs/2103.06957,"Models of physics beyond the Standard Model often contain a large number of parameters. These form a high-dimensional space that is computationally intractable to fully explore. Experimental constraints project onto a subspace of viable parameters, but mapping these constraints to the underlying parameters is also typically intractable. Instead, physicists often resort to scanning small subsets of the full parameter space and testing for experimental consistency. We propose an alternative approach that uses generative models to significantly improve the computational efficiency of sampling high-dimensional parameter spaces. To demonstrate this, we sample the constrained and phenomenological Minimal Supersymmetric Standard Models subject to the requirement that the sampled points are consistent with the measured Higgs boson mass. Our method achieves orders of magnitude improvements in sampling efficiency compared to a brute force search. ","Efficient sampling of constrained high-dimensional theoretical spaces
  with machine learning",6,"['Who says you can’t use machine learning in theoretical physics?\n\nNew paper:\n\n<LINK>\n\nEfficient sampling of constrained high-dimensional theoretical spaces with machine learning. \n\nLed by Jake Hollingsworth, with @FlipTanedo and M. Ratz.', '@FlipTanedo Simple question: what fraction of supersymmetry theories have a Higgs that matches the one in our Universe?  \n\nWE DON’T KNOW because the space is too large to explore. \n\nIt has 100+ parameters! We can’t generate ~100^100 points.', 'Theorists try to reduce the dimensions by making a bunch of assumptions.  \n\nBut what’s missed?  Are there islands that nobody has seen in that vast space? \n\nHow can we explore it?', 'Machine learning is GREAT at exploring high-dim spaces. We used ML to learn where the Higgs-compatible spots are, letting us sample the space orders of magnitude more quickly. https://t.co/0hDbWRSLIj', ""It's much faster AND not biased. https://t.co/jfV8WuDSaQ"", 'We hope to use this tool to search for unexplored islands in theory space. Stay tuned!']",21,03,956
86,70,1406787680168382470,1093387119148462081,Daniel Green,"New paper today with Tim Cohen, Akhil Premkumar and Alec Ridgway: ""Stochastic Inflation at NNLO"" <LINK> If you have ever wondered what Stochastic Inflation is and what is has to do with QFT in de Sitter space, then this paper is for you The old idea (due to Starobinsky) is intuitive: de Sitter gives you quantum fluctuations (noise) and a classical potential makes them role (drift) Still I found it confusing: (1) Why does SI only include Gaussian noise? (2) How would I know to use SI if I started from QFT in dS? What we show is that SI is the equation for RG flow: it is describes the mixing of operators. The original set of equation are just the leading order anomalous dimensions / beta functions. If you have ever calculated anomalous dimensions you will recognize some of our diagrams <LINK> We find two kinds of corrections: corrections to the effective potential (higher powers of the field) and, for the first time, the appearance of non-Gaussian noise (higher derivatives).  This is the full equation for SI in \lambda phi^4 at next-to-next-to leading order (NNLO) <LINK> My take-away: we can now (in principle) understand light fields in de Sitter at the same level we understand QFT in four dimensions. Everything follows from dimensional analysis and we know what diagrams to calculate to get the precise numbers. Next up: loops in inflation @JazayeriSadra Yes, I think so. I hope to have a paper out soon on that case. Most diagrams involving the metric aren’t actually important since they are derivatively coupled. This isn’t obvious in most ways of calculating, but is manifest in SdSET @EKuflik @geller_mic For conventional models of slow roll inflation, the corrections will be small (except perhaps on the tails of the distribution) and are unlikely to change the qualitative picture. In models with derivative interactions / primordial non-Gaussianity, the corrections could matter @REasther I’m not sure we have anything new to add for this question, other than to say that we understand the limitations of the equations better. Eventually the uphill evolution balances the potential and you get equilibrium, like running to a non trivial fixed point in RG",https://arxiv.org/abs/2106.09728,"Stochastic Inflation is an important framework for understanding the physics of de Sitter space and the phenomenology of inflation. In the leading approximation, this approach results in a Fokker-Planck equation that calculates the probability distribution for a light scalar field as a function of time. Despite its successes, the quantum field theoretic origins and the range of validity for this equation have remained elusive, and establishing a formalism to systematically incorporate higher order effects has been an area of active study. In this paper, we calculate the next-to-next-to-leading order (NNLO) corrections to Stochastic Inflation using Soft de Sitter Effective Theory (SdSET). In this effective description, Stochastic Inflation manifests as the renormalization group evolution of composite operators. The leading impact of non-Gaussian quantum fluctuations appears at NNLO, which is presented here for the first time; we derive the coefficient of this term from a two-loop anomalous dimension calculation within SdSET. We solve the resulting equation to determine the NNLO equilibrium distribution and the low-lying relaxation eigenvalues. In the process, we must match the UV theory onto SdSET at one-loop order, which provides a non-trivial confirmation that the separation into Wilson-coefficient corrections and contributions to initial conditions persists beyond tree level. Furthermore, these results illustrate how the naive factorization of time and momentum integrals in SdSET no longer holds in the presence of logarithmic divergences. It is these effects that ultimately give rise to the renormalization group flow that yields Stochastic Inflation. ",Stochastic Inflation at NNLO,8,"['New paper today with Tim Cohen, Akhil Premkumar and Alec Ridgway:  ""Stochastic Inflation at NNLO""\n\n<LINK>\n\nIf you have ever wondered what Stochastic Inflation is and what is has to do with QFT in de Sitter space, then this paper is for you', 'The old idea (due to Starobinsky) is intuitive: de Sitter gives you quantum fluctuations (noise) and a classical potential makes them role (drift)\n\nStill I found it confusing:\n(1) Why does SI only include Gaussian noise?\n(2) How would I know to use SI if I started from QFT in dS?', 'What we show is that SI is the equation for RG flow: it is describes the mixing of operators.  The original set of equation are just the leading order anomalous dimensions / beta functions.  If you have ever calculated anomalous dimensions you will recognize some of our diagrams https://t.co/ObkF9uV6ew', 'We find two kinds of corrections: corrections to the effective potential (higher powers of the field) and, for the first time, the appearance of non-Gaussian noise (higher derivatives).  \n\nThis is the full equation for SI in \\lambda phi^4 at next-to-next-to leading order (NNLO) https://t.co/MrCfvugNn5', 'My take-away: we can now (in principle) understand light fields in de Sitter at the same level we understand QFT in four dimensions.  Everything follows from dimensional analysis and we know what diagrams to calculate to get the precise numbers.\n\nNext up: loops in inflation', '@JazayeriSadra Yes, I think so.  I hope to have a paper out soon on that case.  Most diagrams involving the metric aren’t actually important since they are derivatively coupled.  This isn’t obvious in most ways of calculating, but is manifest in SdSET', '@EKuflik @geller_mic For conventional models of slow roll inflation, the corrections will be small (except perhaps on the tails of the distribution) and are unlikely to change the qualitative picture.  In models with derivative interactions / primordial non-Gaussianity, the corrections could matter', '@REasther I’m not sure we have anything new to add for this question, other than to say that we understand the limitations of the equations better.  Eventually the uphill evolution balances the potential and you get equilibrium, like running to a non trivial fixed point in RG']",21,06,2182
87,0,1161661512156684289,1158385581476515840,Allyson Ettinger,New paper up! “What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models” <LINK>. Tests that have elicited informative patterns in the brain prove quite useful for clarifying strengths and limitations of BERT pre-training.,https://arxiv.org/abs/1907.13528,"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about the information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation. ","What BERT is not: Lessons from a new suite of psycholinguistic
  diagnostics for language models",1,['New paper up! “What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models” <LINK>. Tests that have elicited informative patterns in the brain prove quite useful for clarifying strengths and limitations of BERT pre-training.'],19,07,259
88,146,1499432071902674949,969035645938294785,Erik Thiede (he /him),"All you free energy aficionados looking for your morning dose of math, here it is! New paper w. Sherry Li on understanding the error in MBAR dropped on arXiv at <LINK>. Some takeaways in the comments... - You really can't ignore the effects of the sampling dynamics in the error. Dynamical effects can contribute just as much -- if not more -- to the error than static properties. - Your intuition that you should be discarding those high-free energy umbrella sampling windows is probably right: you can actually *decrease* the variance in other parts of your PMF by throwing out hard-to-sample windows. - In fact, sometimes surprisingly few states often contribute to most of the MBAR error. We might be able to tune our free energy calculations much better than we have been, now that we have comprehensive tools for understanding the error.",https://arxiv.org/abs/2203.01227,"Multiple sampling strategies commonly used in molecular dynamics, such as umbrella sampling and alchemical free energy methods, involve sampling from multiple thermodynamic states. Commonly, the data are then recombined to construct estimates of free energies and ensemble averages using the Multistate Bennett Acceptance Ratio (MBAR) formalism. However, the error of the MBAR estimator is not well-understood: previous error analysis of MBAR assumed independent samples and did not permit attributing contributions to the total error to individual thermodynamic states. In this work, we derive a novel central limit theorem for MBAR estimates. This central limit theorem yields an error estimator which can be decomposed into contributions from the individual Markov chains used to sample the states. We demonstrate the error estimator for an umbrella sampling calculation of the alanine dipeptide in two dimensions and an alchemical calculation of the hydration free energy of methane. In both cases, the states' individual contributions to the error provide insight into the sources of error of the simulations. Our numerical results demonstrate that the time required for the Markov chain to decorrelate in individual thermodynamic states contributes considerably to the total MBAR error. Moreover, they indicate that it may be possible to use the contributions to tune the sampling and improve the accuracy of MBAR calculations. ",Understanding the Sources of Error in MBAR through Asymptotic Analysis,4,"['All you free energy aficionados looking for your morning dose of math, here it is!  New paper w. Sherry Li on understanding the error in MBAR dropped on arXiv at <LINK>.  Some takeaways in the comments...', ""- You really can't ignore the effects of the sampling dynamics in the error.  Dynamical effects  can contribute just as much -- if not more -- to the error than static properties."", '- Your intuition that you should be discarding those high-free energy umbrella sampling windows is probably right: you can actually *decrease* the variance in other parts of your PMF by throwing out hard-to-sample windows.', '- In fact, sometimes surprisingly few states often contribute to most of the MBAR error.  We might be able to tune our free energy calculations much better than we have been, now that we have comprehensive tools for understanding the error.']",22,03,843
89,155,1459888302438367241,12309242,Onur Mutlu,"""Uncovering In-DRAM #RowHammer Protection Mechanisms: A New Methodology, Custom RowHammer Patterns, and Implications"", MICRO'21 talk, Hasan Hassan.  Tomorrow 6:30pm Zurich time.  Youtube: <LINK> Paper: <LINK> @SAFARI_ETH_CMU @ETH_en @CSatETH This work introduces a rigorous methodology for uncovering the operational principles & details of RowHammer protection mechanisms employed in modern DDR4 DRAM chips. We show that one can use this methodology to circumvent existing protections & cause many more RowHammer bitflips Our methodology, U-TRR (Uncovering TRR) can help enable fundamentally-secure and robust solutions to RowHammer.  Collaborative research with @kavehrazavi and @vvdveen. Full paper: <LINK>  Full Talk Slides (PDF): <LINK>",https://arxiv.org/abs/2110.10603,"The RowHammer vulnerability in DRAM is a critical threat to system security. To protect against RowHammer, vendors commit to security-through-obscurity: modern DRAM chips rely on undocumented, proprietary, on-die mitigations, commonly known as Target Row Refresh (TRR). At a high level, TRR detects and refreshes potential RowHammer-victim rows, but its exact implementations are not openly disclosed. Security guarantees of TRR mechanisms cannot be easily studied due to their proprietary nature. To assess the security guarantees of recent DRAM chips, we present Uncovering TRR (U-TRR), an experimental methodology to analyze in-DRAM TRR implementations. U-TRR is based on the new observation that data retention failures in DRAM enable a side channel that leaks information on how TRR refreshes potential victim rows. U-TRR allows us to (i) understand how logical DRAM rows are laid out physically in silicon; (ii) study undocumented on-die TRR mechanisms; and (iii) combine (i) and (ii) to evaluate the RowHammer security guarantees of modern DRAM chips. We show how U-TRR allows us to craft RowHammer access patterns that successfully circumvent the TRR mechanisms employed in 45 DRAM modules of the three major DRAM vendors. We find that the DRAM modules we analyze are vulnerable to RowHammer, having bit flips in up to 99.9% of all DRAM rows. ","Uncovering In-DRAM RowHammer Protection Mechanisms: A New Methodology,
  Custom RowHammer Patterns, and Implications",3,"['""Uncovering In-DRAM #RowHammer Protection Mechanisms: A New Methodology, Custom RowHammer Patterns, and Implications"", MICRO\'21 talk, Hasan Hassan. \n\nTomorrow 6:30pm Zurich time. \n\nYoutube: <LINK>\n\nPaper: <LINK>\n\n@SAFARI_ETH_CMU @ETH_en @CSatETH', 'This work introduces a rigorous methodology for uncovering the operational principles &amp; details of RowHammer protection mechanisms employed in modern DDR4 DRAM chips. We show that one can use this methodology to circumvent existing protections &amp; cause many more RowHammer bitflips', 'Our methodology, U-TRR (Uncovering TRR) can help enable fundamentally-secure and robust solutions to RowHammer. \n\nCollaborative research with @kavehrazavi and @vvdveen.\n\nFull paper: https://t.co/94xVVr8ZLM \n\nFull Talk Slides (PDF): https://t.co/ImY6SPPF7r']",21,10,741
90,153,1444930365748129795,2778729792,Saquib Sarfraz,Checkout our latest work accepted at #NeurIPS2021 on leveraging model uncertainty for adapting object detectors to new domains. Practical use in #AutonomousDriving. A very fruitful collaboration with #ITU #MBZUAI and #DaimlerTSS paper: <LINK> <LINK> <LINK>,https://arxiv.org/abs/2110.00249,"We study adapting trained object detectors to unseen domains manifesting significant variations of object appearance, viewpoints and backgrounds. Most current methods align domains by either using image or instance-level feature alignment in an adversarial fashion. This often suffers due to the presence of unwanted background and as such lacks class-specific alignment. A common remedy to promote class-level alignment is to use high confidence predictions on the unlabelled domain as pseudo labels. These high confidence predictions are often fallacious since the model is poorly calibrated under domain shift. In this paper, we propose to leverage model predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. Specifically, we measure predictive uncertainty on class assignments and the bounding box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-supervision, whereas the ones with higher uncertainty are used to generate tiles for an adversarial feature alignment stage. This synergy between tiling around the uncertain object regions and generating pseudo-labels from highly certain object regions allows us to capture both the image and instance level context during the model adaptation stage. We perform extensive experiments covering various domain shift scenarios. Our approach improves upon existing state-of-the-art methods with visible margins. ","Synergizing between Self-Training and Adversarial Learning for Domain
  Adaptive Object Detection",1,['Checkout our latest work accepted at #NeurIPS2021 on leveraging model uncertainty for adapting object detectors to new domains. Practical use in #AutonomousDriving. A very fruitful collaboration with #ITU #MBZUAI and #DaimlerTSS\npaper: <LINK> <LINK> <LINK>'],21,10,256
91,50,1430124498561404938,64710103,Lasse Rempe (he/him),"New paper today with @davidmartipete and James Waterman. We show that Fatou components of a transcendental entire function can form ""Lakes of Wada"". In the context of rational functions, this was asked by Fatou in 1920. #Maths <LINK> ""Lakes of Wada"" refer to a situation where more than two domains in the plane have the same boundary - this is possible, although hard to imagine! This is a picture of a ""Lakes of Wada"" continuum - the white, black, light grey and dark grey regions all have the same boundary. (Picture by James Waterman) <LINK>",https://arxiv.org/abs/2108.10256,"We develop a general technique for realising full closed subsets of the complex plane as wandering sets of entire functions. Using this construction, we solve a number of open problems. (1) We construct a counterexample to Eremenko's conjecture, a central problem in transcendental dynamics that asks whether every connected component of the set of escaping points of a transcendental entire function is unbounded. (2) We prove that there is a transcendental entire function for which infinitely many Fatou components share the same boundary. This resolves the long-standing problem whether ""Lakes of Wada continua"" can arise in complex dynamics, and answers the analogue of a question of Fatou from 1920 concerning Fatou components of rational functions. (3) We answer a question of Rippon concerning the existence of non-escaping points on the boundary of a bounded escaping wandering domain, that is, a wandering Fatou component contained in the escaping set. In fact we show that the set ofsuch points can have positive Lebesgue measure. (4) We give the first example of an entire function having a simply connected Fatou component whose closure has a disconnected complement, answering a question of Boc Thaler. In view of (3), we introduce the concept of ""maverick points"": points on the boundary of a wandering domain whose accumulation behaviour differs from that of internal points. We prove that the set of such points has harmonic measure zero, but that both escaping and oscillating wandering domains can contain large sets of maverick points. ","Eremenko's conjecture, Wandering Lakes of Wada, and Maverick Points",3,"['New paper today with @davidmartipete and James Waterman. We show that Fatou components of a transcendental entire function can form ""Lakes of Wada"". In the context of rational functions, this was asked by Fatou in 1920. #Maths <LINK>', '""Lakes of Wada"" refer to a situation where more than two domains in the plane have the same boundary - this is possible, although hard to imagine!', 'This is a picture of a ""Lakes of Wada"" continuum - the white, black, light grey and dark grey regions all have the same boundary. (Picture by James Waterman) https://t.co/1ZPRYslQ9f']",21,08,545
92,201,1470763114572439553,1315232054271934464,Giovanni Russo,"How can we integrate data-driven and model-based control? Find it out with the latest work by @FrancescoDeLel4 @coraggio_marco @mircomusolesi @mdiberna and myself: <LINK> In the paper we are proposing an architecture where a feedback controller derived on an approximate environment model tutors a learning algorithm. Results point out that, by doing so, we can improve data efficiency of the learning process!",https://arxiv.org/abs/2112.06018,"We present an architecture where a feedback controller derived on an approximate model of the environment assists the learning process to enhance its data efficiency. This architecture, which we term as Control-Tutored Q-learning (CTQL), is presented in two alternative flavours. The former is based on defining the reward function so that a Boolean condition can be used to determine when the control tutor policy is adopted, while the latter, termed as probabilistic CTQL (pCTQL), is instead based on executing calls to the tutor with a certain probability during learning. Both approaches are validated, and thoroughly benchmarked against Q-Learning, by considering the stabilization of an inverted pendulum as defined in OpenAI Gym as a representative problem. ","Control-Tutored Reinforcement Learning: Towards the Integration of
  Data-Driven and Model-Based Control",2,"['How can we integrate data-driven and model-based control?  Find it out with the latest work by @FrancescoDeLel4 @coraggio_marco @mircomusolesi @mdiberna  and myself:\n\n<LINK>', 'In the paper we are proposing an architecture where a feedback controller derived on an approximate environment model tutors a learning algorithm.\n\nResults point out that, by doing so, we can improve data efficiency of the learning process!']",21,12,410
93,41,1073157291044757509,3313806489,Tim Roberts,"***KLAXON*** We did a new catalogue paper, including ~ 400 ULXs. Find it on <LINK> If nothing else, it would make an excellent gift for your loved ones this Christmas(*). (* Disclaimer: it may not make an excellent gift for your loved ones this Christmas.)",http://arxiv.org/abs/1812.04684,"We have created a new, clean catalogue of extragalactic non-nuclear X-ray sources by correlating the 3XMM-DR4 data release of the XMM-Newton Serendipitous Source Catalogue with the Third Reference Catalogue of Bright Galaxies and the Catalogue of Neighbouring Galaxies, using an improved version of the method presented in Walton et al. (2011). Our catalogue contains 1,314 sources, of which 384 are candidate ultraluminous X-ray sources (ULXs). The resulting catalogue improves upon previous catalogues in its handling of spurious detections by taking into account XMM-Newton quality flags. We estimate the contamination of ULXs by background sources to be 24 per cent. We define a 'complete' subsample as those ULXs in galaxies for which the sensitivity limit is below $10^{39}$ erg/s and use it to examine the hardness ratio properties between ULX and non-ULX sources, and ULXs in different classes of host galaxy. We find that ULXs have a similar hardness ratio distribution to lower-luminosity sources, consistent with previous studies. We also find that ULXs in spiral and elliptical host galaxies have similar distributions to each other independent of host galaxy morphology, however our results do support previous indications that the population of ULXs is more luminous in star-forming host galaxies than in non-star-forming galaxies. Our catalogue contains further interesting subpopulations for future study, including Eddington Threshold sources and highly variable ULXs. We also examine the highest-luminosity (L$_X$ > $5 \times 10^{40}$ erg/s) ULXs in our catalogue in search of intermediate-mass black hole candidates, and find nine new possible candidates. ","A new, clean catalogue of extragalactic non-nuclear X-ray sources in
  nearby galaxies",1,"['***KLAXON***\nWe did a new catalogue paper, including ~ 400 ULXs.  Find it on\n\n<LINK>\n\nIf nothing else, it would make an excellent gift for your loved ones this Christmas(*).\n\n(* Disclaimer: it may not make an excellent gift for your loved ones this Christmas.)']",18,12,256
94,216,1313502257154060294,1212714725571612672,David Holzmüller,"1. In a new paper (<LINK>), I prove a distribution-independent lower bound on the label-noise-induced generalization error in ridgeless linear regression with (random) features under weak assumptions. <LINK> 2. The main assumption is that the sample-feature-matrix has full rank with probability one. I prove this for several analytic feature maps, especially for random deep neural networks with analytic activation functions, but also for random Fourier features and polynomial kernels. <LINK> 3. The paper also contains more experiments and a result for the uniform distribution on the unit sphere, which seems to provide the lowest sensitivity to label noise among all distributions satisfying the assumptions. <LINK> 4. The proof of the lower bound is less than two pages long and is mainly based on Jensen's inequality, the Schur complement and other matrix algebra. <LINK> 5. The proof of the main assumption for random neural networks is mainly based on the identity theorem for analytic functions. This strategy also provides a way to reliably computationally verify the main assumption for analytic finite-width Neural Tangent Kernel feature maps. <LINK> 6. I wrote a blog post (<LINK>) with some additional discussion. In the blog post, I also show how to prove the theorem below using combinatorics! <LINK> 7. Thanks to my supervisor, Ingo Steinwart, for helpful comments and to @SimTechStuttga2 @Uni_Stuttgart for funding my research.",https://arxiv.org/abs/2010.01851,"We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functions like sigmoid, tanh, softplus or GELU. As further examples, we show that feature maps from random Fourier features and polynomial kernels also satisfy our assumptions. We complement our theory with further experimental and analytic results. ",On the Universality of the Double Descent Peak in Ridgeless Regression,7,"['1. In a new paper (<LINK>), I prove a distribution-independent lower bound on the label-noise-induced generalization error in ridgeless linear regression with (random) features under weak assumptions. <LINK>', '2. The main assumption is that the sample-feature-matrix has full rank with probability one. I prove this for several analytic feature maps, especially for random deep neural networks with analytic activation functions, but also for random Fourier features and polynomial kernels. https://t.co/mJXlRxRk9L', '3. The paper also contains more experiments and a result for the uniform distribution on the unit sphere, which seems to provide the lowest sensitivity to label noise among all distributions satisfying the assumptions. https://t.co/SIgm7BSFah', ""4. The proof of the lower bound is less than two pages long and is mainly based on Jensen's inequality, the Schur complement and other matrix algebra. https://t.co/H3XWd2cto7"", '5. The proof of the main assumption for random neural networks is mainly based on the identity theorem for analytic functions. This strategy also provides a way to reliably computationally verify the main assumption for analytic finite-width Neural Tangent Kernel feature maps. https://t.co/qW40HOVnVF', '6. I wrote a blog post (https://t.co/TejtYuJTw3) with some additional discussion. In the blog post, I also show how to prove the theorem below using combinatorics! https://t.co/KnN2tuyA4Y', '7. Thanks to my supervisor, Ingo Steinwart, for helpful comments and to @SimTechStuttga2 @Uni_Stuttgart for funding my research.']",20,10,1447
95,9,1410404498238066689,64958481,Birhanu Eshete,"📢New 📰 📢 Given f(x)=y, you obtain x'= x+δ (δ: feature-space perturbation). As a result, f(x')=y'≠y (f is fooled!). Q: is there correlation b/n δ and feature-based explanation of f(x') = y'? A: check our new SecureComm'21 paper 👇by @AbderrahmenAmi2 <LINK>",https://arxiv.org/abs/2106.15820,"Machine Learning (ML) models are susceptible to evasion attacks. Evasion accuracy is typically assessed using aggregate evasion rate, and it is an open question whether aggregate evasion rate enables feature-level diagnosis on the effect of adversarial perturbations on evasive predictions. In this paper, we introduce a novel framework that harnesses explainable ML methods to guide high-fidelity assessment of ML evasion attacks. Our framework enables explanation-guided correlation analysis between pre-evasion perturbations and post-evasion explanations. Towards systematic assessment of ML evasion attacks, we propose and evaluate a novel suite of model-agnostic metrics for sample-level and dataset-level correlation analysis. Using malware and image classifiers, we conduct comprehensive evaluations across diverse model architectures and complementary feature representations. Our explanation-guided correlation analysis reveals correlation gaps between adversarial samples and the corresponding perturbations performed on them. Using a case study on explanation-guided evasion, we show the broader usage of our methodology for assessing robustness of ML models. ",Explanation-Guided Diagnosis of Machine Learning Evasion Attacks,1,"[""📢New 📰 📢\nGiven f(x)=y, you obtain  x'= x+δ (δ: feature-space perturbation). As a result, f(x')=y'≠y (f is fooled!).\nQ: is there correlation b/n δ and feature-based explanation of f(x') = y'?\nA: check our new SecureComm'21 paper 👇by @AbderrahmenAmi2\n<LINK>""]",21,06,254
96,102,1503700997566308352,952949678533849088,Kareem El-Badry,"New paper! We study two “mass gap” black hole candidates in binaries with red giant stars, “the Unicorn” and “the Giraffe”. 1/n <LINK> <LINK> We used spectral disentangling to search for possible luminous companions (as opposed to BHs) to the giants. We found them! 2/n <LINK> What that means, roughly, is that two luminous stars do (much) a better job fitting the spectra than one. <LINK> In both systems, the disentangled spectra of the companions look like subgiant stars (i.e., cooler and larger than main-sequence stars; warmer and smaller than giants). 4/n <LINK> Because these subgiants are cooler than main-sequence stars of similar mass, they are faint in the UV and consistent with the observed spectral energy distributions and UV limits. 5/n <LINK> We can measure the masses of the giants from the observed ellipsoidal variation. In both systems, they are 0.3-0.5 Msun. This is very low for a giant, and implies most of their initial mass was stripped off by a companion. 6/n <LINK> We can also measure the masses of the subgiants dynamically. The dynamically-inferred values (1.8 and 2.8 Msun) are in reasonably good agreement with what we'd estimate from their temperature and luminosity. 7/n <LINK> We used binary evolution models to investigate how these systems formed and how they'll evolve in the future. We think the giant are almost completely stripped of their envelopes and will soon contract to become low-mass helium white dwarfs. 8/n <LINK> This scenario (and the component masses) is almost identical to how we think Regulus, the ~20th brightest star in the sky, formed. It's a main-sequence star with a helium white dwarf companion in a wide orbit. <LINK> 9/n The fact that the companions are subgiants (i.e, off the main sequence) implies that either the initial mass ratio was very close to 1 (like, q&gt;0.99), or that the companions are temporarily inflated due to rapid accretion. 9/ The second possibility is particularly exciting, but it will take more work (ideally a population model of interacting giant binaries) to test it. The Unicorn and Giraffe join a growing population of mass-transfer binaries recently observed at various stages of the stripping process. Several of these other objects have also been previously interpreted as BHs. <LINK> Summary: stellar-mass BHs are small needles in a very large haystack. But they haystack contains a lot of other interesting stuff! n/n",https://arxiv.org/abs/2203.06348,"We analyze two binary systems containing giant stars, V723 Mon (""the Unicorn"") and 2M04123153+6738486 (""the Giraffe""). Both giants orbit more massive but less luminous companions, previously proposed to be mass-gap black holes. Spectral disentangling reveals luminous companions with star-like spectra in both systems. Joint modeling of the spectra, light curves, and spectral energy distributions robustly constrains the masses, temperatures, and radii of both components: the primaries are luminous, cool giants ($T_{\rm eff,\,giant} = 3,800\,\rm K$ and $4,000\,\rm K$, $R_{\rm giant}= 22.5\,R_{\odot}$ and $25\,R_{\odot}$) with exceptionally low masses ($M_{\rm giant} \approx 0.4\,M_{\odot}$) that likely fill their Roche lobes. The secondaries are only slightly warmer subgiants ($T_{\rm eff,\,2} = 5,800\,\rm K$ and $5,150\,\rm K$, $R_2= 8.3\,R_{\odot}$ and $9\,R_{\odot}$) and thus are consistent with observed UV limits that would rule out main-sequence stars with similar masses ($M_2 \approx 2.8\,M_{\odot}$ and $\approx 1.8\,M_{\odot}$). In the Unicorn, rapid rotation blurs the spectral lines of the subgiant, making it challenging to detect even at wavelengths where it dominates the total light. Both giants have surface abundances indicative of CNO processing and subsequent envelope stripping. The properties of both systems can be reproduced by binary evolution models in which a $1-2\,M_{\odot}$ primary is stripped by a companion as it ascends the giant branch. The fact that the companions are also evolved implies either that the initial mass ratio was very near unity, or that the companions are temporarily inflated due to rapid accretion. The Unicorn and Giraffe offer a window into into a rarely-observed phase of binary evolution preceding the formation of wide-orbit helium white dwarfs, and eventually, compact binaries containing two helium white dwarfs. ","Unicorns and Giraffes in the binary zoo: stripped giants with subgiant
  companions",13,"['New paper! We study two “mass gap” black hole candidates in binaries with red giant stars, “the Unicorn” and “the Giraffe”. 1/n <LINK> <LINK>', 'We used spectral disentangling to search for possible luminous companions (as opposed to BHs) to the giants. We found them! 2/n https://t.co/j5WPzU3ikL', 'What that means, roughly, is that two luminous stars do (much) a better job fitting the spectra than one. https://t.co/bogv9ie8nu', 'In both systems, the disentangled spectra of the companions look like subgiant stars (i.e., cooler and larger than main-sequence stars; warmer and smaller than giants). 4/n https://t.co/2cdMaFwDn2', 'Because these subgiants are cooler than main-sequence stars of similar mass, they are faint in the UV and consistent with the observed spectral energy distributions and UV limits. 5/n https://t.co/S20JlBiznj', 'We can measure the masses of the giants from the observed ellipsoidal variation. In both systems, they are 0.3-0.5 Msun. This is very low for a giant, and implies most of their initial mass was stripped off by a companion. 6/n https://t.co/jpx3Pk4kkh', ""We can also measure the masses of the subgiants dynamically. The dynamically-inferred values (1.8 and 2.8 Msun) are in reasonably good agreement with what we'd estimate from their temperature and luminosity. 7/n https://t.co/KSLzPBlDdm"", ""We used binary evolution models to investigate how these systems formed and how they'll evolve in the future. We think the giant are almost completely stripped of their envelopes and will soon contract to become low-mass helium white dwarfs. 8/n https://t.co/NKLLw40ob0"", ""This scenario (and the component masses) is almost identical to how we think Regulus, the ~20th brightest star in the sky, formed. It's a main-sequence star with a helium white dwarf companion in a wide orbit. https://t.co/nk0iFv0E9K 9/n"", 'The fact that the companions are subgiants (i.e, off the main sequence) implies that either the initial mass ratio was very close to 1 (like, q&gt;0.99), or that the companions are temporarily inflated due to rapid accretion. 9/', 'The second possibility is particularly exciting, but it will take more work (ideally a population model of interacting giant binaries) to test it.', 'The Unicorn and Giraffe join a growing population of mass-transfer binaries recently observed at various stages of the stripping process. Several of these other objects have also been previously interpreted as BHs. https://t.co/sxKQRKlkSb', 'Summary: stellar-mass BHs are small needles in a very large haystack. But they haystack contains a lot of other interesting stuff! n/n']",22,03,2420
97,214,1411928301455695874,802543221943439360,Andrea Caputo,Paper day! <LINK> We continue exploring the linear dynamics of an electromagnetic field propagating in curved spacetime with plasma. In particular we study the Kerr metric and show that plasma-driven modes become superradiantly unstable at the linear level. <LINK>,https://arxiv.org/abs/2107.01174,"Motivated by electromagnetic-field confinement due to plasma near accreting black holes, we continue our exploration of the linear dynamics of an electromagnetic field propagating in curved spacetime in the presence of plasma by including three effects that were neglected in our previous analysis: collisions in the plasma, thermal corrections, and the angular momentum of the background black-hole spacetime. We show that: (i) the plasma-driven long-lived modes survive in a collisional plasma except when the collision timescale is unrealistically small; (ii) thermal effects, which might be relevant for accretion disks around black holes, do not affect the axial long-lived modes; (iii) in the case of a spinning black hole the plasma-driven modes become superradiantly unstable at the linear level; (iv) the polar sector in the small-frequency regime admits a reflection point due to the resonant properties of the plasma. Dissipative effects such as absorption, formation of plasma waves, and nonlinear dynamics play a crucial role in the vicinity of this resonant point. ","Plasma-photon interaction in curved spacetime II: collisions, thermal
  corrections, and superradiant instabilities",1,['Paper day! <LINK>\nWe continue exploring the linear dynamics of an electromagnetic field propagating in curved spacetime with plasma. In particular we study the Kerr metric and show that plasma-driven modes become superradiantly unstable at the linear level. <LINK>'],21,07,264
98,7,1091020609071534082,35724743,Adrien Ecoffet,"Go-Explore paper out. New high/average scores: 18 million/650k on Montezuma's Revenge (44k w/o domain knowledge) & 100k+/~60k on Pitfall, all tested w/ sticky actions! Huge thanks to my co-authors @Joost_Huizinga @joelbot3000 @jeffclune @kenneth0stanley. <LINK> <LINK>",https://arxiv.org/abs/1901.10995,"A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of ""superhuman"" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics). ",Go-Explore: a New Approach for Hard-Exploration Problems,1,"[""Go-Explore paper out. New high/average scores: 18 million/650k on Montezuma's Revenge (44k w/o domain knowledge) &amp; 100k+/~60k on Pitfall, all tested w/ sticky actions! Huge thanks to my co-authors @Joost_Huizinga @joelbot3000 @jeffclune @kenneth0stanley. <LINK> <LINK>""]",19,01,268
99,51,1351906274074652676,991380306,James Jackman,"Today's the big day! We have a new paper out! It's a study of white-light stellar flares from ""blended and neighbouring stars"" in the Kepler short cadence data (<LINK>). But what does it mean? Every so often in astronomical studies, you spot a flare that doesn't actually come from the type of star you're interested in (e.g. a Sun-like star). Instead, it comes from a faint star just in the edge of your aperture. This can be a nuisance and is thrown out. These intruders can be avoided by using a carefully curated target list (e.g. no stars with close neighbours), or by vetting your data each time you see a signal. But what happens to all those discarded flares? Can we do anything with them? That's what we decided to look into, using the Kepler short cadence data. We realised that some of the discarded flares could be really energetic. After all, the faint star would have to contribute a decent bit of light relative to the bright source just for the flare to be seen. So, we reanalysed the Kepler SC data, looking for flares that would have been thrown out by other studies. We found that about 7% of flares in the Kepler SC data come from these interlopers. We found that some of the largest flares in our sample pushed the energies that had been seen before with Kepler, and measured the flare rates of some of these stars. The highest amplitude flare changed the brightness of the star by 4 magnitudes in the Kepler bandpass! Looking for flares from stars close to other stars also meant we found flares on wide binary systems! We found that in this co-eval systems, the lower mass stars flared more often, as we expected from previous works. This was a fun paper to work on and it highlights that while it's good to have a clean sample, sometimes it's worth double checking the things you throw away. Enjoy :) <LINK>",https://arxiv.org/abs/2101.07269,"We present the results of a search for stellar flares from stars neighbouring the target sources in the Kepler short cadence data. These flares have been discarded as contaminants in previous surveys and therefore provide an unexplored resource of flare events, in particular high energy events from faint stars. We have measured M dwarf flare energies up to 1.5$\times$10^35 erg, pushing the limit for flare energies measured using Kepler data. We have used our sample to study theflaring activity of wide binaries, finding that the lower mass counterpart in a wide binary flares more often at a given energy. Of the 4430 flares detected in our original search, 298 came from a neighbouring star, a rate of 6.7$\pm$0.4 per cent for the Kepler short cadence lightcurves. We have used our sample to estimate a 5.8$\pm$0.1 per cent rate of false positive flare events in studies using TESS short cadence data. ","Stellar flares from blended and neighbouring stars in Kepler short
  cadence observations",8,"['Today\'s the big day! We have a new paper out! It\'s a study of white-light stellar flares from ""blended and neighbouring stars"" in the Kepler short cadence data (<LINK>). But what does it mean?', ""Every so often in astronomical studies, you spot a flare that doesn't actually come from the type of star you're interested in (e.g. a Sun-like star). Instead, it comes from a faint star just in the edge of your aperture. This can be a nuisance and is thrown out."", 'These intruders can be avoided by using a carefully curated target list (e.g. no stars with close neighbours), or by vetting your data each time you see a signal. But what happens to all those discarded flares? Can we do anything with them?', ""That's what we decided to look into, using the Kepler short cadence data. We realised that some of the discarded flares could be really energetic. After all, the faint star would have to contribute a decent bit of light relative to the bright source just for the flare to be seen."", 'So, we reanalysed the Kepler SC data, looking for flares that would have been thrown out by other studies. We found that about 7% of flares in the Kepler SC data come from these interlopers.', 'We found that some of the largest flares in our sample pushed the energies that had been seen before with Kepler, and measured the flare rates of some of these stars. The highest amplitude flare changed the brightness of the star by 4 magnitudes in the Kepler bandpass!', 'Looking for flares from stars close to other stars also meant we found flares on wide binary systems! We found that in this co-eval systems, the lower mass stars flared more often, as we expected from previous works.', ""This was a fun paper to work on and it highlights that while it's good to have a clean sample, sometimes it's worth double checking the things you throw away. Enjoy :) https://t.co/LyN3CWfehp""]",21,01,1831
100,15,1432880589963763712,4438354094,Tom Wong,"New paper with @Creighton undergraduate Jacob Rapoza! ""Search by Lackadaisical Quantum Walk with Symmetry Breaking."" Jacob started doing research with me the summer before his freshman year, and he is now a junior. <LINK> <LINK> There's something incredibly rewarding about mentoring students in their first research project, to say, ""You've contributed to the body of scientific knowledge!"" Or, ""You've solved a problem that no one else has solved before!"" Or, ""You're the world expert on this topic!""",https://arxiv.org/abs/2108.13856,"The lackadaisical quantum walk is a lazy version of a discrete-time, coined quantum walk, where each vertex has a weighted self-loop that permits the walker to stay put. They have been used to speed up spatial search on a variety of graphs, including periodic lattices, strongly regular graphs, Johnson graphs, and the hypercube. In these prior works, the weights of the self-loops preserved the symmetries of the graphs. In this paper, we show that the self-loops can break all the symmetries of vertex-transitive graphs while providing the same computational speedups. Only the weight of the self-loop at the marked vertex matters, and the remaining self-loop weights can be chosen randomly, as long as they are small compared to the degree of the graph. ",Search by Lackadaisical Quantum Walk with Symmetry Breaking,2,"['New paper with @Creighton undergraduate Jacob Rapoza! ""Search by Lackadaisical Quantum Walk with Symmetry Breaking."" Jacob started doing research with me the summer before his freshman year, and he is now a junior. <LINK> <LINK>', 'There\'s something incredibly rewarding about mentoring students in their first research project, to say, ""You\'ve contributed to the body of scientific knowledge!"" Or, ""You\'ve solved a problem that no one else has solved before!"" Or, ""You\'re the world expert on this topic!""']",21,08,502
101,229,1510812130433724418,836417100864344064,Takuma Udagawa,"Does large-scale language model (LLM) rescoring help near state-of-the-art ASR systems? We study the effect of LLM rescoring on a competitive Conformer-Transducer baseline and conducted some informative analyses. Paper submitted to Interspeech 2022! <LINK> Happy to share my first paper at IBM Research, and thank you to my coauthors @szuk_m and @gakuto_kurata!",https://arxiv.org/abs/2204.00212,"Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been successfully applied to ASR N-best rescoring. However, whether or how they can benefit competitive, near state-of-the-art ASR systems remains unexplored. In this study, we incorporate LLM rescoring into one of the most competitive ASR baselines: the Conformer-Transducer model. We demonstrate that consistent improvement is achieved by the LLM's bidirectionality, pretraining, in-domain finetuning and context augmentation. Furthermore, our lexical analysis sheds light on how each of these components may be contributing to the ASR performance. ","Effect and Analysis of Large-scale Language Model Rescoring on
  Competitive ASR Systems",2,"['Does large-scale language model (LLM) rescoring help near state-of-the-art ASR systems? We study the effect of LLM rescoring on a competitive Conformer-Transducer baseline and conducted some informative analyses. Paper submitted to Interspeech 2022!\n\n<LINK>', 'Happy to share my first paper at IBM Research, and thank you to my coauthors @szuk_m and @gakuto_kurata!']",22,04,361
102,142,1368849730474754048,493582529,Michele Lucente 🇺🇦,"New paper out! <LINK> I show that an overlooked production mechanism within the minimal Type-I Seesaw model can account for the observed dark matter abundance in the form of a keV sterile neutrino. @TTK_RWTH @RWTH @AvHStiftung @UCLouvain_be  1/3 This population can be produced by the decay of the heavier neutral leptons, with masses above the Higgs mass scale, while they are in thermal equilibrium in the early Universe (freeze-in). 2/3 <LINK> Moreover, the implementation of the relevant phenomenological constraints (relic abundance, indirect detection and structure formation) on this model automatically selects a region of the parameter space featuring an approximate lepton number symmetry! 3/3 <LINK>",https://arxiv.org/abs/2103.03253,"We show that the minimal Type-I Seesaw mechanism can successfully account for the observed dark matter abundance in the form of a keV sterile neutrino. This population can be produced by the decay of the heavier neutral leptons, with masses above the Higgs mass scale, while they are in thermal equilibrium in the early Universe (freeze-in). Moreover, the implementation of the relevant phenomenological constraints (relic abundance, indirect detection and structure formation) on this model automatically selects a region of the parameter space featuring an approximate lepton number symmetry. ",Freeze-In Dark Matter within the Seesaw mechanism,3,"['New paper out! <LINK>\n\nI show that an overlooked production mechanism within the minimal Type-I Seesaw model can account for the observed dark matter abundance in the form of a keV sterile neutrino.\n\n@TTK_RWTH @RWTH @AvHStiftung @UCLouvain_be \n\n1/3', 'This population can be produced by the decay of the heavier neutral leptons, with masses above the Higgs mass scale, while they are in thermal equilibrium in the early Universe (freeze-in).\n\n2/3 https://t.co/gl1GQ41SoM', 'Moreover, the implementation of the relevant phenomenological constraints (relic abundance, indirect detection and structure formation) on this model automatically selects a region of the parameter space featuring an approximate lepton number symmetry!\n\n3/3 https://t.co/4g1UHJwbMx']",21,03,710
103,155,1395766500477177863,824426964140249089,Hao Sheng,"[1/6] I'm excited to share our new @AIESConf paper ""Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data"" w/ @KenielYao and @5harad <LINK> 🧵 [2/6] Rapid advances in face-recognition technology have made the privacy implications of surveillance cameras more acute. But it's hard to know where or how many cameras there are. [3/6] We built a computer vision model to detect cameras from 1.6 million street-view images sampled from 16 cities around the world. Human experts then verified positive model detections to ensure near-perfect precision. <LINK> [4/6] We found camera density varies widely: there are 4x as many cameras per km in Boston and NYC than in Seattle and LA. <LINK> [5/6] We also found more cameras in neighborhoods with higher proportions of non-white residents. This concentration of cameras in majority-minority neighborhoods points to the potential disparate impacts of surveillance technology on communities of color. <LINK> [6/6] Check out our website for more, including maps of the locations of verified cameras, our computer vision model, and data to use in your own analysis. <LINK>",https://arxiv.org/abs/2105.01764,"The use of video surveillance in public spaces -- both by government agencies and by private citizens -- has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here, we combine computer vision, human verification, and statistical analysis to estimate the spatial distribution of surveillance cameras. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.2 (in Los Angeles) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents -- a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color. ","Surveilling Surveillance: Estimating the Prevalence of Surveillance
  Cameras with Street View Data",6,"['[1/6] I\'m excited to share our new @AIESConf paper ""Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data"" w/ @KenielYao and @5harad <LINK> 🧵', ""[2/6] Rapid advances in face-recognition technology have made the privacy implications of surveillance cameras more acute. But it's hard to know where or how many cameras there are."", '[3/6] We built a computer vision model to detect cameras from 1.6 million street-view images sampled from 16 cities around the world. Human experts then verified positive model detections to ensure near-perfect precision. https://t.co/masqfxgO03', '[4/6] We found camera density varies widely: there are 4x as many cameras per km in Boston and NYC than in Seattle and LA. https://t.co/mOwye4yPyH', '[5/6] We also found more cameras in neighborhoods with higher proportions of non-white residents. This concentration of cameras in majority-minority neighborhoods points to the potential disparate impacts of surveillance technology on communities of color. https://t.co/jaxiUVmaZP', '[6/6] Check out our website for more, including maps of the locations of verified cameras, our computer vision model, and data to use in your own analysis. https://t.co/tshGui6W5W']",21,05,1153
104,50,1194335282734227456,23000769,Christopher Conselice,"On the @arxiv on Monday, Amy Whitney (Notts PhD student) et al. released a new paper on the evolution of differential galaxy sizes, accepted to ApJ. A number of things are worth noting in this paper. A thread. (1/n) <LINK> We develop a new method of distinguishing high-z galaxies from foreground galaxies. Because of the Lyman-break, we are able to remove contamination. We call this ""2D Lyman-break imaging""  Example here, 2nd from left is original image, far right, cleaned of foreground. (2/n) <LINK> Using the Petrosian radius, which is independent of distances and surface brightness dimming we can measure how 'inner' and 'outer' parts of galaxies grow with time.  The 'eta' values are the ratio of the surface brightness at a radius to the surface brightness within a radius. <LINK> Taking the difference in the outer and inner radii it is clear that the outer radii are growing at a fast rate than the inner radii at z &lt; 7.  We confirm that this is an actual effect and not a bias by carrying out extensive simulations. (4/n) <LINK> There is more detail in the paper (have a look) but in general this demonstrates that inside-out growth of galaxies is the dominate process for forming galaxies at z=7 down to z=1.  How this happens is another question which we discuss, but minor mergers is one good way. (5/n)",https://arxiv.org/abs/1911.02589,"We present a size analysis of a sample of $\sim$ 49,000 galaxies from the CANDELS GOODS North and South fields using redshift-independent relative surface brightness metrics to determine an unbiased measure of the differential size evolution of galaxies at $1 \leq z \leq 7$. We introduce a novel method of removing foreground objects from distant galaxy ($z > 3$) images that makes use of the Lyman-break at 912{\AA}, in what we call `2-D Lyman-Break Imaging'. The images used are in the rest-frame optical at $z < 3$ and progressively bluer bands at $z > 3$. They are therefore subject to K-correction and cosmological dimming effects which are tested and corrected for. We separately consider a mass-selected sample (with masses in the range 10$^9$M$_{\odot}$$\leq$M$_*$$\leq$10$^{10.5}$M$_{\odot}$) and a number density selected sample (using a constant number density of $n = 1\times10^{-4}$Mpc$^{-3}$). Instead of utilising the commonly used, but potentially biased, effective radii for size measurements, we measure the redshift-independent Petrosian radius, defined by the parameter $\eta$, for each galaxy for three values of $\eta$ and use this as a proxy for size. The evolution of the measured radii can be described by a power-law of the form $R_{Petr} = \alpha(1+z)^\beta$kpc where $\beta < 0$. We find that the outer radius increases more rapidly, suggesting that as a galaxy grows mass is added to its outer regions via an inside-out growth. This growth is stronger for the number density selected sample, with a growth rate of nearly three in the outer radii compared to the inner. We test and confirm these results using a series of image simulations. ","Unbiased Differential Size Evolution and the Inside-Out Growth of
  Galaxies in the Deep CANDELS GOODS Fields at $1 \leq z \leq 7$",5,"['On the @arxiv on Monday, Amy Whitney (Notts PhD student) et al. released a new paper on the evolution of differential galaxy sizes, accepted to ApJ.  A number of things are worth noting in this paper. A thread. (1/n)\n\n<LINK>', 'We develop a new method of distinguishing high-z galaxies from foreground galaxies. Because of the Lyman-break, we are able to remove contamination.  We call this ""2D Lyman-break imaging"" \n\nExample here, 2nd from left is original image, far right, cleaned of foreground.  (2/n) https://t.co/mLS3LRMbzJ', ""Using the Petrosian radius, which is independent of distances and surface brightness dimming we can measure how 'inner' and 'outer' parts of galaxies grow with time.   The 'eta' values are the ratio of the surface brightness at a radius to the surface brightness within a radius. https://t.co/8GSqk37aEt"", 'Taking the difference in the outer and inner radii it is clear that the outer radii are growing at a fast rate than the inner radii at z &lt; 7.   We confirm that this is an actual effect and not a bias by carrying out extensive simulations. (4/n) https://t.co/z4SgezyTok', 'There is more detail in the paper (have a look) but in general this demonstrates that inside-out growth of galaxies is the dominate process for forming galaxies at z=7 down to z=1.   How this happens is another question which we discuss, but minor mergers is one good way. (5/n)']",19,11,1322
105,27,1408461856532930561,268203365,gerald pao,"Our new paper is a 1st attempt to download brains from organisms into computers with using whole brain electrophysiology & embedding activity into a network of manifolds to reproduce realistic behavioral as well as neuronal dynamics in generative mode. <LINK> <LINK> @_rdgao Thanks Richard. Scale free embedding! @S_Sigfusson Also larval Zebrafish and human visual pathways @snpc_404 real data has noise both from observation and process noise so my general approach is to keep it simple. Using the geodesic might be better but I generally try to do as little as possible to the data to prevent overfitting. Therefore I use the Sugihara minimalist approach. @snpc_404 I am not opposed to anything in principle as long as it works better. Observatoins are not smooth so for it to be you either need to interpolate or fit with ODEs which is something that requires you to know the underlying variables that might be missing. @snpc_404 So we use a combination of real time series of observed neurons and delays thereof as formulated in the generalized Takens theorem, where you can combine real observables with ""placeholder"" unknowns that show up as delays in the embedding. <LINK> @snpc_404 In summary Riemannian manifolds would be nice because they would be nice but I don't know if they would work better with real world data. But I am open to is if it works. Thank you for your comment and question",https://arxiv.org/abs/2106.10627,"We propose an algorithm grounded in dynamical systems theory that generalizes manifold learning from a global state representation, to a network of local interacting manifolds termed a Generative Manifold Network (GMN). Manifolds are discovered using the convergent cross mapping (CCM) causal inference algorithm which are then compressed into a reduced redundancy network. The representation is a network of manifolds embedded from observational data where each orthogonal axis of a local manifold is an embedding of a individually identifiable neuron or brain area that has exact correspondence in the real world. As such these can be experimentally manipulated to test hypotheses derived from theory and data analysis. Here we demonstrate that this representation preserves the essential features of the brain of flies,larval zebrafish and humans. In addition to accurate near-term prediction, the GMN model can be used to synthesize realistic time series of whole brain neuronal activity and locomotion viewed over the long term. Thus, as a final validation of how well GMN captures essential dynamic information, we show that the artificially generated time series can be used as a training set to predict out-of-sample observed fly locomotion, as well as brain activity in out of sample withheld data not used in model building. Remarkably, the artificially generated time series show realistic novel behaviors that do not exist in the training data, but that do exist in the out-of-sample observational data. This suggests that GMN captures inherently emergent properties of the network. We suggest our approach may be a generic recipe for mapping time series observations of any complex nonlinear network into a model that is able to generate naturalistic system behaviors that identifies variables that have real world correspondence and can be experimentally manipulated. ",Experimentally testable whole brain manifolds that recapitulate behavior,7,"['Our new paper is a 1st attempt to download brains from organisms into computers with using whole brain electrophysiology &amp; embedding activity into a network of manifolds to reproduce realistic behavioral as well as neuronal dynamics in generative mode.\n<LINK> <LINK>', '@_rdgao Thanks Richard. Scale free embedding!', '@S_Sigfusson Also larval Zebrafish and human visual pathways', '@snpc_404 real data has noise both from observation and process noise so my general approach is to keep it simple. Using the geodesic might be better but I generally try to do as little as possible to the data to prevent overfitting. Therefore I use the Sugihara minimalist approach.', '@snpc_404 I am not opposed to anything in principle as long as it works better. Observatoins are not smooth so for it to be you either need to interpolate or fit with ODEs which is something that requires you to know the underlying variables that might be missing.', '@snpc_404 So we use a combination of real time series of observed neurons and delays thereof as formulated in the generalized Takens theorem, where you can combine real observables with ""placeholder"" unknowns that show up as delays in the embedding.\nhttps://t.co/rMyP03Ax2J', ""@snpc_404 In summary Riemannian manifolds would be nice because they would be nice but I don't know if they would work better with real world data. But I am open to is if it works. Thank you for your comment and question""]",21,06,1400
106,23,988585800588709888,369569444,Takahiro TERADA (寺田 隆広),"Our new paper is available now! This is relevant in scenarios with enhanced curvature perturbations or matter dominance, e.g. PBH scenarios and some of curvaton scenarios. <LINK>  By the way, my first [gr-qc] paper (actually changed from hep-ph by arXiv admin). <LINK>",https://arxiv.org/abs/1804.08577,"Whether or not the primordial gravitational wave (GW) produced during inflation is sufficiently strong to be observable, GWs are necessarily produced from the primordial curvature perturbations in the second order of perturbation. The induced GWs can be enhanced by curvature perturbations enhanced at small scales or by the presence of matter-dominated stages of the cosmological history. We analytically calculate the integral in the expression of the power spectrum of the induced GWs which is a universal part independent of the spectrum of the primordial curvature perturbations. This makes the subsequent numerical integrals significantly easy. In simple cases, we derive fully analytic formulas for the induced GW spectrum. ","Semianalytic Calculation of Gravitational Wave Spectrum Nonlinearly
  Induced from Primordial Curvature Perturbations",1,"['Our new paper is available now! This is relevant in scenarios with enhanced curvature perturbations or matter dominance, e.g. PBH scenarios and some of curvaton scenarios. <LINK>   By the way, my first [gr-qc] paper (actually changed from hep-ph by arXiv admin). <LINK>']",18,04,268
107,10,1069257342968086528,17373048,Rodrigo Nemmen,Paper out by our group: what do globular clusters in Our Galaxy look like in gamma-rays? Analysis of @NASAFermi observations of 157 GCs. New GCs found. Large encounter rates seem necessary for getting ms pulsars. Work led by grad student Raniere Menezes <LINK>,https://arxiv.org/abs/1811.06957,"Globular clusters (GCs) are evolved stellar systems containing entire populations of millisecond pulsars (MSPs), which are efficient gamma-ray emitters. Observations of this emission can be used as a powerful tool to explore the dynamical processes leading to binary system formation in GCs. In this work, 9 years of Fermi Large Area Telescope data were used to investigate the gamma-ray emission from all GCs in the Milky Way. 23 clusters were found as gamma-ray bright, with 2 of them never having been reported before. It was also found that magnetic braking probably has a smaller impact on the formation rate of binary systems in metal-rich GCs than previously suggested, while a large value for the two-body encounter rate seems to be a necessary condition. The influence of the encounter rate per formed binary was for the first time explored in conjunction with gamma-ray data, giving evidence that if this quantity is very high, binary systems will get destroyed before having time to evolve into MSPs, thus decreasing the total number of MSPs in a GC. No extended emission was found even for clusters whose optical extent is ~0.5 degrees; all of them are point-like sources spatially in agreement with the optical cores of the GCs, supporting previous X-rays results of heavier objects sinking into the clusters' cores via dynamical friction. The possibility of extrapolating these results to ultra-compact dwarf galaxies is discussed, as these systems are believed to be the intermediate case between GCs and dwarf galaxies. ","Milky Way globular clusters in gamma-rays: analyzing the dynamical
  formation of millisecond pulsars",1,['Paper out by our group: what do globular clusters in Our Galaxy look like in gamma-rays? Analysis of @NASAFermi observations of 157 GCs. New GCs found. Large encounter rates seem necessary for getting ms pulsars. Work led by grad student Raniere Menezes <LINK>'],18,11,260
108,234,1312926204073185280,755924666,Brant Robertson,New theory paper from the Lyman Continuum Escape Survey (LACES) team led by Kirk Barrow (@KIPAC1). We look at the connection btwn oxygen emission line ratios & Lyman continuum escape in radiative transfer calcs applied to cosmo sims of galaxy formation: <LINK>,https://arxiv.org/abs/2010.00592,"Escaping Lyman continuum photons from galaxies likely reionized the intergalactic medium at redshifts $z\gtrsim6$. However, the Lyman continuum is not directly observable at these redshifts and secondary indicators of Lyman continuum escape must be used to estimate the budget of ionizing photons. Observationally, at redshifts $z\sim2-3$ where the Lyman continuum is observationally accessible, surveys have established that many objects that show appreciable Lyman continuum escape fractions $f_{esc}$ also show enhanced [OIII]/[OII] (O$_{32}$) emission line ratios. Here, we use radiative transfer analyses of cosmological zoom-in simulations of galaxy formation to study the physical connection between $f_{esc}$ and O$_{32}$. Like the observations, we find that the largest $f_{esc}$ values occur at elevated O$_{32}\sim3-10$ and that the combination of high $f_{esc}$ and low O$_{32}$ is extremely rare. While high $f_{esc}$ and O$_{32}$ often are observable concurrently, the timescales of the physical origin for the processes are very different. Large O$_{32}$ values fluctuate on short ($\sim$1 Myr) timescales during the Wolf-Rayet-powered phase after the formation of star clusters, while channels of low absorption are established over tens of megayears by collections of supernovae. We find that while there is no direct causal relation between $f_{esc}$ and O$_{32}$, high $f_{esc}$ most often occurs after continuous input from star formation-related feedback events that have corresponding excursions to large O$_{32}$ emission. These calculations are in agreement with interpretations of observations that large $f_{esc}$ tends to occur when O$_{32}$ is large, but large O$_{32}$ does not necessarily imply efficient Lyman continuum escape. ","The Lyman Continuum Escape Survey: Connecting Time-Dependent [OIII] and
  [OII] Line Emission with Lyman Continuum Escape Fraction in Simulations of
  Galaxy Formation",1,['New theory paper from the Lyman Continuum Escape Survey (LACES) team led by Kirk Barrow (@KIPAC1). We look at the connection btwn oxygen emission line ratios &amp; Lyman continuum escape in radiative transfer calcs applied to cosmo sims of galaxy formation:\n\n<LINK>'],20,10,260
109,38,1408081802347089928,202211003,Alan Winfield 💙,"Experiments in Artificial Culture: from noisy imitation to storytelling robots <LINK> A pre-print of our new paper (in review), which (a) outlines 10 years of experimental work modelling aspects of cultural evolution with robots, and 1/2 (b) asks for ideas and research questions we could explore with our Storybots. 2/2",https://arxiv.org/abs/2106.11754,"This paper presents a series of experiments in collective social robotics, spanning more than 10 years, with the long-term aim of building embodied models of (aspects) of cultural evolution. Initial experiments demonstrated the emergence of behavioural traditions in a group of social robots programmed to imitate each other's behaviours (we call these Copybots). These experiments show that the noisy (i.e. less than perfect fidelity) imitation that comes for free with real physical robots gives rise naturally to variation in social learning. More recent experimental work extends the robots' cognitive capabilities with simulation-based internal models, equipping them with a simple artificial theory of mind. With this extended capability we explore, in our current work, social learning not via imitation but robot-robot storytelling, in an effort to model this very human mode of cultural transmission. In this paper we give an account of the methods and inspiration for these experiments, the experiments and their results, and an outline of possible directions for this programme of research. It is our hope that this paper stimulates not only discussion but suggestions for hypotheses to test with the Storybots. ","Experiments in Artificial Culture: from noisy imitation to storytelling
  robots",2,"['Experiments in Artificial Culture: from noisy imitation to storytelling robots <LINK> A pre-print of our new paper (in review), which (a) outlines 10 years of experimental work modelling aspects of cultural evolution with robots, and 1/2', '(b) asks for ideas and research questions we could explore with our Storybots. 2/2']",21,06,320
110,113,1212304062819053569,760022547895377920,Federico Errica,"Happy New Year with our #Tutorial on #DeepLearning for #Graphs we just released on arXiv! Any feedback would be very appreciated! Spoiler: let's stop calling everything a #GNN. We instead propose the term Deep Graph Networks, (DGNs). <LINK> With D. Bacciu, A. Micheli and @poddaccio!",https://arxiv.org/abs/1912.12693,"The adaptive processing of graph data is a long-standing research topic which has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is designed as a tutorial introduction to the field of deep learning for graphs. It favours a consistent and progressive introduction of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view to the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. It introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. The methodological exposition is complemented by a discussion of interesting research challenges and applications in the field. ",A Gentle Introduction to Deep Learning for Graphs,2,"[""Happy New Year with our #Tutorial on #DeepLearning for #Graphs we just released on arXiv! Any feedback would be very appreciated!\n\nSpoiler: let's stop calling everything a #GNN. We instead propose the term Deep Graph Networks, (DGNs).\n<LINK>"", 'With D. Bacciu, A. Micheli and @poddaccio!']",19,12,283
111,187,1292715732061626368,786855300322172928,Alkistis Pourtsidou,"Paper alert (with a delay due to holiday season!) -- in <LINK> led by @CunningtonSD we studied the degeneracy between primordial non-gaussianity (PNG) and foreground removal systematics for intensity mapping experiments [thread]. Foreground removal methods remove some signal (unless one is very conservative, but then the error budget becomes much larger due to the residuals), especially on the large scales, where PNG signatures are expected to lie! With simulated data and MCMC we studied the effects of this on the precision accuracy with which we can probe the f_NL local parameter with a large SKA1-MID @SKA_telescope survey, using FG removal methods that are currently used in real data analyses. If we ignore the possibility of FG removal effects on the signal, we get *extremely biased* (wrong) estimates of f_NL, as shown in the purple contour where our fiducial f_NL = 0. The other contours correspond to the unrealistic cases where FG removal is 0 or perfectly known. <LINK> To add some realism, we devised a model with 1 free, nuisance parameter, which is marginalised over to account for FG removal properly. We found that it works well, and that we can recover unbiased estimates (but the f_NL uncertainties increase a lot!). <LINK> This result has implications also for cross-correlations and multi-tracer methods, and it means that further work is required to get the precision we need to be competitive. We are thinking about solutions -- more, hopefully, soon! As an aside, note that we have made suites of simulated data and power spectrum and MCMC codes available at our group's repository <LINK> -- @CunningtonSD @psahds also see <LINK>",https://arxiv.org/abs/2007.12126,"Potential evidence for primordial non-Gaussianity (PNG) is expected to lie in the largest scales mapped by cosmological surveys. Forthcoming 21cm intensity mapping experiments will aim to probe these scales by surveying neutral hydrogen (HI) within galaxies. However, foreground signals dominate the faint 21cm emission, meaning foreground cleaning is required to recover the cosmological signal. The effect this has is to damp the HI power spectrum on the largest scales, especially along the line-of-sight. Whilst there is agreement that this contamination is potentially problematic for probing PNG, it is yet to be fully explored and quantified. In this work we carry out the first forecasts on $f_\text{NL}$ that incorporate simulated foreground maps that are removed using techniques employed in real data. Using an MCMC analysis, we demonstrate that foreground cleaned data recovers hugely biased values ($f_\text{NL} = -102.1_{-7.96}^{+8.39}$ [68% CL]) on our $f_\text{NL}=0$ fiducial input. Introducing a model with fixed parameters for the foreground contamination allows us to recover unbiased results ($f_\text{NL} = -2.94_{-11.9}^{+11.4}$). However, it is not clear that we will have sufficient understanding of foreground contamination to allow for such rigid models. Treating the main parameter $k_\parallel^\text{FG}$ in our foreground model as a nuisance parameter and marginalizing over it, still recovers unbiased results but at the expense of much larger errors ($f_\text{NL} = 0.75^{+40.2}_{-44.5}$), that can only be reduced by imposing the Planck 2018 prior. Our results show that significant progress on understanding and controlling foreground removal effects is necessary in order to study PNG with HI intensity mapping. ","The degeneracy between primordial non-Gaussianity and foregrounds in
  21cm intensity mapping experiments",7,"['Paper alert (with a delay due to holiday season!) -- in <LINK> led by @CunningtonSD  we studied the degeneracy between primordial non-gaussianity (PNG) and foreground removal systematics for intensity mapping experiments [thread].', 'Foreground removal methods remove some signal (unless one is very conservative, but then the error budget becomes much larger due to the residuals), especially on the large scales, where PNG signatures are expected to lie!', 'With simulated data and MCMC we studied the effects of this on the precision accuracy with which we can probe the f_NL local parameter with a large SKA1-MID @SKA_telescope survey, using FG removal methods that are currently used in real data analyses.', 'If we ignore the possibility of FG removal effects on the signal, we get *extremely biased* (wrong) estimates of f_NL, as shown in the purple contour where our fiducial f_NL = 0. The other contours correspond to the unrealistic cases where FG removal is 0 or perfectly known. https://t.co/Dugeg7WHzB', 'To add some realism, we devised a model with 1 free, nuisance parameter, which is marginalised over to account for FG removal properly. We found that it works well, and that we can recover unbiased estimates (but the f_NL uncertainties increase a lot!). https://t.co/QIxyRc9uyP', 'This result has implications also for cross-correlations and multi-tracer methods, and it means that further work is required to get the precision we need to be competitive. We are thinking about solutions -- more, hopefully, soon!', ""As an aside, note that we have made suites of simulated data and power spectrum and MCMC codes available at our group's repository https://t.co/FCvccjqwAy -- @CunningtonSD @psahds also see https://t.co/aoXY9jEffy""]",20,07,1659
112,84,1382483856339603459,710263466351681536,Max Vladymyrov 🇺🇦,"New paper! 📜 We introduce BLUR (Bidirectional Learning Update Rules), a novel optimization framework that parametrizes forward and back-propagation via a set of very low-dimensional meta-learned matrices. <LINK> 🧵 to follow... We first demonstrate that a classical neural net can be seen as a special case of a 2-state network (one for activations and one for gradients) with synapses updated according to a Hebbian rule. <LINK> We then generalize this to a multi-state bidirectional network, where interactions between the states are controlled by a small set of meta-learned parameters called the ""genome"".  No loss function & no backprop gradients required! <LINK> As an additional bonus: BLUR can learn with asymmetric forward and backward synapses (removing the “weight transport problem” and hinting at biological plausibility), using activations on the backward pass and more! <LINK> We demonstrate that our system can learn faster than SGD, with genomes trained using a standard off-the-shelf optimizer, such as Adam.  What’s more, genomes also can be trained with non-differentiable objectives (like accuracy) using Evolutionary training. <LINK> Genomes meta-trained on MNIST can generalize to multiple other datasets, even for different input resolutions, number of classes or different architectures. <LINK> We hope this work will open doors for a more general paradigm where the learning process itself is: - deeply parametrized by meta-training data; - not restricted by backpropagation, including but not limited to, having explicit loss functions and gradients. Joint work with my coworkers: Mark Sandler, Andrey Zhmoginov, Nolan Miller, Andrew Jackson, Tom Madams, Blaise Agüera y Arcas @blaiseaguera",https://arxiv.org/abs/2104.04657,"In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional ""genome"". We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks. ",Meta-Learning Bidirectional Update Rules,8,"['New paper! 📜\n\nWe introduce BLUR (Bidirectional Learning Update Rules), a novel optimization framework that parametrizes forward and back-propagation via a set of very low-dimensional meta-learned matrices.\n\n<LINK>\n\n🧵 to follow...', 'We first demonstrate that a classical neural net can be seen as a special case of a 2-state network (one for activations and one for gradients) with synapses updated according to a Hebbian rule. https://t.co/LUsSqxUCER', 'We then generalize this to a multi-state bidirectional network, where interactions between the states are controlled by a small set of meta-learned parameters called the ""genome"". \n\nNo loss function &amp; no backprop gradients required! https://t.co/SHPwl58SXc', 'As an additional bonus: BLUR can learn with asymmetric forward and backward synapses (removing the “weight transport problem” and hinting at biological plausibility), using activations on the backward pass and more! https://t.co/NHv1nC5HyD', 'We demonstrate that our system can learn faster than SGD, with genomes trained using a standard off-the-shelf optimizer, such as Adam. \n\nWhat’s more, genomes  also can be trained with non-differentiable objectives (like accuracy) using Evolutionary training. https://t.co/K974IpuggD', 'Genomes meta-trained on MNIST can generalize to multiple other datasets, even for different input resolutions, number of classes or different architectures. https://t.co/sTDESDi2s3', 'We hope this work will open doors for a more general paradigm where the learning process itself is:\n- deeply parametrized by meta-training data;\n- not restricted by backpropagation, including but not limited to, having explicit loss functions and gradients.', 'Joint work with my coworkers:\nMark Sandler,\nAndrey Zhmoginov,\nNolan Miller,\nAndrew Jackson,\nTom Madams, \nBlaise Agüera y Arcas @blaiseaguera']",21,04,1716
113,170,1339947842241253377,84822240,Luca Bertinetto 🇮🇹 🇪🇺 🌐,"New preprint🐣from Steinar Laenen and me. ""On Episodes, Prototypical Networks, and Few-shot Learning"" - <LINK> To better understand the usefulness of episodes in few-shot learning, we start with a case study on Prototypical (and Matching) Networks. More 👇1/3 By functionally dividing batches in the disjoint support set S and query set Q, the episodic strategy of ProtoNets ignores many training pairs (left). If we forego this separation (right), we can use a number of extra pairs that grows as O(ways^2(queries^2+shots^2)). 2/3 <LINK> With a series of ablations that connect Proto/Matching Nets to the NCA loss [Goldberger et al, 2005], we show that significant performance can be recovered in these methods by simply using standard mini-batches instead of episodes. Check out the paper for much more. 3/3 <LINK>",https://arxiv.org/abs/2012.09831,"Episodic learning is a popular practice among researchers and practitioners interested in few-shot learning. It consists of organising training in a series of learning problems (or episodes), each divided into a small training and validation subset to mimic the circumstances encountered during evaluation. But is this always necessary? In this paper, we investigate the usefulness of episodic learning in methods which use nonparametric approaches, such as nearest neighbours, at the level of the episode. For these methods, we not only show how the constraints imposed by episodic learning are not necessary, but that they in fact lead to a data-inefficient way of exploiting training batches. We conduct a wide range of ablative experiments with Matching and Prototypical Networks, two of the most popular methods that use nonparametric approaches at the level of the episode. Their ""non-episodic"" counterparts are considerably simpler, have less hyperparameters, and improve their performance in multiple few-shot classification datasets. ","On Episodes, Prototypical Networks, and Few-shot Learning",3,"['New preprint🐣from Steinar Laenen and me.\n\n""On Episodes, Prototypical Networks, and Few-shot Learning"" - <LINK>\n\nTo better understand the usefulness of episodes in few-shot learning, we start with a case study on Prototypical (and Matching) Networks.\n\nMore 👇1/3', 'By functionally dividing batches in the disjoint support set S and query set Q, the episodic strategy of ProtoNets ignores many training pairs (left).\nIf we forego this separation (right), we can use a number of extra pairs that grows as O(ways^2(queries^2+shots^2)).\n\n2/3 https://t.co/33D7XgXK6n', 'With a series of ablations that connect Proto/Matching Nets to the NCA loss [Goldberger et al, 2005], we show that significant performance can be recovered in these methods by simply using standard mini-batches instead of episodes.\n\nCheck out the paper for much more.\n3/3 https://t.co/Xr9ehBbVRl']",20,12,814
114,175,1364127006435213314,786855300322172928,Alkistis Pourtsidou,"Paper alert! In <LINK>, led by @CunningtonSD and @CatAstro_Phy, we present a simulations and modelling study of the HI intensity mapping bispectrum, including nasty observational effects from foregrounds with polarisation leakage + a beam with sidelobes. <LINK> And here's how the covariance looks including these effects <LINK>",https://arxiv.org/abs/2102.11153,"The bispectrum is a 3-point statistic with the potential to provide additional information beyond power spectra analyses of survey datasets. Radio telescopes which broadly survey the 21cm emission from neutral hydrogen (HI) are a promising way to probe LSS and in this work we present an investigation into the HI intensity mapping (IM) bispectrum using simulations. We present a model of the redshift space HI IM bispectrum including observational effects from the radio telescope beam and 21cm foreground contamination. We validate our modelling prescriptions with measurements from robust IM simulations, inclusive of these observational effects. Our foreground simulations include polarisation leakage, on which we use a Principal Component Analysis cleaning method. We also investigate the effects from a non-Gaussian beam including side-lobes. For a MeerKAT-like single-dish IM survey at $z=0.39$, we find that foreground removal causes a 8% reduction in the equilateral bispectrum's signal-to-noise ratio $S/N$, whereas the beam reduces it by 62%. We find our models perform well, generally providing $\chi^2_\text{dof}\sim 1$, indicating a good fit to the data. Whilst our focus is on post-reionisation, single-dish IM, our modelling of observational effects, especially foreground removal, can also be relevant to interferometers and reionisation studies. ",The HI intensity mapping bispectrum including observational effects,2,"['Paper alert! In <LINK>, led by @CunningtonSD and @CatAstro_Phy, we present a simulations and modelling study of the HI intensity mapping bispectrum, including nasty observational effects from foregrounds with polarisation leakage + a beam with sidelobes. <LINK>', ""And here's how the covariance looks including these effects https://t.co/Jk8SyDUOTT""]",21,02,328
115,40,1276436332047413249,60084334,Frank Verstraete,"Simulating frustrated spin systems with tensor networks can lead to obstacles similar to the sign problem in Monte Carlo. Our new paper shows how to overcome those: <LINK> @IRFDMRG Thanks for the reference! As far as i understand, Kanamori's method does not give any information about the ground state rules and hence does not allow to calculate the residual entropy. It might well be a dual formulation of the problem.",http://arxiv.org/abs/2006.14341,"Motivated by the recent success of tensor networks to calculate the residual entropy of spin ice and kagome Ising models, we develop a general framework to study frustrated Ising models in terms of infinite tensor networks %, i.e. tensor networks that can be contracted using standard algorithms for infinite systems. This is achieved by reformulating the problem as local rules for configurations on overlapping clusters chosen in such a way that they relieve the frustration, i.e. that the energy can be minimized independently on each cluster. We show that optimizing the choice of clusters, including the weight on shared bonds, is crucial for the contractibility of the tensor networks, and we derive some basic rules and a linear program to implement them. We illustrate the power of the method by computing the residual entropy of a frustrated Ising spin system on the kagome lattice with next-next-nearest neighbour interactions, vastly outperforming Monte Carlo methods in speed and accuracy. The extension to finite-temperature is briefly discussed. ",Solving frustrated Ising models using tensor networks,2,"['Simulating frustrated spin systems with tensor networks can lead to obstacles similar to the sign problem in Monte Carlo. Our new paper shows how to overcome those: <LINK>', ""@IRFDMRG Thanks for the reference! As far as i understand, Kanamori's method does not give any information about the ground state rules and hence does  not allow to calculate the residual entropy. It might well  be a dual formulation  of the problem.""]",20,06,419
116,223,1446188786203959311,252608121,Yaneer Bar-Yam,"Modeling complex systems: A case study of compartmental models in epidemiology with PK Kollepara, @AlexSiegenfeld  We ""show how assumptions can constrain model outcomes to a narrow portion of the wide landscape of potential epidemic behaviors"" 1/ <LINK> No model accurately captures all the details of the system that it represents, but some models are nonetheless accurate because certain large-scale behaviors don't depend on all these details. The key to good modeling is understanding which details matter and which do not.  2/",https://arxiv.org/abs/2110.02947,"Compartmental epidemic models have been widely used for predicting the course of epidemics, from estimating the basic reproduction number to guiding intervention policies. Studies commonly acknowledge these models' assumptions but less often justify their validity in the specific context in which they are being used. Our purpose is not to argue for specific alternatives or modifications to compartmental models, but rather to show how assumptions can constrain model outcomes to a narrow portion of the wide landscape of potential epidemic behaviors. This concrete examination of well-known models also serves to illustrate general principles of modeling that can be applied in other contexts. ","Modeling complex systems: A case study of compartmental models in
  epidemiology",2,"['Modeling complex systems: A case study of compartmental models in epidemiology\n\nwith PK Kollepara, @AlexSiegenfeld \n\nWe ""show how assumptions can constrain model outcomes to a narrow portion of the wide landscape of potential epidemic behaviors""\n\n1/\n\n<LINK>', ""No model accurately captures all the details of the system that it represents, but some models are nonetheless accurate because certain large-scale behaviors don't depend on all these details. The key to good modeling is understanding which details matter and which do not. \n\n2/""]",21,10,531
117,41,1376452403390713856,1297174706622205955,Andrea Palermo,"A week after the theoretical paper, it's time for some simulations. What would imply the new spin-shear coupling for the local polarization sign puzzle? Find out in our new paper, today on the arXiv! <LINK> 1/6 The sign of the polarization along the beam direction predicted by the local equilibrium picture is opposite with respect to the experiments. This instance is known as ""polarization sign puzzle"" and there have been several attempts to solve it. 2/6 For instance, hydro simulations considering the usual spin-vorticity coupling give the following polarization as a function of transverse momentum; in experiments what is red would be blue and vice-versa! 3/6 <LINK> Can the spin-shear coupling solve the problem? Indeed if we consider the thermal shear the simulation gives a result closer to the experiments, but not yet satisfactory... 4/6 <LINK> However, at very high energy the decoupling hypersurface (where the plasma is no longer a fluid) is believed to be at constant temperature. Keeping this in mind we obtain a formula for isothermal local equilibrium, coupling spin with the gradients of the velocity. 5/6 <LINK> This new formula restores the agreement between theory and experiments, and we get the correct sign with two different hydro codes! Also, polarization seems to be very sensitive to the decoupling temperature providing a wonderful tool to study hadronization. 6/6 <LINK>",https://arxiv.org/abs/2103.14621,"We show that the inclusion of a recently found additional term of the spin polarization vector at local equilibrium which is linear in the symmetrized gradients of the velocity field, and the assumption of hadron production at constant temperature restore the quantitative agreement between hydrodynamic model predictions and local polarization measurements in relativistic heavy ion collisions at $\sqrt{s_{\rm NN}} = 200$ GeV. The longitudinal component of the spin polarization vector turns out to be very sensitive to the temperature value, with a good fit around 155 MeV. The implications of this finding are discussed. ","Local polarization and isothermal local equilibrium in relativistic
  heavy ion collisions",6,"[""A week after the theoretical paper, it's time for some simulations. What would imply the new spin-shear coupling for the local polarization sign puzzle?\n\nFind out in our new paper, today on the arXiv!\n<LINK>\n\n1/6"", 'The sign of the polarization along the beam direction predicted by the local equilibrium picture is opposite with respect to the experiments. This instance is known as ""polarization sign puzzle"" and there have been several attempts to solve it. \n2/6', 'For instance, hydro simulations considering the usual spin-vorticity coupling give the following polarization as a function of transverse momentum; in experiments what is red would be blue and vice-versa! \n3/6 https://t.co/JVA4TVPKp9', 'Can the spin-shear coupling solve the problem? Indeed if we consider the thermal shear the simulation gives a result closer to the experiments, but not yet satisfactory...\n4/6 https://t.co/GZ9wR0m3pi', 'However, at very high energy the decoupling hypersurface (where the plasma is no longer a fluid) is believed to be at constant temperature. Keeping this in mind we obtain a formula for isothermal local equilibrium, coupling spin with the gradients of the velocity. \n5/6 https://t.co/ToOzbzSSnR', 'This new formula restores the agreement between theory and experiments, and we get the correct sign with two different hydro codes! Also, polarization seems to be very sensitive to the decoupling temperature providing a wonderful tool to study hadronization. \n6/6 https://t.co/Z20xOJ5GK4']",21,03,1404
118,132,1412435996344467463,1019760963569049601,Almog Yalinewich,"1/3 New paper on the arxiv with my former student Andrey Remorov. We report a novel, universal conservation law for strong explosions in steep atmospheric density gradients <LINK> 2/3 We find a simple power law relation between swept up mass and shock velocity. Shown below is a map of the power law index vs adiabatic index and density profile <LINK> 3/3 This conservation law reproduces experiments and numerical simulations, and we show how it can be used to model many astrophysical problems, such as bolides (e.g. Chelyabinsk event), impact craters, asteroid impact avoidance and hypervelocity white dwarfs",https://arxiv.org/abs/2107.01701,"In this work we present a mathematical model for the propagation of the shock waves that occur in graded density profiles. These waves can occur in a wide range of astrophysical events, such as collisions in planetary and stellar atmospheres, common envelope explosions and peculiar type Ia supernovae. The behaviour of the shock wave and its evolution can be modelled using type II self similar solutions. In such solutions the evolution of the shock wave is determined by boundary conditions at the shock front and a singular point in the shocked region. We show how the evolution can be determined for different equations of state and density profiles, and compare these results to numerical simulations. These findings are also applied to a variety of astrophysical phenomena to further test their validity. ",The Propagation of Strong Shocks into Planetary and Stellar Atmospheres,3,"['1/3 New paper on the arxiv with my former student Andrey Remorov. We report a novel, universal conservation law for strong explosions in steep atmospheric density gradients\n<LINK>', '2/3 We find a simple power law relation between swept up mass and shock velocity. Shown below is a map of the power law index vs adiabatic index and density profile https://t.co/SCKm5469wL', '3/3 This conservation law reproduces experiments and numerical simulations, and we show how it can be used to model many astrophysical problems, such as bolides (e.g. Chelyabinsk event), impact craters, asteroid impact avoidance and hypervelocity white dwarfs']",21,07,611
119,221,1500864861617541122,194377912,Brian Keating,New @CMB3K primordial B-mode results! We increased the data volume by 80% and find the measured spectrum is consistent with ΛCDM (including foregrounds). We place an upper limit on the tensor-to-scalar ratio of r&lt; 0.33 at 95% confidence. <LINK> 📸: Debra Kellner <LINK>,https://arxiv.org/abs/2203.02495,"We report an improved measurement of the degree-scale CMB $B$-mode angular power spectrum over 670 square-degree sky area with POLARBEAR. In the original analysis of the data, errors in the angle measurement of the continuously rotating half-wave plate, a polarization modulator, caused significant data loss. By introducing an angle-correction algorithm, the data volume is increased by a factor of 1.8. We report a new analysis using the larger data set. We find the measured $B$-mode spectrum is consistent with the $\Lambda$CDM model with Galactic foregrounds. We place an upper limit on the tensor-to-scalar ratio $r$ < 0.33 at 95% confidence level. ","Improved upper limit on degree-scale CMB B-mode polarization power from
  the 670 square-degree POLARBEAR survey",1,['New @CMB3K primordial B-mode results! We increased the data volume by 80% and find the measured spectrum is consistent with ΛCDM (including foregrounds). We place an upper limit on the tensor-to-scalar ratio of r&lt; 0.33 at 95% confidence. \n<LINK>\n📸: Debra Kellner <LINK>'],22,03,271
120,105,1169775264034779136,35031140,Bas Hofstra,"Hello #metascience2019 & other friends, we just posted our new paper on arXiv: <LINK>. Title basically covers content: ""Diversity Breeds Innovation With Discounted Impact and Recognition."" Studied through records and texts from nearly all US PhDs across 30years. Elaborates and extends prior work on innovation through text analyses and asks where the diversity-paradox in science (diversity breeds innovation, whereas diverse folks have less science careers) comes from. With my great colleagues Sebastian Munoz-Najar Galvez, Bryan He, @viveksc, and Daniel A. McFarland @viveksc I'm very glad to tentatively share this piece, it integrates many things/processes that are/were rewarding: an important question and answer, integration of multiple data sets, novel metrics and computation, interdisciplinary set of authors.",https://arxiv.org/abs/1909.02063,"Prior work finds a diversity paradox: diversity breeds innovation, and yet, underrepresented groups that diversify organizations have less successful careers within them. Does the diversity paradox hold for scientists as well? We study this by utilizing a near-population of ~1.2 million US doctoral recipients from 1977-2015 and following their careers into publishing and faculty positions. We use text analysis and machine learning to answer a series of questions: How do we detect scientific innovations? Are underrepresented groups more likely to generate scientific innovations? And are the innovations of underrepresented groups adopted and rewarded? Our analyses show that underrepresented groups produce higher rates of scientific novelty. However, their novel contributions are devalued and discounted: e.g., novel contributions by gender and racial minorities are taken up by other scholars at lower rates than novel contributions by gender and racial majorities, and equally impactful contributions of gender and racial minorities are less likely to result in successful scientific careers than for majority groups. These results suggest there may be unwarranted reproduction of stratification in academic careers that discounts diversity's role in innovation and partly explains the underrepresentation of some groups in academia. ",The Diversity-Innovation Paradox in Science,4,"['Hello #metascience2019 &amp; other friends, we just posted our new paper on arXiv: <LINK>. Title basically covers content: ""Diversity Breeds Innovation With Discounted Impact and Recognition."" Studied through records and texts from nearly all US PhDs across 30years.', 'Elaborates and extends prior work on innovation through text analyses and asks where the diversity-paradox in science (diversity breeds innovation, whereas diverse folks have less science careers) comes from.', 'With my great colleagues Sebastian Munoz-Najar Galvez, Bryan He, @viveksc, and Daniel A. McFarland', ""@viveksc I'm very glad to tentatively share this piece, it integrates many things/processes that are/were rewarding: an important question and answer, integration of multiple data sets, novel metrics and computation, interdisciplinary set of authors.""]",19,09,821
121,2,1357442374239023104,14754639,Kai Lukoff,"Ever get lost on YouTube? We have a new CHI 2021 paper just for you: “How the Design of YouTube Influences User Sense of Agency” <LINK> Co-authored w/ @ulyngs @himanshuzade Vera Liao, James Choi, Kaiyue Fan, @smunson & Alexis Hiniker [1/5] We identify which design mechanisms users say affect their sense of control over the time they spend in the YouTube mobile app. Less control: recommendations, ads, autoplay. More control: Playlists, search, subscriptions, play controls, watch history [2/5] <LINK> The mechanism called out most often is recommendations. YouTube is wickedly good at the local optimization problem: Out of millions of videos, which one is the user most likely to watch? But the user lacks the ability to align these video recs w/ their long-term goals. [3/5] On the flipside, a design idea to support greater control: microplaning, i.e., making a lightweight plan to guide behavior for a short time. For example, encourage the user to create a short video playlist for their current session of use. [4/5] Our paper builds on fantastic earlier work by @UichinLee @EricPSB @gezakovacs @youngho_yhkim @gratydesign @AnnaCox_ @jcccf @elena_agapie and many others! [5/5]",https://arxiv.org/abs/2101.11778,"In the attention economy, video apps employ design mechanisms like autoplay that exploit psychological vulnerabilities to maximize watch time. Consequently, many people feel a lack of agency over their app use, which is linked to negative life effects such as loss of sleep. Prior design research has innovated external mechanisms that police multiple apps, such as lockout timers. In this work, we shift the focus to how the internal mechanisms of an app can support user agency, taking the popular YouTube mobile app as a test case. From a survey of 120 U.S. users, we find that autoplay and recommendations primarily undermine sense of agency, while search and playlists support it. From 13 co-design sessions, we find that when users have a specific intention for how they want to use YouTube they prefer interfaces that support greater agency. We discuss implications for how designers can help users reclaim a sense of agency over their media use. ",How the Design of YouTube Influences User Sense of Agency,5,"['Ever get lost on YouTube? We have a new CHI 2021 paper just for you: “How the Design of YouTube Influences User Sense of Agency” <LINK> Co-authored w/ @ulyngs @himanshuzade Vera Liao, James Choi, Kaiyue Fan, @smunson &amp; Alexis Hiniker [1/5]', 'We identify which design mechanisms users say affect their sense of control over the time they spend in the YouTube mobile app. Less control: recommendations, ads, autoplay. More control: Playlists, search, subscriptions, play controls, watch history [2/5] https://t.co/Ltps7vbcT4', 'The mechanism called out most often is recommendations. YouTube is wickedly good at the local optimization problem: Out of millions of videos, which one is the user most likely to watch? But the user lacks the ability to align these video recs w/ their long-term goals. [3/5]', 'On the flipside, a design idea to support greater control: microplaning, i.e., making a lightweight plan to guide behavior for a short time. For example, encourage the user to create a short video playlist for their current session of use. [4/5]', 'Our paper builds on fantastic earlier work by @UichinLee @EricPSB @gezakovacs @youngho_yhkim @gratydesign @AnnaCox_ @jcccf @elena_agapie and many others! [5/5]']",21,01,1185
122,41,974372669184401408,307826617,Kev Abazajian ⤷⏳🌎,New paper on galaxy formation in WDM or sterile ν dark matter Universes: “The discovery of young ultra-faint dwarf galaxies with no ancient star formation -- which do not exist in our CDM simulations -- would therefore provide evidence in support of WDM.” <LINK> <LINK>,https://arxiv.org/abs/1803.05424,"We study the impact of a warm dark matter (WDM) cosmology on dwarf galaxy formation through a suite of cosmological hydrodynamical zoom-in simulations of $M_{\rm halo} \approx10^{10}\,M_{\odot}$ dark matter halos as part of the Feedback in Realistic Environments (FIRE) project. A main focus of this paper is to evaluate the combined effects of dark matter physics and stellar feedback on the well-known small-scale issues found in cold dark matter (CDM) models. We find that the $z=0$ stellar mass of a galaxy is strongly correlated with the central density of its host dark matter halo at the time of formation, $z_{\rm f}$, in both CDM and WDM models. WDM halos follow the same $M_{\star}(z=0)-V_{\rm max}(z_{\rm f})$ relation as in CDM, but they form later, are less centrally dense, and therefore contain galaxies that are less massive than their CDM counterparts. As a result, the impact of baryonic effects on the central gravitational potential is typically diminished relative to CDM. However, the combination of delayed formation in WDM and energy input from stellar feedback results in dark matter profiles with lower overall densities. The WDM galaxies studied here have a wider diversity of star formation histories (SFHs) than the same systems simulated in CDM, and the two lowest $M_{\star}$ WDM galaxies form all of their stars at late times. The discovery of young ultra-faint dwarf galaxies with no ancient star formation -- which do not exist in our CDM simulations -- would therefore provide evidence in support of WDM. ","Warm FIRE: Simulating Galaxy Formation with Resonant Sterile Neutrino
  Dark Matter",1,['New paper on galaxy formation in WDM or sterile ν dark matter Universes: “The discovery of young ultra-faint dwarf galaxies with no ancient star formation -- which do not exist in our CDM simulations -- would therefore provide evidence in support of WDM.” <LINK> <LINK>'],18,03,269
123,142,1511480013870157828,1134563269262360577,Yang Zheng,"Two new papers on arXiv: 1. <LINK>. The optimization landscape of LQG problems has saddle points. We show in this paper that many of them are not bad in the sense that they are ``equivalent'' to strict saddles. Perturbed gradient descent can escape them quickly. 2. <LINK>. Controller parameterization in the frequency domain is convex but infinitely dimensional. They have no immediately efficient computation. In this paper, we make a somewhat surprising connection with filtering and introduce the first LMI for computation",https://arxiv.org/abs/2204.00912,"First order policy optimization has been widely used in reinforcement learning. It guarantees to find the optimal policy for the state-feedback linear quadratic regulator (LQR). However, the performance of policy optimization remains unclear for the linear quadratic Gaussian (LQG) control where the LQG cost has spurious suboptimal stationary points. In this paper, we introduce a novel perturbed policy gradient (PGD) method to escape a large class of bad stationary points (including high-order saddles). In particular, based on the specific structure of LQG, we introduce a novel reparameterization procedure which converts the iterate from a high-order saddle to a strict saddle, from which standard random perturbations in PGD can escape efficiently. We further characterize the high-order saddles that can be escaped by our algorithm. ","Escaping High-order Saddles in Policy Optimization for Linear Quadratic
  Gaussian (LQG) Control",2,"[""Two new papers on arXiv: \n1. <LINK>. The optimization landscape of LQG problems has saddle points. We show in this paper that many of them are not bad in the sense that they are ``equivalent'' to strict saddles. Perturbed gradient descent can escape them quickly."", '2. https://t.co/x6vNhfpohQ. Controller parameterization in the frequency domain is convex but infinitely dimensional. They have no immediately efficient computation. In this paper, we make a somewhat surprising connection with filtering and introduce the first LMI for computation']",22,04,526
124,133,1212764548492668928,907232486735958018,Jaki Noronha-Hostler,Check out Jamie Stafford's preview of work that will be done shortly on the freeze-out of light versus strange particles: <LINK> We study the influence of different # of particles & find the flavor hierarchy still holds. Thermal fit results should be out soon. <LINK>,https://arxiv.org/abs/1912.12968,"We calculate the mean-over-variance ratio of the net-kaon fluctuations in the Hadron Resonance Gas (HRG) Model for the five highest energies of the RHIC Beam Energy Scan (BES) for different particle data lists. We compare these results with the latest experimental data from the STAR collaboration in order to extract sets of chemical freeze-out parameters for each list. We focused on the PDG2012 and PDG2016+ particle lists, which differ largely in the number of resonant states. Our analysis determines the effect of the amount of resonances included in the HRG on the freeze-out conditions. ","Determination of Chemical Freeze-out Parameters from Net-kaon
  Fluctuations at RHIC",1,"[""Check out Jamie Stafford's preview of work that will be done shortly on the freeze-out of light versus strange particles: <LINK>\n\nWe study the influence of different # of particles &amp; find the flavor hierarchy still holds. Thermal fit results should be out soon. <LINK>""]",19,12,267
125,108,1092779809422434304,748850869366468609,"Rishi Paudel, PhD","Here is our paper: ""Origin of radio-quiet coronal mass ejections in flare stars"" <LINK>. We have used a hybrid magnetic model to study the Alfen speed profile of flaring M dwarfs and explain the origin of ""radio-quiet"" CMEs. @johngizis @rachelosten @pkgw",https://arxiv.org/abs/1902.00810,"Type II radio bursts are observed in the Sun in association with many coronal mass ejections (CME's. In view of this association, there has been an expectation that, by scaling from solar flares to the flares which are observed on M dwarfs, radio emission analogous to solar Type II bursts should be detectable in association with M dwarf flares. However, several surveys have revealed that this expectation does not seem to be fulfilled. Here we hypothesize that the presence of larger global field strengths in low-mass stars, suggested by recent magneto-convective modeling, gives rise to such large Alfven speeds in the corona that it becomes difficult to satisfy the conditions for the generation of Type II radio bursts. As a result, CME's propagating in the corona/wind of a flare stars are expected to be ""radio-quiet"" as regards Type II bursts. In view of this, we suggest that, in the context of Type II bursts, scaling from solar to stellar flares is of limited effectiveness. ",Origin of radio-quiet coronal mass ejections in flare stars,1,"['Here is our paper: ""Origin of radio-quiet coronal mass ejections in flare stars"" <LINK>.\nWe have used a hybrid magnetic model to study the Alfen speed profile of flaring M dwarfs and explain the origin of ""radio-quiet"" CMEs.\n@johngizis @rachelosten @pkgw']",19,02,254
126,244,1404652964564791300,2915749124,Dhiraj Hazra,We propose an inflationary primordial feature model that can explain both the large and small-scale anomalies in the currently observed CMB data (<LINK>). Can we detect these anomalies with future observations ? With @xingangchen01 and Matteo Braglia. <LINK> Can we detect these anomalies with future observations ? Using the specifications of future space based  #LiteBIRD and ground based @SimonsObs we address this question. A #PICO or #CMBBHARAT type near-ultimate observation will be able to provide a decisive evidence.,https://arxiv.org/abs/2106.07546,"We propose an inflationary primordial feature model that can explain both the large and small-scale anomalies in the currently measured cosmic microwave background (CMB) anisotropy spectra, revealing a clip of adventurous history of the Universe during its primordial epoch. Although the model is currently statistically indistinguishable from the Standard Model, we show that future observations such as the Simons Observatory and LiteBIRD will complement each other in distinguishing the model differences due to their accurate E-mode polarization measurements, and the PICO mission, if funded, can put stringent constraints on all characteristic properties. The model predicts a signal of classical primordial standard clock, which can also be used to distinguish the inflation and alternative scenarios in a model-independent fashion. ","Uncovering the History of Cosmic Inflation from Anomalies in Cosmic
  Microwave Background Spectra",2,"['We propose an inflationary primordial feature model that can explain both the large and small-scale anomalies in the currently observed CMB data (<LINK>). Can we detect these anomalies with future observations ? With @xingangchen01 and Matteo Braglia. <LINK>', 'Can we detect these anomalies with future observations ? Using the specifications of future space based   #LiteBIRD and ground based @SimonsObs  we address this question. A #PICO or #CMBBHARAT type near-ultimate observation will be able to provide a decisive evidence.']",21,06,525
127,20,1111127085903278080,742606297,Ben Oppenheimer 🇺🇦,"I had a lot of help with this Astro2020 Decadal white paper. The X-ray circumgalactic medium is a true frontier. I will always be a UV CGM person, but I also love new challenges! Thanks goes to my twitter co-authors @astrogrant and @rcrain_astro.  <LINK>",https://arxiv.org/abs/1903.11130,"The majority of baryons reside beyond the optical extent of a galaxy in the circumgalactic and intergalactic media (CGM/IGM). Gaseous halos are inextricably linked to the appearance of their host galaxies through a complex story of accretion, feedback, and continual recycling. The energetic processes, which define the state of gas in the CGM, are the same ones that 1) regulate stellar growth so that it is not over-efficient, and 2) create the diversity of today's galaxy colors, SFRs, and morphologies spanning Hubble's Tuning Fork Diagram. They work in concert to set the speed of growth on the star-forming Main Sequence, transform a galaxy across the Green Valley, and maintain a galaxy's quenched appearance on the Red Sequence. Most baryons in halos more massive than 10^12 Msolar along with their high-energy physics and dynamics remain invisible because that gas is heated above the UV ionization states. We argue that information on many of the essential drivers of galaxy evolution is primarily contained in this ""missing"" hot gas phase. Completing the picture of galaxy formation requires uncovering the physical mechanisms behind stellar and SMBH feedback driving mass, metals, and energy into the CGM. By opening galactic hot halos to new wavebands, we not only obtain fossil imprints of >13 Gyrs of evolution, but observe on-going hot-mode accretion, the deposition of superwind outflows into the CGM, and the re-arrangement of baryons by SMBH feedback. A description of the flows of mass, metals, and energy will only be complete by observing the thermodynamic states, chemical compositions, structure, and dynamics of T>=10^6 K halos. These measurements are uniquely possible with a next-generation X-ray observatory if it provides the sensitivity to detect faint CGM emission, spectroscopic power to measure absorption lines and gas motions, and high spatial resolution to resolve structures. ",Imprint of Drivers of Galaxy Formation in the Circumgalactic Medium,1,"['I had a lot of help with this Astro2020 Decadal white paper. The X-ray circumgalactic medium is a true frontier.  I will always be a UV CGM person, but I also love new challenges!  Thanks goes to my twitter co-authors @astrogrant and @rcrain_astro.  \n<LINK>']",19,03,254
128,87,1348126920542621696,1348060735746703365,sehoonkim,"TinyML for NLP? We have just released I-BERT, which is a new low precision *integer-only* BERT quantization framework, targeted for accurate NLP at the edge. 👇 Paper: <LINK> Code: <LINK> [1/5] Unlike many CNN models, which consist only of linear and piecewise linear operations, Transformer based NLP models use non-linear operations such as GELU and Softmax. These operations prevent the previous quantization schemes from performing integer-only inference. [2/5] <LINK> To address this, we propose a novel method to approximate non-linear operations with low-degree polynomials. Since polynomials consist only of additions and multiplications, they can be computed with integer arithmetic and yield the same result as floating point arithmetic. [3/5] <LINK> Based on our approximation methods, I-BERT performs the entire inference of Transformer architectures with INT8. This includes the nonlinear activations such as GELU and Softmax, as well as the rest of the network. [4/5] I-BERT achieves comparable and even *slightly higher* GLUE score as compared to FP32 RoBERTa-Base and Large despite being quantized and 4x smaller.  In summary, our method could be the key for bringing computationally heavy Transformer based NLP models onto edge devices. [5/5] <LINK> See the details in: ""I-BERT: Integer-only BERT Quantization"" By @sehoonkim14, @a__gholami, @ZheweiYao, Michael W. Mahoney, Kurt Keutzer Paper: <LINK>",https://arxiv.org/abs/2101.01321,"Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive efficient inference at the edge, and even at the data center. While quantization can be a viable solution for this, previous work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot efficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM processors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has been open-sourced. ",I-BERT: Integer-only BERT Quantization,6,"['TinyML for NLP?\nWe have just released I-BERT, which is a new low precision *integer-only* BERT quantization framework, targeted for accurate NLP at the edge. 👇\nPaper: <LINK>\nCode: <LINK>\n[1/5]', 'Unlike many CNN models, which consist only of linear and piecewise linear operations, Transformer based NLP models use non-linear operations such as GELU and Softmax. These operations prevent the previous quantization schemes from performing integer-only inference.\n[2/5] https://t.co/6bo8ryn8mI', 'To address this, we propose a novel method to approximate non-linear operations with low-degree polynomials. Since polynomials consist only of additions and multiplications, they can be computed with integer arithmetic and yield the same result as floating point arithmetic.\n[3/5] https://t.co/Dk3B4iioZf', 'Based on our approximation methods, I-BERT performs the entire inference of Transformer architectures with INT8. \nThis includes the nonlinear activations such as GELU and Softmax, as well as the rest of the network. \n[4/5]', 'I-BERT achieves comparable and even *slightly higher* GLUE score as compared to FP32 RoBERTa-Base and Large despite being quantized and 4x smaller. \n\nIn summary, our method could be the key for bringing computationally heavy Transformer based NLP models onto edge devices.\n[5/5] https://t.co/2OxHrMDkVg', 'See the details in:\n\n""I-BERT: Integer-only BERT Quantization""\n\nBy @sehoonkim14, @a__gholami, \n@ZheweiYao, Michael W. Mahoney, Kurt Keutzer\n\nPaper: https://t.co/S3OCyLx1GH']",21,01,1415
129,49,1276073042842660865,134172617,Jonathan Davies,"Very pleased to release a new paper on the arXiv today in collaboration with @rcrain_astro and @apontzen! We utilise ""genetically modified"" simulations to explore how assembly history directly influences the evolution of a galaxy and its CGM. <LINK> @LJMU_Astro We find that systematically shifting the assembly history of a star-forming disc galaxy's dark matter halo to earlier times results in a quenched, spheroidal galaxy instead. A movie demonstrating this can be found here: <LINK>",https://arxiv.org/abs/2006.13221,"We examine the influence of dark matter halo assembly on the evolution of a simulated $\sim L^\star$ galaxy. Starting from a zoom-in simulation of a star-forming galaxy evolved with the EAGLE galaxy formation model, we use the genetic modification technique to create a pair of complementary assembly histories: one in which the halo assembles later than in the unmodified case, and one in which it assembles earlier. Delayed assembly leads to the galaxy exhibiting a greater present-day star formation rate than its unmodified counterpart, whilst in the accelerated case the galaxy quenches at $z\simeq 1$, and becomes spheroidal. We simulate each assembly history nine times, adopting different seeds for the random number generator used by EAGLE's stochastic subgrid implementations of star formation and feedback. The systematic changes driven by differences in assembly history are significantly stronger than the random scatter induced by this stochasticity. The sensitivity of $\sim L^\star$ galaxy evolution to dark matter halo assembly follows from the close coupling of the growth histories of the central black hole (BH) and the halo, such that earlier assembly fosters the formation of a more massive BH, and more efficient expulsion of circumgalactic gas. In response to this expulsion, the circumgalactic medium reconfigures at a lower density, extending its cooling time and thus inhibiting the replenishment of the interstellar medium. Our results indicate that halo assembly history significantly influences the evolution of $\sim L^\star$ central galaxies, and that the expulsion of circumgalactic gas is a crucial step in quenching them. ","Quenching and morphological evolution due to circumgalactic gas
  expulsion in a simulated galaxy with a controlled assembly history",2,"['Very pleased to release a new paper on the arXiv today in collaboration with @rcrain_astro and @apontzen! We utilise ""genetically modified"" simulations to explore how assembly history directly influences the evolution of a galaxy and its CGM. <LINK> @LJMU_Astro', ""We find that systematically shifting the assembly history of a star-forming disc galaxy's dark matter halo to earlier times results in a quenched, spheroidal galaxy instead. A movie demonstrating this can be found here: https://t.co/Id02q9V6XK""]",20,06,488
130,93,1113603294453280773,184338062,Andrew Drozdov,"Now with paper link: <LINK> And code: <LINK> New results on unsupervised parsing: +6.5 F1 compared to ON-LSTM (2019), +6 F1 compared to PRLG (2011). <LINK> Our model learns to predict tokens with a cloze-like objective, incorporating a latent chart parser that allows us to decode constituency parses after training with the CKY algorithm. <LINK> Our results improve on previous work. We also use a simple heuristic during inference time that further boosts performance. <LINK> We include a handful of examples parses in the paper / appendix, including on some longer examples. Don't be offended by the ""box tree"" parse representation — they're compact and easy to stack vertically. <LINK> Of course, I had a typo in the initial tweet. The full title is: Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders (DIORA) #NAACL2019",https://arxiv.org/abs/1904.02142,"We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. ","Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive
  Autoencoders",5,"['Now with paper link: <LINK>\nAnd code: <LINK>\n\nNew results on unsupervised parsing: +6.5 F1 compared to ON-LSTM (2019), +6 F1 compared to PRLG (2011). <LINK>', 'Our model learns to predict tokens with a cloze-like objective, incorporating a latent chart parser that allows us to decode constituency parses after training with the CKY algorithm. https://t.co/aHkxTiuF0C', 'Our results improve on previous work. We also use a simple heuristic during inference time that further boosts performance. https://t.co/7qoEWeYI8g', 'We include a handful of examples parses in the paper / appendix, including on some longer examples. Don\'t be offended by the ""box tree"" parse representation — they\'re compact and easy to stack vertically. https://t.co/CNV7Sfbupg', 'Of course, I had a typo in the initial tweet. The full title is: Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders (DIORA) #NAACL2019']",19,04,856
131,29,952865519442460672,3716338821,Mikko Tuomi,"Our new paper led by @MatiasDiazM, @ExoplanetJJ (also @phillippro, @johannateske et al.) is out! We study the radial velocities of #HD26965 and find a consistent periodic signal. But is it caused by an #exoplanet orbiting the star with P = 42.36d? <LINK> <LINK> We find that the signal is there beyond any reasonable doubt, supported by RV data from #HARPS, #HIRES, #PFS, and #CHIRON. If it is caused by a planet, which is what I think, it corresponds to a hot mini-Neptune with a minimum mass of 7Me. <LINK> The planet, should it exist, is ""hot"" because it is located interior to the inner edge of the liquid-water habitable zone of #HD26965, i.e. the it is too hot for liquid water to exist on the planet's surface. Here is the current detection threshold for additional planets. <LINK> But there is a catch - #HD26965 shows evidence for activity-induced variations. There is a long-period magnetic activity cycle (CaII H&K emission data from Mount Wilson) and possible rotation period of 37d, or 43d coinciding with the RV signal. <LINK> Evidence for activity-induced periodicity of 43d is rather weak. However, because it is very close to the RV period, it is plausible that they are caused by the same process, i.e. stellar rotation. Literature gives rotation period of 37 - 38d. So have we found a planet at all? Read more at about it in our paper accepted for publication in AJ. <LINK> ""This study serves as an excellent test case for future works that aim to detect small planets orbiting ‘Sun-like’ stars using radial velocity measurements."" <LINK> #HD26965 is rather similar to the Sun, a bit less massive and less luminous. But we could not detect planets smaller than 10Me in the star's habitable zone. This is the typical and so unfortunate state-of-the-art precision with radial velocities. <LINK> @TomimPA @MatiasDiazM @ExoplanetJJ @phillippro @johannateske Who knows? I certainly am not aware anymore, and the concept of ""group"" is very vague as well. Not to mention, there is no such thing as ""certain"". So this is a very simple question to ask, but almost impossible to answer. The best answer I can give is ""dozens"".",https://arxiv.org/abs/1801.03970,"We report the discovery of a radial velocity signal that can be interpreted as a planetary-mass candidate orbiting the K dwarf HD26965, with an orbital period of 42.364$\pm$0.015 days, or alternatively, as the presence of residual, uncorrected rotational activity in the data. Observations include data from HIRES, PFS, CHIRON, and HARPS, where 1,111 measurements were made over 16 years. Our best solution for HD26965 $b$ is consistent with a super-Earth that has a minimum mass of 6.92$\pm$0.79 M$_{\oplus}$ orbiting at a distance of 0.215$\pm$0.008 AU from its host star. We have analyzed the correlation between spectral activity indicators and the radial velocities from each instrument, showing moderate correlations that we include in our model. From this analysis, we recover a $\sim$38 day signal, which matches some literature values of the stellar rotation period. However, from independent Mt. Wilson HK data for this star, we find evidence for a significant 42 day signal after subtraction of longer period magnetic cycles, casting doubt on the planetary hypothesis for this period. Although our statistical model strongly suggests that the 42-day signal is Doppler in origin, we conclude that the residual effects of stellar rotation are difficult to fully model and remove from this dataset, highlighting the difficulties to disentangle small planetary signals and photospheric noise, particularly when the orbital periods are close to the rotation period of the star. This study serves as an excellent test case for future works that aim to detect small planets orbiting `Sun-like' stars using radial velocity measurements. ","The test case of HD26965: difficulties disentangling weak Doppler
  signals from stellar activity",8,"['Our new paper led by @MatiasDiazM, @ExoplanetJJ (also @phillippro, @johannateske et al.) is out! We study the radial velocities of #HD26965 and find a consistent periodic signal. But is it caused by an #exoplanet orbiting the star with P = 42.36d? <LINK> <LINK>', 'We find that the signal is there beyond any reasonable doubt, supported by RV data from #HARPS, #HIRES, #PFS, and #CHIRON. If it is caused by a planet, which is what I think, it corresponds to a hot mini-Neptune with a minimum mass of 7Me. https://t.co/Od0LaIoAja', 'The planet, should it exist, is ""hot"" because it is located interior to the inner edge of the liquid-water habitable zone of #HD26965, i.e. the it is too hot for liquid water to exist on the planet\'s surface. Here is the current detection threshold for additional planets. https://t.co/CdBD8zQUw4', 'But there is a catch - #HD26965 shows evidence for activity-induced variations. There is a long-period magnetic activity cycle (CaII H&amp;K emission data from Mount Wilson) and possible rotation period of 37d, or 43d coinciding with the RV signal. https://t.co/qV27MjtnR8', 'Evidence for activity-induced periodicity of 43d is rather weak. However, because it is very close to the RV period, it is plausible that they are caused by the same process, i.e. stellar rotation. Literature gives rotation period of 37 - 38d. So have we found a planet at all?', 'Read more at about it in our paper accepted for publication in AJ. https://t.co/qQrY0ee4vG\n\n""This study serves as an excellent test case for future works that aim to detect small planets orbiting ‘Sun-like’ stars using radial velocity measurements."" https://t.co/UBH97hpc0X', ""#HD26965 is rather similar to the Sun, a bit less massive and less luminous. But we could not detect planets smaller than 10Me in the star's habitable zone. This is the typical and so unfortunate state-of-the-art precision with radial velocities. https://t.co/mT6TfEHNKj"", '@TomimPA @MatiasDiazM @ExoplanetJJ @phillippro @johannateske Who knows? I certainly am not aware anymore, and the concept of ""group"" is very vague as well. Not to mention, there is no such thing as ""certain"". So this is a very simple question to ask, but almost impossible to answer. The best answer I can give is ""dozens"".']",18,01,2135
132,213,1513771265471172617,1352638067761311744,Joseph Imperial,"Say you're writing a story for a Grade 2 learner, how do you ensure that what you'll write next will still be readable by the student? Can you maintain the text's complexity throughout? We propose the task of Uniform Complexity for Text Generation. <LINK> We investigate if humans and neural language models (GPT-2) trained to produce coherent stories can *maintain* the linguistic complexities of generated continuations with respect to the prompts.  We explored over 160 features. Turns out, both generally struggle in this task. This project is supported by my @GoogleAI and @TensorFlow grant. Also big thanks to @Alexir563 for helping me with the GPT-2 models 💯 @mrinmayasachan @GoogleAI @TensorFlow @Alexir563 I was about to email you the link 😅",https://arxiv.org/abs/2204.05185,"Powerful language models such as GPT-2 have shown promising results in tasks such as narrative generation which can be useful in an educational setup. These models, however, should be consistent with the linguistic properties of triggers used. For example, if the reading level of an input text prompt is appropriate for low-leveled learners (ex. A2 in the CEFR), then the generated continuation should also assume this particular level. Thus, we propose the task of uniform complexity for text generation which serves as a call to make existing language generators uniformly complex with respect to prompts used. Our study surveyed over 160 linguistic properties for evaluating text complexity and found out that both humans and GPT-2 models struggle in preserving the complexity of prompts in a narrative generation setting. ",Uniform Complexity for Text Generation,4,"[""Say you're writing a story for a Grade 2 learner, how do you ensure that what you'll write next will still be readable by the student? Can you maintain the text's complexity throughout?\n\nWe propose the task of Uniform Complexity for Text Generation.\n\n<LINK>"", 'We investigate if humans and neural language models (GPT-2) trained to produce coherent stories can *maintain* the linguistic complexities of generated continuations with respect to the prompts. \n\nWe explored over 160 features. Turns out, both generally struggle in this task.', 'This project is supported by my @GoogleAI and @TensorFlow grant. Also big thanks to @Alexir563 for helping me with the GPT-2 models 💯', '@mrinmayasachan @GoogleAI @TensorFlow @Alexir563 I was about to email you the link 😅']",22,04,750
133,169,1316289280348688384,22392129,Jasmijn Bastings,"New #BlackboxNLP 2020 paper with @fajtak: The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? <LINK> #NLProc We summarize the debate on whether attention is explanation, and find that often the goal (whether explicitly stated or not) is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are much better suited as explanation, and we discuss a few of those. We hope to shift attention from attention to saliency methods, while also discussing some limitations that saliency methods have & going beyond. @TuhinChakr @fajtak If you care about what words humans choose you're evaluating plausibility rather than faithfulness. We argue that a model developer is interested in the latter mostly, and wants to know what the model is using. That doesn't necessarily align with human intuition. @TuhinChakr @fajtak Thanks for the reference to YAKE :-) It seems somewhat different from the methods that we describe in the paper, that focus on neural NLP models for which we can exploit gradients or the backwards pass to obtain explanations.",https://arxiv.org/abs/2010.05607,"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations. ","The elephant in the interpretability room: Why use attention as
  explanation when we have saliency methods?",5,"['New #BlackboxNLP 2020 paper with @fajtak: The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? <LINK> #NLProc', 'We summarize the debate on whether attention is explanation, and find that often the goal (whether explicitly stated or not) is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer.', 'For this goal and user, we argue that input saliency methods are much better suited as explanation, and we discuss a few of those. We hope to shift attention from attention to saliency methods, while also discussing some limitations that saliency methods have &amp; going beyond.', ""@TuhinChakr @fajtak If you care about what words humans choose you're evaluating plausibility rather than faithfulness. We argue that a model developer is interested in the latter mostly, and wants to know what the model is using. That doesn't necessarily align with human intuition."", '@TuhinChakr @fajtak Thanks for the reference to YAKE :-) It seems somewhat different from the methods that we describe in the paper, that focus on neural NLP models for which we can exploit gradients or the backwards pass to obtain explanations.']",20,10,1234
134,26,1288833841478750208,752184524121993216,Menelaos Kanakis,"Our paper ""Reparameterizing Convolutions for Incremental Multi-Task Learning without Task Interference"" has been accepted to #ECCV2020. We reparameterize the convs to eliminate task interference and allow for the incremental learning of new tasks. Paper: <LINK>",https://arxiv.org/abs/2007.12540,"Multi-task networks are commonly utilized to alleviate the need for a large number of highly specialized single-task networks. However, two common challenges in developing multi-task models are often overlooked in literature. First, enabling the model to be inherently incremental, continuously incorporating information from new tasks without forgetting the previously learned ones (incremental learning). Second, eliminating adverse interactions amongst tasks, which has been shown to significantly degrade the single-task performance in a multi-task setup (task interference). In this paper, we show that both can be achieved simply by reparameterizing the convolutions of standard neural network architectures into a non-trainable shared part (filter bank) and task-specific parts (modulators), where each modulator has a fraction of the filter bank parameters. Thus, our reparameterization enables the model to learn new tasks without adversely affecting the performance of existing ones. The results of our ablation study attest the efficacy of the proposed reparameterization. Moreover, our method achieves state-of-the-art on two challenging multi-task learning benchmarks, PASCAL-Context and NYUD, and also demonstrates superior incremental learning capability as compared to its close competitors. ","Reparameterizing Convolutions for Incremental Multi-Task Learning
  without Task Interference",1,"['Our paper ""Reparameterizing Convolutions for Incremental Multi-Task Learning without Task Interference"" has been accepted to #ECCV2020. We reparameterize the convs to eliminate task interference and allow for the incremental learning of new tasks.\nPaper: <LINK>']",20,07,261
135,49,1339752653681610753,1920417332,Ryan Glasser,Our new paper on the feasibility of #MachineLearning in experimental #quantum state reconstruction is on the arxiv! Fun using #ibmq quantum computer! <LINK> @slohani_ai @ProfTSearles @ArmyResearchLab @USArmy @Tulane @TulaneSSE @HowardUniv @IBMResearch @qiskit <LINK>,https://arxiv.org/abs/2012.09432,"We determine the resource scaling of machine learning-based quantum state reconstruction methods, in terms of inference and training, for systems of up to four qubits when constrained to pure states. Further, we examine system performance in the low-count regime, likely to be encountered in the tomography of high-dimensional systems. Finally, we implement our quantum state reconstruction method on an IBM Q quantum computer, and compare against both unconstrained and constrained MLE state reconstruction. ","On the experimental feasibility of quantum state reconstruction via
  machine learning",1,['Our new paper on the feasibility of #MachineLearning in experimental #quantum state reconstruction is on the arxiv!  Fun using  #ibmq quantum computer!\n\n<LINK>\n\n@slohani_ai @ProfTSearles @ArmyResearchLab @USArmy @Tulane @TulaneSSE @HowardUniv @IBMResearch @qiskit <LINK>'],20,12,266
136,33,1397840987913863169,1061375141274439685,Xabier Cid Vidal 🛰️,"So, when you publish at a certain rate per year the excitement of each new paper decreases to some extent… However, I’m super excited this time, we’re unleashing the full power of LHCb to probe Stealth New Physics! <LINK> (thread) It's been a while since several of us at LHCb realised that our wonderful detector could be exploited much more, providing sensitivity to physics beyond the SM in many unexpected ways. We encompassed all this signatures with the tag ""Stealth physics"" This refers to dynamics beyond the SM, not including searches that focus on energetic objects or precision measurements of known processes. Examples of Stealth physics include long-lived particles and light resonances produced very rarely or together with huge backgrounds So, @MartinoBorsato, Yuhsin Tsai, @CharlesTheVS, Jose Zurita and me came to the conclusion that we needed to make an effort to collect all the theory and experimental efforts in this regard in a single place. It's been quite a ride, but truly worth it! The first step was organizing a workshop in Santiago last year (<LINK>). It was the last one before the pandemics, and looking at the outcome, I'm convinced it was very useful, setting the stage for what it was about to come. Now we have our document in the arxiv (hopefully to be published soon), as a useful guide for theorists and experimentalists who want to tackle Stealth physics at LHCb. There are more ideas that could have probably been included, but I ensure you'll find plenty of exciting scientific ideas. It is a very nice effort, with a lot of work and a powerful list of authors behind it, which to be honest was a pleasure to coordinate on our side. Now, we really hope you enjoy the read!",https://arxiv.org/abs/2105.12668,"In this paper, we describe the potential of the LHCb experiment to detect Stealth physics. This refers to dynamics beyond the Standard Model that would elude searches that focus on energetic objects or precision measurements of known processes. Stealth signatures include long-lived particles and light resonances that are produced very rarely or together with overwhelming backgrounds. We will discuss why LHCb is equipped to discover this kind of physics at the Large Hadron Collider and provide examples of well-motivated theoretical models that can be probed with great detail at the experiment. ",Unleashing the full power of LHCb to probe Stealth New Physics,7,"['So, when you publish at a certain rate per year the excitement of each new paper decreases to some extent… However, I’m super excited this time, we’re unleashing the full power of LHCb to probe Stealth New Physics!\n<LINK>\n(thread)', 'It\'s been a while since several of us at LHCb realised that our wonderful detector could be exploited much more, providing sensitivity to physics beyond the SM in many unexpected ways. We encompassed all this signatures with the tag ""Stealth physics""', 'This refers to dynamics beyond the SM, not including searches that focus on energetic objects or precision measurements of known processes. Examples of Stealth physics include  long-lived particles and light resonances  produced very rarely or together with huge backgrounds', ""So, @MartinoBorsato, Yuhsin Tsai, @CharlesTheVS, Jose Zurita and me came to the conclusion that we needed to make an effort to collect all the theory and experimental efforts in this regard in a single place. It's been quite a ride, but truly worth it!"", ""The first step was organizing a workshop in Santiago last year (https://t.co/YVar3Fw8S3). It was the last one before the pandemics, and looking at the outcome, I'm convinced it was very useful, setting the stage for what it was about to come."", 'Now we have our document in the arxiv (hopefully to be published soon), as a useful guide for theorists and experimentalists who want to tackle Stealth physics at LHCb. There are more ideas that could have probably been included,', ""but I ensure you'll find plenty of exciting scientific ideas. It is a very nice effort, with a lot of work and a powerful list of authors behind it, which to be honest was a pleasure to coordinate on our side. Now, we really hope you enjoy the read!""]",21,05,1713
137,167,1133841519759093760,1091045790242533376,Mariya Toneva,"Excited to share joint work with Leila Wehbe on using brain recordings of people reading naturalistic text to interpret long-range context information in the intermediate layers of BERT, Transformer-XL, ELMo, and USE. We were most surprised to find..(1/2) <LINK> ..that removing the pretrained attention in the shallow BERT layers results in better brain predictions and applying this insight to syntactic NLP tasks improved performance by up to 8%. Very excited for this transfer of insight from language in the brain to language in machines!",https://arxiv.org/abs/1905.11833,"Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination. ","Interpreting and improving natural-language processing (in machines)
  with natural language-processing (in the brain)",2,"['Excited to share joint work with Leila Wehbe on using brain recordings of people reading naturalistic text to interpret long-range context information in the intermediate layers of BERT, Transformer-XL, ELMo, and USE. We were most surprised to find..(1/2) <LINK>', '..that removing the pretrained attention in the shallow BERT layers results in better brain predictions and applying this insight to syntactic NLP tasks improved performance by up to 8%. Very excited for this transfer of insight from language in the brain to language in machines!']",19,05,543
138,101,1127299092445421568,348637346,Diana Powell,"Can we measure total protoplanetary disk mass without assuming a tracer-to-H2 ratio? Check out our new paper on the locations of planet formation where we derive total gaseous surface densities for 7 protoplanetary disks using dust lines! <LINK> The disks in our sample have newly derived masses that are 9-27% of their host stellar mass, substantially larger than the minimum mass solar nebula! All are stable to gravitational collapse except for one which approaches the limit of Toomre-Q stability. These masses are determined independent of an assumed dust opacity! Check out how the new total surface densities compared to previously derived values and the minimum mass solar nebula! These are massive disks! <LINK> Our mass estimates are 2-15 times larger than estimates from integrated optically thin dust emission. In these models, the disks formed with an initial dust mass that is a factor of ∼10 greater than is presently observed. More dust mass at early times! Of the three disks in our sample with resolved CO line emission, the masses of HD 163296, AS 209, and TW Hya are roughly 3, 115, and 40 times more massive than estimates from CO respectively. More evidence that the CO story in disks is complicated! Our method of determining surface density using dust lines is robust even if particles form as aggregates and is useful even in the presence of dust substructure caused by pressure traps. <LINK> The low Toomre-Q values observed in this sample indicate that at least some disks do not accrete efficiently. <LINK>",https://arxiv.org/abs/1905.03252,"We present new determinations of disk surface density, independent of an assumed dust opacity, for a sample of 7 bright, diverse protoplanetary disks using measurements of disk dust lines. We develop a robust method for determining the location of dust lines by modeling disk interferometric visibilities at multiple wavelengths. The disks in our sample have newly derived masses that are 9-27% of their host stellar mass, substantially larger than the minimum mass solar nebula. All are stable to gravitational collapse except for one which approaches the limit of Toomre-Q stability. Our mass estimates are 2-15 times larger than estimates from integrated optically thin dust emission. We derive depleted dust-to-gas ratios with typical values of ~$10^{-3}$ in the outer disk. Using coagulation models we derive dust surface density profiles that are consistent with millimeter dust observations. In these models, the disks formed with an initial dust mass that is a factor of ~10 greater than is presently observed. Of the three disks in our sample with resolved CO line emission, the masses of HD 163296, AS 209, and TW Hya are roughly 3, 115, and 40 times more massive than estimates from CO respectively. This range indicates that CO depletion is not uniform across different disks and that dust is a more robust tracer of total disk mass. Our method of determining surface density using dust lines is robust even if particles form as aggregates and is useful even in the presence of dust substructure caused by pressure traps. The low Toomre-Q values observed in this sample indicate that at least some disks do not accrete efficiently. ","New Constraints From Dust Lines On The Surface Densities Of
  Protoplanetary Disks",8,"['Can we measure total protoplanetary disk mass without assuming a tracer-to-H2 ratio? Check out our new paper on the locations of planet formation where we derive total gaseous surface densities for 7 protoplanetary disks using dust lines! <LINK>', 'The disks in our sample have newly derived masses that are 9-27% of their host stellar mass, substantially larger than the minimum mass solar nebula! All are stable to gravitational collapse except for one which approaches the limit of Toomre-Q stability.', 'These masses are determined independent of an assumed dust opacity!', 'Check out how the new total surface densities compared to previously derived values and the minimum mass solar nebula! These are massive disks! https://t.co/d4PhLzwPsA', 'Our mass estimates are 2-15 times larger than estimates from integrated optically thin dust emission.  In these models, the disks formed with an initial dust mass that is a factor of ∼10 greater than is presently observed. More dust mass at early times!', 'Of the three disks in our sample with resolved CO line emission, the masses of HD 163296, AS 209, and TW Hya are roughly 3, 115, and 40 times more massive than estimates from CO respectively. More evidence that the CO story in disks is complicated!', 'Our method of determining surface density using dust lines is robust even if particles form as aggregates and is useful even in the presence of dust substructure caused by pressure traps. https://t.co/gmVGS7NNPx', 'The low Toomre-Q values observed in this sample indicate that at least some disks do not accrete efficiently. https://t.co/P9Vf2aj4jX']",19,05,1534
139,252,1318639882575597568,3067132687,Kiana Ehsani,"Most representation learning works these days focus on contrastive approaches. What if we considered human attention and muscle movements as a supervisory signal? Check out our new large-scale dataset and representation learning approach. Paper: <LINK> <LINK> Code: <LINK> Joint work with @danielgordon100, Tom Nguyen, @RoozbehMottaghi, and Ali Farhadi.",https://arxiv.org/abs/2010.08539,"Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ""muscly-supervised"" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: this https URL ","What Can You Learn from Your Muscles? Learning Visual Representation
  from Human Interactions",2,"['Most representation learning works these days focus on contrastive approaches. What if we considered human attention and muscle movements as a supervisory signal? Check out our new large-scale dataset and representation learning approach. \nPaper: <LINK> <LINK>', 'Code: https://t.co/R8BO5DYZj4\nJoint work with @danielgordon100, Tom Nguyen, @RoozbehMottaghi, and Ali Farhadi.']",20,10,353
140,19,1198784316639195136,256513537,Dr Chiara Mingarelli,"My new paper in collaboration with @AstroStephen, B. S. Sathyaprakash, and @farrwill. We show how Omega_gw is related between different gravitational wave experiments, and how to relate these Omega_gw measurements. SKA curve from @JeffreyHazboun's Hasasia <LINK> <LINK>",http://arxiv.org/abs/1911.09745,"In this paper we provide a comprehensive derivation of the energy density in the stochastic gravitational-wave background $\Omega_\mathrm{gw}(f)$, and show how this quantity is measured in ground-based detectors such as Laser Interferometer Gravitational-Wave Observatory (LIGO), space-based Laser Interferometer Space Antenna (LISA), and Pulsar Timing Arrays. By definition $\Omega_\mathrm{gw}(f) \propto S_h(f)$ -- the power spectral density (PSD) of the Fourier modes of the gravitational-wave background. However, this is often confused with the PSD of the strain signal, which we call $S_\mathrm{gw}(f)$, and is a detector-dependent quantity. This has led to confusing definitions of $\Omega_\mathrm{gw}(f)$ in the literature which differ by factors of up to 5 when written in a detector-dependent way. In addition to clarifying this confusion, formulas presented in this paper facilitate easy comparison of results from different detector groups, and how to convert from one measure of the strength of the background (or an upper limit) to another. Our codes are public and on GitHub. ",Understanding $\Omega_\mathrm{gw}(f)$ in Gravitational Wave Experiments,1,"[""My new paper in collaboration with @AstroStephen, B. S. Sathyaprakash, and  @farrwill. We show how Omega_gw is related between different gravitational wave experiments, and how to relate these Omega_gw measurements. SKA curve from @JeffreyHazboun's Hasasia <LINK> <LINK>""]",19,11,269
141,25,1386933153835663362,2503999452,Arnau Rios,"New paper today on #ArXiv <LINK> with @BindingBlocks colleagues @AleStyle81 @amromero92 @PhysicsatYork @UNCPhysics. We provide free, easy-to-use worksheets to develop #Outreach activities on #NeutronStars & discuss the #ExoticPhysics involved in these objects <LINK> Activity 1 (Minimum Mass) continues our past work on #LiquidDrop #BindingEnergy activities in <LINK> (published here <LINK>). Activity 2 (Maximum mass) uses an analytical model for the #Interior of #NeutronStars <LINK> We fill the gap in past literature by focusing on activities for #Alevel students. #Outreach work to discuss recent advances by @LIGO, #NICER & future #SpaceMissions. Limits of theoretical modelling can also be assessed. Watch this space for more, updated #worksheets! <LINK>",https://arxiv.org/abs/2104.12449,"We introduce two simple online activities to explore the physics of neutron stars. These provide an introduction to the basic properties of compact objects, like their masses and radii, for secondary school students. The first activity explores the idea of the minimum mass of a neutron star. It is directly linked to the concept of binding energy and follows on from our previous activities. The second activity focuses on the maximum mass of neutron stars using a solvable model of the neutron star interior. The activities are based on spreadsheets, provided as Supplementary Material, and can be easily adapted to different levels, age groups and discussion topics. In particular, these activities can naturally lead towards discussions on extrapolations and limits of theoretical models. ","From nuclei to neutron stars: simple binding energy computer modelling
  in the classroom (part 2)",3,"['New paper today on #ArXiv <LINK> with @BindingBlocks colleagues @AleStyle81 @amromero92 @PhysicsatYork @UNCPhysics. We provide free, easy-to-use worksheets to develop #Outreach activities on #NeutronStars &amp; discuss the #ExoticPhysics involved in these objects <LINK>', 'Activity 1 (Minimum Mass) continues our past work on #LiquidDrop #BindingEnergy activities in https://t.co/CkJd8ZM5M2 (published here https://t.co/NfTASN08p4). Activity 2 (Maximum mass) uses an analytical model for the #Interior of #NeutronStars https://t.co/KS37xWnuxF', 'We fill the gap in past literature by focusing on activities for #Alevel students. #Outreach work to discuss recent advances by @LIGO, #NICER &amp; future #SpaceMissions. Limits of theoretical modelling can also be assessed. Watch this space for more, updated #worksheets! https://t.co/9gXhcSArzw']",21,04,761
142,10,1399900430927233029,941131462744539137,Liangming Pan,"Hi all, our new #ACL2021 paper on ""Zero-shot Fact Verification by Claim Generation"" is now at <LINK> #NLProc. We propose a framework based on question generation to automatically generate (evidence, claim) pairs to train the fact verification model. <LINK>",https://arxiv.org/abs/2105.14682,"Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model's F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available. ",Zero-shot Fact Verification by Claim Generation,1,"['Hi all, our new #ACL2021 paper on ""Zero-shot Fact Verification by Claim Generation"" is now at <LINK> #NLProc. We propose a framework based on question generation to automatically generate (evidence, claim) pairs to train the fact verification model. <LINK>']",21,05,256
143,70,1184868134676828161,763033119356256256,Rishad Shafik,Temporal encoding offers a novel #PervasiveAI hardware design approach and a *significant leap* towards circuits with minimal power management/regulation. Read our new Royal Soc. Philo. Trans. A paper here: <LINK> #PervasiveAI #RealPowerCircuits @nclmicrosystems,https://arxiv.org/abs/1910.07492,"Neural Networks (NNs) are steering a new generation of artificial intelligence (AI) applications at the micro-edge. Examples include wireless sensors, wearables and cybernetic systems that collect data and process them to support real-world decisions and controls. For energy autonomy, these applications are typically powered by energy harvesters. As harvesters and other power sources which provide energy autonomy inevitably have power variations, the circuits need to robustly operate over a dynamic power envelope. In other words, the NN hardware needs to be able to function correctly under unpredictable and variable supply voltages. In this paper, we propose a novel NN design approach using the principle of pulse width modulation (PWM). PWM signals represent information with their duty cycle values which may be made independent of the voltages and frequencies of the carrier signals. We design a PWM-based perceptron which can serve as the fundamental building block for NNs, by using an entirely new method of realising arithmetic in the PWM domain. We analyse the proposed approach building from a 3x3 perceptron circuit to a complex multi-layer NN. Using handwritten character recognition as an exemplar of AI applications, we demonstrate the power elasticity, resilience and efficiency of the proposed NN design in the presence of functional and parametric variations including large voltage variations in the power supply. ","Neural Network Design for Energy-Autonomous AI Applications using
  Temporal Encoding",1,['Temporal encoding offers a novel #PervasiveAI hardware design approach and a *significant leap* towards circuits with minimal power management/regulation. Read our new Royal Soc. Philo. Trans. A paper here: <LINK>\n#PervasiveAI #RealPowerCircuits @nclmicrosystems'],19,10,262
144,121,1249636945719951360,797888987675365377,Tom Rainforth,"Our new paper <LINK> shows how we can do blind contact tracing at scale for COVID-19 *without* requiring people to install an app or giving governments access to tracking data. All credit goes to my collaborators @jk_fitzsimons @atulmantri91 @jansen_zhao and Rob <LINK> With @oblivious_AI we will shortly be deploying this in the wild in a city of 5mil+, shout @jk_fitzsimons for more details",https://arxiv.org/abs/2004.05116,"The current COVID-19 pandemic highlights the utility of contact tracing, when combined with case isolation and social distancing, as an important tool for mitigating the spread of a disease [1]. Contact tracing provides a mechanism of identifying individuals with a high likelihood of previous exposure to a contagious disease, allowing additional precautions to be put in place to prevent continued transmission. Here we consider a cryptographic approach to contact tracing based on secure two-party computation (2PC). We begin by considering the problem of comparing a set of location histories held by two parties to determine whether they have come within some threshold distance while at the same time maintaining the privacy of the location histories. We propose a solution to this problem using pre-shared keys, adapted from an equality testing protocol due to Ishai et al [2]. We discuss how this protocol can be used to maintain privacy within practical contact tracing scenarios, including both app-based approaches and approaches which leverage location history held by telecoms and internet service providers. We examine the efficiency of this approach and show that existing infrastructure is sufficient to support anonymised contact tracing at a national level. ","A note on blind contact tracing at scale with applications to the
  COVID-19 pandemic",2,"['Our new paper <LINK> shows how we can do blind contact tracing at scale for COVID-19 *without* requiring people to install an app or giving governments access to tracking data.  All credit goes to my collaborators @jk_fitzsimons @atulmantri91 @jansen_zhao and Rob <LINK>', 'With @oblivious_AI we will shortly be deploying this in the wild in a city of 5mil+, shout @jk_fitzsimons for more details']",20,04,392
145,62,1238442953284235270,1068545181576773632,Kenneth Brown,"Natalie Brown (@GTPhys), Andrew Cross (#IBMQ), and I posted a new paper on leakage errors in the surface code to the arXiv today. <LINK> @DukeEngineering #DukeQuantum Natalie recently defended her thesis on leakage errors. First, she compared whether it was better to have a magnetic field sensitive qubit or a leaky qubit using the standard depolarizing error model for the leaky qubit interaction. <LINK> Then Natalie, Mike Newman ,and I realized the physical model of how leaked states and qubit states interact for trapped ion gates is nicer than the standard depolarizing error model. <LINK> <LINK> What about the leakage interaction models for other physical systems? Natalie was visiting Andrew at IBM as part of the @NSF QISE-Net triplet program. @jaygambetta mentioned to Natalie that for cross-resonance two-qubit gates, one qubit is more likely to leak than the other. From her work on ions, Natalie knew that leakage on syndrome qubits was more damaging than leakage on data qubits because of how error spread. Taking advantage of the asymmetry of leakage after a cross-resonance gate, she could design improved gate compilations for the code. Again we see that the physical errors of the system inform the best choice for circuit compilation. For those allergic to device specific error models, Natalie went one step further and designed a new leakage reduction circuit that works for the standard leakage error model and saves gates by fixing leakage on syndrome and data qubits separately.",https://arxiv.org/abs/2003.05843,"Leakage is a particularly damaging error that occurs when a qubit leaves the defined computational subspace. Leakage errors limit the effectiveness of quantum error correcting codes by spreading additional errors to other qubits and corrupting syndrome measurements. The effects of leakage errors on the surface code has been studied in various contexts. However, the effects of a leaked data qubit versus a leaked ancilla qubit can be quite different. Here, we study the effects of data leakage and ancilla leakage separately. We show that data leakage is much less damaging. We show that the surface code maintains its distance in the presence of leakage by either confining leakage to data qubits or eliminating aniclla qubit leakage at the critical fault location. We also introduce new techniques for handling leakage by using gates with one-sided leakage and by mixing two types of leakage reducing circuits: one to handle data leakage and one to handle ancilla leakage. ",Critical faults of leakage errors on the surface code,7,"['Natalie Brown (@GTPhys), Andrew Cross (#IBMQ), and I  posted a new paper on leakage errors in the surface code to the arXiv today. <LINK>  @DukeEngineering  #DukeQuantum', 'Natalie recently defended her thesis on leakage errors. First, she compared whether it was better to have a magnetic field sensitive qubit or a leaky qubit using the standard depolarizing error model for the leaky qubit interaction. https://t.co/4pbNQzJeHN', 'Then Natalie, Mike Newman ,and I realized the physical model of how leaked states and qubit states interact for trapped ion gates is nicer than the standard depolarizing error model. https://t.co/8GtMy7jM05 https://t.co/Dc88XKpG6c', 'What about the leakage interaction models for other physical systems?  Natalie was visiting Andrew at IBM as part of the @NSF QISE-Net triplet program. @jaygambetta mentioned to Natalie that for cross-resonance two-qubit gates, one qubit is more likely to leak than the other.', 'From her work on ions, Natalie knew that leakage on syndrome qubits was more damaging than leakage on data qubits because of how error spread.  Taking advantage of the asymmetry of leakage after a cross-resonance gate, she could design improved gate compilations for the code.', 'Again we see that the physical errors of the system inform the best choice for circuit compilation.', 'For those allergic to device specific error models, Natalie went one step further and designed a new leakage reduction circuit that works for the standard leakage error model and saves gates by fixing leakage on syndrome and data qubits separately.']",20,03,1504
146,55,1384396211248013312,56468788,Ehsan Adeli,"Check our paper @CVPR 2021: Metadata Normalization (MDN), a new batch-level operation (end2end training) to correct the influence of metadata (#bias, #confounder, you name it) on feature distributions. W/ @drfeifei @jcniebles et al. <LINK> <LINK> <LINK> Code will be released soon: <LINK> Many thanks to the team from @StanfordAILab @StanfordSVL @StanfordMed, especially Mandy Lu and Qingyu Zhao, for this great work!",https://arxiv.org/abs/2104.09052,"Batch Normalization (BN) and its variants have delivered tremendous success in combating the covariate shift induced by the training step of deep learning methods. While these techniques normalize feature distributions by standardizing with batch statistics, they do not correct the influence on features from extraneous variables or multiple distributions. Such extra variables, referred to as metadata here, may create bias or confounding effects (e.g., race when classifying gender from face images). We introduce the Metadata Normalization (MDN) layer, a new batch-level operation which can be used end-to-end within the training framework, to correct the influence of metadata on feature distributions. MDN adopts a regression analysis technique traditionally used for preprocessing to remove (regress out) the metadata effects on model features during training. We utilize a metric based on distance correlation to quantify the distribution bias from the metadata and demonstrate that our method successfully removes metadata effects on four diverse settings: one synthetic, one 2D image, one video, and one 3D medical image dataset. ",Metadata Normalization,2,"['Check our paper @CVPR 2021: Metadata Normalization (MDN), a new batch-level operation (end2end training) to correct the influence of metadata (#bias, #confounder, you name it) on feature distributions. W/ @drfeifei @jcniebles et al.\n<LINK>\n<LINK> <LINK>', 'Code will be released soon: https://t.co/BSvhIFzbBg\nMany thanks to the team from @StanfordAILab @StanfordSVL @StanfordMed, especially Mandy Lu and Qingyu Zhao, for this great work!']",21,04,417
147,31,1020106065059328000,837133583558987776,Colin Raffel,"New paper w/ @D_Berthelot_ML Aurko Roy and @goodfellow_ian where we propose an adversarial regularizer for improving interpolation in autoencoders and measure whether it also improves representation learning performance. Paper <LINK>, code <LINK> <LINK>",http://arxiv.org/abs/1807.07543,"Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can ""interpolate"": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations. ","Understanding and Improving Interpolation in Autoencoders via an
  Adversarial Regularizer",1,"['New paper w/ @D_Berthelot_ML Aurko Roy and @goodfellow_ian where we propose an adversarial regularizer for improving interpolation in autoencoders and measure whether it also improves representation learning performance. Paper <LINK>, code <LINK> <LINK>']",18,07,253
148,215,1447942998419980290,380420289,Denizalp,"My forthcoming NeurIPS paper with my iconic advisor Amy Greenwald is up. We study min-max optimization problems with dependent feasible sets, which seems to have been first studied as ""Wald's maximin model"" and which we call min-max Stackelberg games. <LINK> We provide to the best of our knowledge the first polynomial-time algorithms to solve min-max Stackelberg games. The problem we study has many applications such as the training of neural networks for optimal auctions, and robust optimization. In our paper, we also show that the computation of competitive equilibria in Fisher markets is an instance of a min-max Stackelberg game. This allows us to uncover an equivalence between games and a large class of markets as well as methods to solve games and market dynamics. Share, comment, subscribe for more content.🥰 @sbmeti 🥰😘",https://arxiv.org/abs/2110.05192,"Min-max optimization problems (i.e., min-max games) have been attracting a great deal of attention because of their applicability to a wide range of machine learning problems. Although significant progress has been made recently, the literature to date has focused on games with independent strategy sets; little is known about solving games with dependent strategy sets, which can be characterized as min-max Stackelberg games. We introduce two first-order methods that solve a large class of convex-concave min-max Stackelberg games, and show that our methods converge in polynomial time. Min-max Stackelberg games were first studied by Wald, under the posthumous name of Wald's maximin model, a variant of which is the main paradigm used in robust optimization, which means that our methods can likewise solve many convex robust optimization problems. We observe that the computation of competitive equilibria in Fisher markets also comprises a min-max Stackelberg game. Further, we demonstrate the efficacy and efficiency of our algorithms in practice by computing competitive equilibria in Fisher markets with varying utility structures. Our experiments suggest potential ways to extend our theoretical results, by demonstrating how different smoothness properties can affect the convergence rate of our algorithms. ",Convex-Concave Min-Max Stackelberg Games,5,"['My forthcoming NeurIPS paper with my iconic advisor Amy Greenwald is up. We study min-max optimization problems with dependent feasible sets, which seems to have been first studied as ""Wald\'s maximin model"" and which we call min-max Stackelberg games.\n<LINK>', 'We provide to the best of our knowledge the first polynomial-time algorithms to solve min-max Stackelberg games. The problem we study has many applications such as the training of neural networks for optimal auctions, and robust optimization.', 'In our paper, we also show that the computation of competitive equilibria in Fisher markets is an instance of a min-max Stackelberg game. This allows us to uncover an equivalence between games and a large class of markets as well as methods to solve games and market dynamics.', 'Share, comment, subscribe for more content.🥰', '@sbmeti 🥰😘']",21,10,834
149,109,1392187171319205890,857151967055015937,Vincent Dutordoir,"New paper out where we show how to make a forward pass through a Deep GP equivalent to a ReLU DNN. Another step towards unifying DGPs and DNNs.  <LINK> @jameshensman and @lawrennd (2014) pointed out the strong similarity between DNNs and DGPs posteriors, but the missing ingredients were fast inference and GP basis functions that induce neural network style activation functions. <LINK> In Dutordoir et al. (2020), we published an important building block to accommodate for this: showing how zonal kernels on the hypersphere have a Mercer decomposition comprising spherical harmonics - giving rise to its RKHS. <LINK> In this paper, we use this RKHS to construct an interdomain inducing variable that leads to GP basis functions c(.) that are approximately identical to the activation functions in neural nets. Left: standard RBF basis functions Right: our ReLU basis functions <LINK> Stacking these GP layers on top of each other gives a Deep GP for which the approximate posterior mean is equivalent to a DNN - allowing us to initialise a DGP with a point estimate from training a normal DNN, and speeding up the overall training time. We provide a consistent way to initialise a Deep GP by using a DNN as opposed to a single layer as in Sun et al. (2020). We also focus on activation functions that are commonly used in DNNs and show the necessity of using the ArcCosine kernel to avoid a mismatch in the spectrum. <LINK> Very fortunate to have worked on this with @jameshensman, @markvanderwilk, @carlhenrikek, @ZoubinGhahrama1 and @NicolasDurrande @jordigraumo Thanks Jordi",https://arxiv.org/abs/2105.04504,"Neural networks and Gaussian processes are complementary in their strengths and weaknesses. Having a better understanding of their relationship comes with the promise to make each method benefit from the strengths of the other. In this work, we establish an equivalence between the forward passes of neural networks and (deep) sparse Gaussian process models. The theory we develop is based on interpreting activation functions as interdomain inducing features through a rigorous analysis of the interplay between activation functions and kernels. This results in models that can either be seen as neural networks with improved uncertainty prediction or deep Gaussian processes with increased prediction accuracy. These claims are supported by experimental results on regression and classification datasets. ",Deep Neural Networks as Point Estimates for Deep Gaussian Processes,8,"['New paper out where we show how to make a forward pass through a Deep GP equivalent to a ReLU DNN. Another step towards unifying DGPs and DNNs.\n \n<LINK>', '@jameshensman and @lawrennd (2014) pointed out the strong similarity between DNNs and DGPs posteriors, but the missing ingredients were fast inference and GP basis functions that induce neural network style activation functions. https://t.co/xPyCnsaUeB', 'In Dutordoir et al. (2020), we published an important building block to accommodate for this: showing how zonal kernels on the hypersphere have a Mercer decomposition comprising spherical harmonics - giving rise to its RKHS.\n\nhttps://t.co/WZ77rhatLH', 'In this paper, we use this RKHS to construct an interdomain inducing variable that leads to GP basis functions c(.) that are approximately identical to the activation functions in neural nets.\n\nLeft: standard RBF basis functions\nRight: our ReLU basis functions https://t.co/awPZPZTQM6', 'Stacking these GP layers on top of each other gives a Deep GP for which the approximate posterior mean is equivalent to a DNN - allowing us to initialise a DGP with a point estimate from training a normal DNN, and speeding up the overall training time.', 'We provide a consistent way to initialise a Deep GP by using a DNN as opposed to a single layer as in Sun et al. (2020). We also focus on activation functions that are commonly used in DNNs and show the necessity of using the ArcCosine kernel to avoid a mismatch in the spectrum. https://t.co/tn68s10ni4', 'Very fortunate to have worked on this with @jameshensman, @markvanderwilk, @carlhenrikek, @ZoubinGhahrama1 and @NicolasDurrande', '@jordigraumo Thanks Jordi']",21,05,1580
150,119,1489295648826564611,883039700,Lenka Zdeborova,"Dear 2nd referee of our ICML 2019 submission. We finally managed to answer your question about the relation between the Saad&Solla analysis of two-layer neural networks and the one referred to as mean-field/hydrodynamic limit. Please see our new paper: <LINK> <LINK> With @rodsveiga @_brloureiro Ludovic Stephan, and @KrzakalaF In the figure the axed are scaling exponents of the learning and of the network width with dimension.",https://arxiv.org/abs/2202.00293,"Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad & Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates. ","Phase diagram of Stochastic Gradient Descent in high-dimensional
  two-layer neural networks",2,"['Dear 2nd referee of our ICML 2019 submission. We finally managed to answer your question about the relation between the Saad&amp;Solla analysis of two-layer neural networks and the one referred to as mean-field/hydrodynamic limit. Please see our new paper: <LINK> <LINK>', 'With @rodsveiga @_brloureiro Ludovic Stephan, and @KrzakalaF  In the figure the axed are scaling exponents of the learning and of the network width with dimension.']",22,02,429
151,11,1058007590569984001,24443979,Dan Stowell,"New paper from us - how to do time-domain audio source separation, efficiently, using Bayesian signal processing. <LINK> Pablo's method outperforms NMF as well as a previous time-domain method - and the source-code is online too. #machinelistening @c4dm",https://arxiv.org/abs/1810.12679,"Gaussian process (GP) audio source separation is a time-domain approach that circumvents the inherent phase approximation issue of spectrogram based methods. Furthermore, through its kernel, GPs elegantly incorporate prior knowledge about the sources into the separation model. Despite these compelling advantages, the computational complexity of GP inference scales cubically with the number of audio samples. As a result, source separation GP models have been restricted to the analysis of short audio frames. We introduce an efficient application of GPs to time-domain audio source separation, without compromising performance. For this purpose, we used GP regression, together with spectral mixture kernels, and variational sparse GPs. We compared our method with LD-PSDTF (positive semi-definite tensor factorization), KL-NMF (Kullback-Leibler non-negative matrix factorization), and IS-NMF (Itakura-Saito NMF). Results show that the proposed method outperforms these techniques. ","Sparse Gaussian Process Audio Source Separation Using Spectrum Priors in
  the Time-Domain",1,"[""New paper from us - how to do time-domain audio source separation, efficiently, using Bayesian signal processing. <LINK>  Pablo's method outperforms NMF as well as a previous time-domain method - and the source-code is online too. #machinelistening @c4dm""]",18,10,253
152,123,1124133641733509120,616196664,Steven Touzard,"Check out our new paper from @Yale_QI on how to implement gates that preserve the noise bias of stabilized Schrodinger cat states <LINK> @matt_reagor @Yale_QI Yeah that's a great paper ! To be fair, these gates are science fiction for now, but very doable ! These engineered dissipation are really fun to play with, much less fun to write in my thesis manuscript right now.",https://arxiv.org/abs/1905.00450,"The code capacity threshold for error correction using qubits which exhibit asymmetric or biased noise channels is known to be much higher than with qubits without such structured noise. However, it is unclear how much this improvement persists when realistic circuit level noise is taken into account. This is because implementations of gates which do not commute with the dominant error un-bias the noise channel. In particular, a native bias-preserving controlled-NOT (CX) gate, which is an essential ingredient of stabilizer codes, is not possible in strictly two-level systems. Here we overcome the challenge of implementing a bias-preserving CX gate by using stabilized cat qubits in driven nonlinear oscillators. The physical noise channel of this qubit is biased towards phase-flips, which increase linearly with the size of the cat, while bit-flips are exponentially suppressed with cat size. Remarkably, the error channel of this native CX gate between two such cat qubits is also dominated by phase-flips, while bit-flips remain exponentially suppressed. This CX gate relies on the topological phase that arises from the rotation of the cat qubit in phase space. The availability of bias-preserving CX gates opens a path towards fault-tolerant codes tailored to biased-noise cat qubits with high threshold and low overhead. As an example, we analyze a scheme for concatenated error correction using cat qubits. We find that the availability of CX gates with moderately sized cat qubits, having mean photon number <10, improves a rigorous lower bound on the fault-tolerance threshold by a factor of two and decreases the overhead in logical Clifford operations by a factor of 5. We expect these estimates to improve significantly with further optimization and with direct use of other codes such as topological codes tailored to biased noise. ",Bias-preserving gates with stabilized cat qubits,2,"['Check out our new paper from @Yale_QI on how to implement gates that preserve the noise bias of stabilized Schrodinger cat states\n<LINK>', ""@matt_reagor @Yale_QI Yeah that's a great paper ! To be fair, these gates are science fiction for now, but very doable !\nThese engineered dissipation are really fun to play with, much less fun to write in my thesis manuscript right now.""]",19,05,373
153,119,1403282459656077313,4249537197,Christian Wolf,New paper: we show that transfer from Oracle input to real improves VQA reasoning when a strong link is established through program supervision: empirical results + analysis of sample complexity. Work with @CorentK @antigregory @moezbac and M. Nadri. <LINK> <LINK>,https://arxiv.org/abs/2106.05597,"Methods for Visual Question Anwering (VQA) are notorious for leveraging dataset biases rather than performing reasoning, hindering generalization. It has been recently shown that better reasoning patterns emerge in attention layers of a state-of-the-art VQA model when they are trained on perfect (oracle) visual inputs. This provides evidence that deep neural networks can learn to reason when training conditions are favorable enough. However, transferring this learned knowledge to deployable models is a challenge, as much of it is lost during the transfer. We propose a method for knowledge transfer based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. We also demonstrate the effectiveness of this approach experimentally on the GQA dataset and show its complementarity to BERT-like self-supervised pre-training. ",Supervising the Transfer of Reasoning Patterns in VQA,1,['New paper: we show that transfer from Oracle input to real improves VQA reasoning when a strong link is established through program supervision: empirical results + analysis of sample complexity.\n\nWork with @CorentK  @antigregory  @moezbac  and M. Nadri. \n<LINK> <LINK>'],21,06,264
154,134,1369203344229679106,869896064802934788,Jan Rybizki,New paper led by @bucktobias6: We implement flexible yield tables into cosmological hydrodynamical simulations. The plot shows a variety of different yield table and GCE assumptions tested. We can now produce these for virtually all elements. Check it out: <LINK> <LINK>,https://arxiv.org/abs/2103.03884,"With the advent of large spectroscopic surveys the amount of high quality chemo-dynamical data in the Milky Way (MW) increased tremendously. Accurately and correctly capturing and explaining the detailed features in the high-quality observational data is notoriously difficult for state-of-the-art numerical models. In order to keep up with the quantity and quality of observational datasets, improved prescriptions for galactic chemical evolution need to be incorporated into the simulations. Here we present a new, flexible, time resolved chemical enrichment model for cosmological simulations. Our model allows to easily change a number of stellar physics parameters such as the shape of the initial mass function (IMF), stellar lifetimes, chemical yields or SN Ia delay times. We implement our model into the Gasoline2 code and perform a series of cosmological simulations varying a number of key parameters, foremost evaluating different stellar yield sets for massive stars from the literature. We find that total metallicity, total iron abundance and gas phase oxygen abundance are robust predictions from different yield sets and in agreement with observational relations. On the other hand, individual element abundances, especially $\alpha$-elements show significant differences across different yield sets and none of our models can simultaneously match constraints on the dwarf and MW mass scale. This offers a unique way of observationally constraining model parameters. For MW mass galaxies we find for most yield tables tested in this work a bimodality in the $[\alpha$/Fe] vs. [Fe/H] plane of rather low intrinsic scatter potentially in tension with the observed abundance scatter. ","The challenge of simultaneously matching the observed diversity of
  chemical abundance patterns in cosmological hydrodynamical simulations",1,['New paper led by @bucktobias6: We implement flexible yield tables into cosmological hydrodynamical simulations. The plot shows a variety of different yield table and GCE assumptions tested. We can now produce these for virtually all elements. Check it out: <LINK> <LINK>'],21,03,270
155,9,1212404920219045888,223440240,Nathan Kallus,"To bring in the new year @XiaojieMao+Masa+I just posted a paper on Localized Debiased ML for estimating causal quantities using ML methods when hi-dim nuisances depend on estimand <LINK> In this thread I'll explain why this prob is so important and what we did 1/ Causal inference on quantile treatment effects is important in assessing the risk to the population to be treated. When dealing with rich confounders/relationships we need to use ML to adjust them. But existing ML-based approaches for efficient estimation in this case, 2/ including DML and TMLE, would require we learn a whole conditional distribution function nonparametrically, which is practically challenging for ML -- especially compared to standard classification/regression, which is all we'd need for efficient average effect estimation. 3/ Instead LDML uses an initial bad guess (eg IPW) to localize the estimation. Via a new 3-way cross-fold method and a finer analysis, we can ensure oracle-like behavior for our estimator without ever learning such complicated nuisances: just plain ML classification/regression. 4/ Sneak peak: under a Fréchet-deriv (stronger than Gateaux deriv used in DML) orthogonality (holds for quantile est + other cases), the oracle estimation equation is asymp equivalent to one where nuisances are evaluated at true parameter value. LDML targets this new formulation. 5/",https://arxiv.org/abs/1912.12945,"We consider the efficient estimation of a low-dimensional parameter in an estimating equation involving high-dimensional nuisances that depend on the parameter of interest. An important example is the (local) quantile treatment effect ((L)QTE) in causal inference, for which the efficient estimating equation involves as a nuisance the covariate-conditional cumulative distribution function evaluated at the quantile to be estimated. Debiased machine learning (DML) is a data-splitting approach to address the need to estimate nuisances using flexible machine learning methods that may not satisfy strong metric entropy conditions, but applying it to problems with parameter-dependent nuisances is impractical. For (L)QTE estimation, DML requires we learn the whole conditional cumulative distribution function, conditioned on potentially high-dimensional covariates, which is far more challenging than the standard supervised regression task in machine learning. We instead propose localized debiased machine learning (LDML), a new data-splitting approach that avoids this burdensome step and needs only estimate the nuisances at a single initial rough guess for the parameter. For (L)QTE estimation, this involves just learning two binary regression (i.e., classification) models, for which many standard, time-tested machine learning methods exist, and the initial rough guess may be given by inverse propensity weighting. We prove that under lax rate conditions on nuisances, our estimator has the same favorable asymptotic behavior as the infeasible oracle estimator that solves the estimating equation with the unknown true nuisance functions. Thus, our proposed approach uniquely enables practically-feasible and theoretically-grounded efficient estimation of important quantities in causal inference such as (L)QTEs and in other coarsened data settings. ","Localized Debiased Machine Learning: Efficient Inference on Quantile
  Treatment Effects and Beyond",5,"[""To bring in the new year @XiaojieMao+Masa+I just posted a paper on Localized Debiased ML for estimating causal quantities using ML methods when hi-dim nuisances depend on estimand <LINK> In this thread I'll explain why this prob is so important and what we did 1/"", 'Causal inference on quantile treatment effects is important in assessing the risk to the population to be treated. When dealing with rich confounders/relationships we need to use ML to adjust them. But existing ML-based approaches for efficient estimation in this case, 2/', ""including DML and TMLE, would require we learn a whole conditional distribution function nonparametrically, which is practically challenging for ML -- especially compared to standard classification/regression, which is all we'd need for efficient average effect estimation. 3/"", 'Instead LDML uses an initial bad guess (eg IPW) to localize the estimation. Via a new 3-way cross-fold method and a finer analysis, we can ensure oracle-like behavior for our estimator without ever learning such complicated nuisances: just plain ML classification/regression. 4/', 'Sneak peak: under a Fréchet-deriv (stronger than Gateaux deriv used in DML) orthogonality (holds for quantile est + other cases), the oracle estimation equation is asymp equivalent to one where nuisances are evaluated at true parameter value. LDML targets this new formulation. 5/']",19,12,1373
156,119,1502050684040105990,429494244,M Charity,Hi y'all! Our paper for Baba is Y'all v2 is out on arXiv! You can read about the new site and the user study results here: <LINK> We talk about the new user-friendly updates to the site as well as the more developed back-end AI assistant to help users make better levels more efficiently. We also conducted a user study and received over 75 survey responses that described their experience with the site. <LINK> You can also make your own levels with the help of an AI assistant! Contribute to the ever-growing database here: <LINK> We've also released the Keke AI Competition which uses levels directly from the site! You can make your AI solver for the Baba is Y'all levels: <LINK>,https://arxiv.org/abs/2203.02035,"This paper describes a new version of the mixed-initiative collaborative level designing system: Baba is Y'all, as well as the results of a user study on the system. Baba is Y'all is a prototype for AI-assisted game design in collaboration with others. The updated version includes a more user-friendly interface, a better level-evolver and recommendation system, and extended site features. The system was evaluated via a user study where participants were required to play a previously submitted level from the site and then create their own levels using the editor. They reported on their individual process creating the level and their overall experience interacting with the site. The results have shown both the benefits and limitations of a mixed-initiative system and how it can help with creating a diversity of `Baba is You' levels that are both human and AI designed while maintaining their quality. ","Baba is Y'all 2.0: Design and Investigation of a Collaborative
  Mixed-Initiative System",4,"[""Hi y'all! Our paper for Baba is Y'all v2 is out on arXiv! You can read about the new site and the user study results here: <LINK>"", 'We talk about the new user-friendly updates to the site as well as the more developed back-end AI assistant to help users make better levels more efficiently. We also conducted a user study and received over 75 survey responses that described their experience with the site. https://t.co/jv2wnYkvma', 'You can also make your own levels with the help of an AI assistant! Contribute to the ever-growing database here: https://t.co/5K3GHaPnbi', ""We've also released the Keke AI Competition which uses levels directly from the site! You can make your AI solver for the Baba is Y'all levels: https://t.co/OLcVJsXPO5""]",22,03,683
157,134,1332005066941882370,475760077,Dr Sarah Casewell,"New paper alert! A through analysis of a massive, inflated brown dwarf in a close orbit with a hot subdwarf! Long term monitoring with the fabulous ULTRACAM instrument has meant we actually observe the orbit shrinking as the binary synchronises. <LINK> @nickcasewell A giant star swallowed its brown dwarf companion, but the brown dwarf survived, it’s a massive brown dwarf, but physically bigger than it should be. It now orbits a 25000 degree not-quite white dwarf in 2.5 hrs. But, we have observed this over 10 yrs and the orbit is shrinking! @nickcasewell Pretty much! You have a while to wait, but in 2.2 billion years the brown dwarf will be close enough for the subdwarf to start pulling mass off it and it will become what’s called a cataclysmic variable. And then it’s basically doomed. @nickcasewell Could be a while ...",https://arxiv.org/abs/2011.10013,"Subdwarf B stars are core-helium burning stars located on the extreme horizontal branch. Extensive mass loss on the red giant branch is necessary to form them. It has been proposed that substellar companions could lead to the required mass-loss when they are engulfed in the envelope of the red giant star. J08205+0008 was the first example of a hot subdwarf star with a close, substellar companion candidate to be found. Here we perform an in-depth re-analysis of this important system with much higher quality data allowing additional analysis methods. From the higher resolution spectra obtained with ESO-VLT/XSHOOTER we derive the chemical abundances of the hot subdwarf as well as its rotational velocity. Using the { it Gaia} parallax and a fit to the spectral energy distribution in the secondary eclipse, tight constraints to the radius of the hot subdwarf are derived. From a long-term photometric campaign we detected a significant period decrease of $-3.2(8)\cdot 10^{-12} \,\rm dd^{-1}$. This can be explained by the non-synchronised hot subdwarf star being spun up by tidal interactions forcing it to become synchronised. From the rate of period decrease we could derive the synchronisation timescale to be 4 Myr, much smaller than the lifetime on EHB. By combining all different methods we could constrain the hot subdwarf to a mass of $0.39-0.50\,\rm M_\odot$ and a radius of $R_{\rm sdB}=0.194\pm0.008\,\rm R_\odot$, and the companion to $0.061-0.071\rm\,M_\odot$ with a radius of $R_{\rm comp}=0.092 \pm 0.005\,\rm R_\odot$, below the hydrogen burning limit. We therefore confirm that the companion is most likely a massive brown dwarf. ","A quantitative in-depth analysis of the prototype sdB+BD system SDSS
  J08205+0008 revisited in the Gaia era",4,"['New paper alert! A through analysis of a massive, inflated brown dwarf in a close orbit with a hot subdwarf! Long term monitoring with the fabulous ULTRACAM instrument has meant we actually observe the orbit shrinking as the binary synchronises.\n\n<LINK>', '@nickcasewell A giant star swallowed its brown dwarf companion, but the brown dwarf survived, it’s a massive brown dwarf, but physically bigger than it should be. It now orbits a 25000 degree not-quite white dwarf in 2.5 hrs. But, we have observed this over 10 yrs and the orbit is shrinking!', '@nickcasewell Pretty much! You have a while to wait, but in 2.2 billion years the brown dwarf will be close enough for the subdwarf to start pulling mass off it and it will become what’s called a cataclysmic variable. And then it’s basically doomed.', '@nickcasewell Could be a while ...']",20,11,830
158,122,1422508607271317507,120325394,Aswin P Vijayan,"Paper day! Our new paper is on arxiv today, FLARES III: The properties of massive galaxies at cosmic dawn. It was really fun to write this paper, hope some of you do find it interesting. Also last chapter in my PhD thesis. <LINK> Here is a short summary of the work: We post-process the FLARES (<LINK>) galaxies with the radiative transfer code SKIRT to produce full SEDs. We look at the LFs, IRX-beta relation and luminosity-weighted dust temperatures in the EoR. We find reasonable agreement of the IR LF of the galaxies, but underpredict the number densities of bright IR galaxies at z=5. <LINK> Most galaxies in FLARES, follow the local starburst IRX-beta relation. <LINK> Peak dust temperature (Tpeak) correlates strongly with sSFR. All luminosity-weighted dust temperatures increases towards high-z, with the slope of the Tpeak-z relation showing higher slope than previous observational and theoretical fits. <LINK> Thanks to all the co-authors for their help. @stewilkins @chrisclovell More soon to come from FLARES. Stay tuned!",https://arxiv.org/abs/2108.00830,"Using the First Light And Reionisation Epoch Simulations (\textsc{Flares}) we explore the dust driven properties of massive high-redshift galaxies at $z\in[5,10]$. By post-processing the galaxy sample using the radiative transfer code \textsc{skirt} we obtain the full spectral energy distribution. We explore the resultant luminosity functions, IRX-$\beta$ relations as well as the luminosity-weighted dust temperatures in the Epoch of Reionisation (EoR). We find that most of our results are in agreement with the current set of observations, but under-predict the number densities of bright IR galaxies, which are extremely biased towards the most overdense regions. We see that the \textsc{Flares} IRX-$\beta$ relation (for $5\le z\le8$) predominantly follows the local starburst relation. The IRX shows an increase with stellar mass, plateauing at the high-mass end ($\sim10^{10}$M$_{\odot}$) and shows no evolution in the median normalisation with redshift. We also look at the dependence of the peak dust temperature ($T_{\mathrm{peak}}$) on various galaxy properties including the stellar mass, IR luminosity and sSFR, finding the correlation to be strongest with sSFR. The luminosity-weighted dust temperatures increase towards higher redshifts, with the slope of the $T_{\mathrm{peak}}$ - redshift relation showing a higher slope than the lower redshift relations obtained from previous observational and theoretical works. The results from \textsc{Flares}, which is able to provide a better statistical sample of high-redshift galaxies compared to other simulations, provides a distinct vantage point for the high-redshift Universe. ","First Light And Reionisation Epoch Simulations (FLARES) III: The
  properties of massive dusty galaxies at cosmic dawn",6,"['Paper day! Our new paper is on arxiv today, FLARES III: The properties of massive galaxies at cosmic dawn. It was really fun to write this paper, hope some of you do find it interesting. Also last chapter in my PhD thesis.\n<LINK>', 'Here is a short summary of the work:\nWe post-process the FLARES (https://t.co/hqNLzsaR62) galaxies with the radiative transfer code SKIRT to produce full SEDs. We look at the LFs, IRX-beta relation and luminosity-weighted dust temperatures in the EoR.', 'We find reasonable agreement of the IR LF of the galaxies, but underpredict the number densities of bright IR galaxies at z=5. https://t.co/5dA9yXs70i', 'Most galaxies in FLARES, follow the local starburst IRX-beta relation. https://t.co/NPpbxF39eW', 'Peak dust temperature (Tpeak) correlates strongly with sSFR. All luminosity-weighted dust temperatures increases towards high-z, with the slope of the Tpeak-z relation showing higher slope than previous observational and theoretical fits. https://t.co/xU5WEjMtjz', 'Thanks to all the co-authors for their help. @stewilkins @chrisclovell \nMore soon to come from FLARES. Stay tuned!']",21,08,1036
159,153,1280558715930845185,2596589880,Nikolaus Kriegeskorte,"New paper: @diedrichsenlab, @EvaBerlot, @marieke_mur, and Heiko Schuett introduce the *unbiased distance correlation* (UDC), an exciting new criterion for RSA model comparison that extends distance correlation & linear CKA to unbiased distance estimates. <LINK> <LINK>",https://arxiv.org/abs/2007.02789,"Representational similarity analysis (RSA) tests models of brain computation by investigating how neural activity patterns reflect experimental conditions. Instead of predicting activity patterns directly, the models predict the geometry of the representation, as defined by the representational dissimilarity matrix (RDM), which captures to what extent experimental conditions are associated with similar or dissimilar activity patterns. RSA therefore first quantifies the representational geometry by calculating a dissimilarity measure for each pair of conditions, and then compares the estimated representational dissimilarities to those predicted by each model. Here we address two central challenges of RSA: First, dissimilarity measures such as the Euclidean, Mahalanobis, and correlation distance, are biased by measurement noise, which can lead to incorrect inferences. Unbiased dissimilarity estimates can be obtained by crossvalidation, at the price of increased variance. Second, the pairwise dissimilarity estimates are not statistically independent, and ignoring this dependency makes model comparison statistically suboptimal. We present an analytical expression for the mean and (co)variance of both biased and unbiased estimators of the squared Euclidean and Mahalanobis distance, allowing us to quantify the bias-variance trade-off. We also use the analytical expression of the covariance of the dissimilarity estimates to whiten the RDM estimation errors. This results in a new criterion for RDM similarity, the whitened unbiased RDM cosine similarity (WUC), which allows for near-optimal model selection combined with robustness to correlated measurement noise. ","Comparing representational geometries using whitened
  unbiased-distance-matrix similarity",1,"['New paper: @diedrichsenlab, @EvaBerlot, @marieke_mur, and Heiko Schuett introduce the *unbiased distance correlation* (UDC), an exciting new criterion for RSA model comparison that extends distance correlation &amp; linear CKA to unbiased distance estimates. <LINK> <LINK>']",20,07,268
160,175,1458280561286397957,1065179437568712704,Michael Girard,"Interestingly, the central retinal vessel trunk (and its branches) appears to be a stronger biomarker for #glaucoma than RNFL thickness! We propose a paradigm where the major retinal vessels may act as a protective skeleton for the optic disc. @arxiv: <LINK>. <LINK>",https://arxiv.org/abs/2111.03997,"Purpose: To assess whether the three-dimensional (3D) structural configuration of the central retinal vessel trunk and its branches (CRVT&B) could be used as a diagnostic marker for glaucoma. Method: We trained a deep learning network to automatically segment the CRVT&B from the B-scans of the optical coherence tomography (OCT) volume of the optic nerve head (ONH). Subsequently, two different approaches were used for glaucoma diagnosis using the structural configuration of the CRVT&B as extracted from the OCT volumes. In the first approach, we aimed to provide a diagnosis using only 3D CNN and the 3D structure of the CRVT&B. For the second approach, we projected the 3D structure of the CRVT&B orthographically onto three planes to obtain 2D images, and then a 2D CNN was used for diagnosis. The segmentation accuracy was evaluated using the Dice coefficient, whereas the diagnostic accuracy was assessed using the area under the receiver operating characteristic curves (AUC). The diagnostic performance of the CRVT&B was also compared with that of retinal nerve fiber layer (RNFL) thickness. Results: Our segmentation network was able to efficiently segment retinal blood vessels from OCT scans. On a test set, we achieved a Dice coefficient of 0.81\pm0.07. The 3D and 2D diagnostic networks were able to differentiate glaucoma from non-glaucoma subjects with accuracies of 82.7% and 83.3%, respectively. The corresponding AUCs for CRVT&B were 0.89 and 0.90, higher than those obtained with RNFL thickness alone. Conclusions: Our work demonstrated that the diagnostic power of the CRVT&B is superior to that of a gold-standard glaucoma parameter, i.e., RNFL thickness. Our work also suggested that the major retinal blood vessels form a skeleton -- the configuration of which may be representative of major ONH structural changes as typically observed with the development and progression of glaucoma. ","The Three-Dimensional Structural Configuration of the Central Retinal
  Vessel Trunk and Branches as a Glaucoma Biomarker",1,"['Interestingly, the central retinal vessel trunk (and its branches) appears to be a stronger biomarker for #glaucoma than RNFL thickness! We propose a paradigm where the major retinal vessels may act as a protective skeleton for the optic disc. @arxiv: <LINK>. <LINK>']",21,11,266
161,22,1322258921420017664,2235411914,Surya Ganguli,Our new #neurips2020 paper combines geometry and dynamics to reveal a rapid universal chaotic to stable transition in deep learning dynamics where in a few epochs the final loss basin is determined and the neural tangent kernel rapidly evolves and improves <LINK> <LINK>,http://arxiv.org/abs/2010.15110,"In suitably initialized wide networks, small learning rates transform deep neural networks (DNNs) into neural tangent kernel (NTK) machines, whose training dynamics is well-approximated by a linear weight expansion of the network at initialization. Standard training, however, diverges from its linearization in ways that are poorly understood. We study the relationship between the training dynamics of nonlinear deep networks, the geometry of the loss landscape, and the time evolution of a data-dependent NTK. We do so through a large-scale phenomenological analysis of training, synthesizing diverse measures characterizing loss landscape geometry and NTK dynamics. In multiple neural architectures and datasets, we find these diverse measures evolve in a highly correlated manner, revealing a universal picture of the deep learning process. In this picture, deep network training exhibits a highly chaotic rapid initial transient that within 2 to 3 epochs determines the final linearly connected basin of low loss containing the end point of training. During this chaotic transient, the NTK changes rapidly, learning useful features from the training data that enables it to outperform the standard initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid chaotic transient, the NTK changes at constant velocity, and its performance matches that of full network training in 15% to 45% of training time. Overall, our analysis reveals a striking correlation between a diverse set of metrics over training time, governed by a rapid chaotic to stable transition in the first few epochs, that together poses challenges and opportunities for the development of more accurate theories of deep learning. ","Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the Neural Tangent Kernel",1,['Our new #neurips2020 paper combines geometry and dynamics to reveal a rapid universal chaotic to stable transition in deep learning dynamics where in a few epochs the final loss basin is determined and the neural tangent kernel rapidly evolves and improves <LINK> <LINK>'],20,10,270
162,153,1489321348237119496,577537524,Pete Florence,"New 🤖 paper led by the awesome @WiYoungsun! <LINK> The paper is essentially ""using the Force* to deform neural fields"" (In this case, DeepSDF-style representations.) A cool thing here is that robots can have tactile (e.g., force-torque) sensing... <LINK> So we can do perception 👀 that fuses shape knowledge together with tactile sensing 👏, even for deformable objects. This can help do things like predict full shape deformation state, with only partial information. <LINK> Note these are *multi-modal (multi-sensory)*--input neural fields, capable of fusing both: - visual data (in this case point cloud data), and - tactile data (in this case force data) <LINK> If you've ever thought about how good humans are at fusing shape information and touch sensing, including of objects that are deforming... this work takes some initial steps towards that direction. <LINK> Just accepted to ICRA. Super happy to see @WiYoungsun's hard work pay off on the project. Led out of @NimaFazeli7's lab at Michigan, where they know a lot about tactile stuff :) . @andyzengtweets and I were lucky to be able to help out. <LINK> Also @WiYoungsun just made her Twitter account like yesterday.... if you're looking for cool new researchers to follow, she is awesome. * yes not actually the Force from Star Wars. (Btw, is it just me or are somehow Episodes 5 and 6 of Boba Fett just way better than the previous episodes?)",https://arxiv.org/abs/2202.00868,"Deformable object manipulation requires computationally efficient representations that are compatible with robotic sensing modalities. In this paper, we present VIRDO:an implicit, multi-modal, and continuous representation for deformable-elastic objects. VIRDO operates directly on visual (point cloud) and tactile (reaction forces) modalities and learns rich latent embeddings of contact locations and forces to predict object deformations subject to external contacts.Here, we demonstrate VIRDOs ability to: i) produce high-fidelity cross-modal reconstructions with dense unsupervised correspondences, ii) generalize to unseen contact formations,and iii) state-estimation with partial visio-tactile feedback ",VIRDO: Visio-tactile Implicit Representations of Deformable Objects,7,"['New 🤖 paper led by the awesome @WiYoungsun!  <LINK>\n\nThe paper is essentially ""using the Force* to deform neural fields"" (In this case, DeepSDF-style representations.)\n\nA cool thing here is that robots can have tactile (e.g., force-torque) sensing... <LINK>', 'So we can do perception 👀 that fuses shape knowledge together with tactile sensing 👏, even for deformable objects.\n\nThis can help do things like predict full shape deformation state, with only partial information. https://t.co/VAk2mACO24', 'Note these are *multi-modal (multi-sensory)*--input neural fields, capable of fusing both:\n- visual data (in this case point cloud data), and\n- tactile data (in this case force data) https://t.co/oqD23UGWYG', ""If you've ever thought about how good humans are at fusing shape information and touch sensing, including of objects that are deforming...\n\nthis work takes some initial steps towards that direction. https://t.co/JRvIXSHZpV"", ""Just accepted to ICRA.\n\nSuper happy to see @WiYoungsun's hard work pay off on the project.\n\nLed out of @NimaFazeli7's lab at Michigan, where they know a lot about tactile stuff :) .  @andyzengtweets and I were lucky to be able to help out. https://t.co/7ITlV88Q5K"", ""Also @WiYoungsun just made her Twitter account like yesterday.... if you're looking for cool new researchers to follow, she is awesome."", '* yes not actually the Force from Star Wars.\n\n(Btw, is it just me or are somehow Episodes 5 and 6 of Boba Fett just way better than the previous episodes?)']",22,02,1404
163,66,1070133813320200194,508659617,Adrien Gaidon,"And here is our new paper: Learning to Fuse Things and Stuff <LINK> Actually an improved version from the ECCV challenge, called TASCNet where we propose a Things and Stuff Consistency loss for end-to-end panoptic segmentation with a single model! <LINK>",https://arxiv.org/abs/1812.01192,"We propose an end-to-end learning approach for panoptic segmentation, a novel task unifying instance (things) and semantic (stuff) segmentation. Our model, TASCNet, uses feature maps from a shared backbone network to predict in a single feed-forward pass both things and stuff segmentations. We explicitly constrain these two output distributions through a global things and stuff binary mask to enforce cross-task consistency. Our proposed unified network is competitive with the state of the art on several benchmarks for panoptic segmentation as well as on the individual semantic and instance segmentation tasks. ",Learning to Fuse Things and Stuff,1,"['And here is our new paper: Learning to Fuse Things and Stuff <LINK> Actually an improved version from the ECCV challenge, called TASCNet where we propose a Things and Stuff Consistency loss for end-to-end panoptic segmentation with a single model! <LINK>']",18,12,254
164,131,1436210551739215872,4807828837,Vishal Upendran,"☀️ New paper with @dktripathi accepted in ApJ, and out on arxiv !☀️ <LINK> In this work, we study the properties of the C II 1334 line which forms in the upper chromosphere, as a function of photospheric magnetic flux density (|B|) in Cor hole & Quiet Sun (1/5) We find CHs to show reduced intensity, excess blueshifts (only blueshifted pixels), excess redshifts (only redshifted pixels), and excess width over QS for similar |B|. These properties are also found to increase with |B| for both CH and QS. (2/5) We also find that the spectral profiles are flatter, and more skewed than a Gaussian, with a dependence on |B|. However, the ""flatness"" and ""skewness"" are consistent between CH and QS. Why are these important? (3/5) This analysis tells us that the fundamental processes giving rise to spectral profiles are similar in both CH and QS. The difference seems to arise mainly due to the magnetic topology, and that's why gross chromospheric features look similar in both CH and QS. (4/5) Hope you enjoy reading this work. However, this is just a part of the work - the remaining contains some interesting implications of these observations. A glimpse on what is yet to come may be known to those who checked out my poster at the IIA 50 conference. Stay tuned! (5/5) @ydnad0 @dktripathi Thank you @ydnad0 !",https://arxiv.org/abs/2109.04287,"Coronal Holes (CHs) have subdued intensity and net blueshifts when compared to Quiet Sun (QS) at coronal temperatures. At transition region temperatures, such differences are obtained for regions with identical photospheric absolute magnetic flux density ($\vert$B$\vert$). In this work, we use spectroscopic measurements of the \car 1334~{\AA} line from Interface Region Imaging Spectrograph (IRIS), formed at chromospheric temperatures, to investigate the intensity, Doppler shift, line width, skew, and excess kurtosis variations with $\vert$B$\vert$. We find the intensity, Doppler shift, and line widths to increase with $\vert$B$\vert$ for CHs and QS. The CHs show deficit in intensity and excess total widths over QS for regions with identical $\vert$B$\vert$. For pixels with only upflows, CHs show excess upflows over QS, while for pixels with only downflows, CHs show excess downflows over QS that cease to exist at $\vert$B$\vert$ $\le$ 40. Finally, the spectral profiles are found to be more skewed and flatter than a Gaussian, with no difference between CH and QS. These results are important in understanding the heating of the atmosphere in CH and QS, including solar wind formation, and provide further constraints on the modeling of the solar atmosphere. ","Properties of the C II 1334 {\AA} line in Coronal Hole and Quiet Sun as
  observed by IRIS",6,"['☀️ New paper with @dktripathi accepted in ApJ, and out on arxiv !☀️\n<LINK>\n\nIn this work, we study the properties of the C II 1334 line which forms in the upper chromosphere, as a function of photospheric magnetic flux density (|B|) in Cor hole &amp; Quiet Sun (1/5)', 'We find CHs to show reduced intensity, excess blueshifts (only blueshifted pixels), excess redshifts (only redshifted pixels), and excess width over QS for similar |B|. These properties are also found to increase with |B| for both CH and QS. (2/5)', 'We also find that the spectral profiles are flatter, and more skewed than a Gaussian, with a dependence on |B|. However, the ""flatness"" and ""skewness"" are consistent between CH and QS. Why are these important? (3/5)', ""This analysis tells us that the fundamental processes giving rise to spectral profiles are similar in both CH and QS. The difference seems to arise mainly due to the magnetic topology, and that's why gross chromospheric features look similar in both CH and QS. (4/5)"", 'Hope you enjoy reading this work. However, this is just a part of the work  - the remaining contains some interesting implications of these observations. A glimpse on what is yet to come may be known to those who checked out my poster at the IIA 50 conference. Stay tuned! (5/5)', '@ydnad0 @dktripathi Thank you @ydnad0 !']",21,09,1310
165,101,1415554652804947974,2377407248,Daniel Whiteson,"New paper! Measuring how well a smartphone camera can detect cosmic muons! <LINK> Led by Jeff Swaney and Mike Mulhearn, with @cshimmin What? Your phone can see particles? When a muon passed through your phone camera, it frees up electrons, just like when a photon does. So the camera sees that pixel as on. If you cover the lens and put the phone in a muon beam, presto, you see tracks! <LINK> We wanted to do much more: to turn the network of smartphones into a world-wide detector for cosmic particles. <LINK> To do that, we needed to measure how often the phone sees or misses a particle. So we put some phones between two scintillators: <LINK> And measured how often we spotted the muon in the phone. Along the way, we had to reverse engineer how the phone turns electrons into digitized values, so we could measure the pure response: <LINK> This will help us understand how well a network of phones can act as a global detector (<LINK>) TLDR: smartphones are about 70-80% efficient at detecting muons! @y0b1byte Yes, if the flux is very high, but that's not a concern for cosmic rays. More of an issue is that the performance degrades if phone is kept at high temperature for too long. @Antony_Clements @SeamusBlackley @cshimmin Maybe! A lot of it is noise from badly-behaving pixels. We had to filter out the hot pixels to get a reliable muon signal.",https://arxiv.org/abs/2107.06332,"A measurement of the efficiency of CMOS sensors in smartphone cameras to cosmic ray muons is presented. A coincidence in external scintillators indicates the passage of a cosmic ray muon, allowing the measurement of the efficiency of the CMOS sensor. The observed flux is consistent with well-established values, and efficiencies are presented as a function of the number of photo-electrons collected from the CMOS silicon photodiode pixels. These efficiencies are vital to understanding the feasibility of large-scale smartphone networks operating as air-shower observatories. ",Measurement of Smartphone Sensor Efficiency to Cosmic Ray Muons,9,"['New paper!\n\nMeasuring how well a smartphone camera can detect cosmic muons!\n\n<LINK>\n\nLed by Jeff Swaney and Mike Mulhearn, with @cshimmin', 'What? Your phone can see particles?\n\nWhen a muon passed through your phone camera, it frees up electrons, just like when a photon does. So the camera sees that pixel as on. If you cover the lens and put the phone in a muon beam, presto, you see tracks! https://t.co/l8AwyMsQlp', 'We wanted to do much more: to turn the network of smartphones into a world-wide detector for cosmic particles. https://t.co/SFtUZQwVdy', 'To do that, we needed to measure how often the phone sees or misses a particle. So we put some phones between two scintillators: https://t.co/W1FCNyvUu9', 'And measured how often we spotted the muon in the phone.\n\nAlong the way, we had to reverse engineer how the phone turns electrons into digitized values, so we could measure the pure response: https://t.co/cMOPXF6drO', 'This will help us understand how well a network of phones can act as a global detector (https://t.co/FWCchA1MX5)', 'TLDR: smartphones are about 70-80% efficient at detecting muons!', ""@y0b1byte Yes, if the flux is very high, but that's not a concern for cosmic rays.  More of an issue is that the performance degrades if phone is kept at high temperature for too long."", '@Antony_Clements @SeamusBlackley @cshimmin Maybe!  A lot of it is noise from badly-behaving pixels. We had to filter out the hot pixels to get a reliable muon signal.']",21,07,1356
166,136,1301792837214797827,561899047,Aki Vehtari,"Akash Dhaka, @AleexCatalina, @Michael_riis, @MansMeg, @jhhhuggins, and I have a new paper ""Robust, Accurate Stochastic Optimization for Variational Inference"" <LINK> <LINK> tl;dr We combine Polyak–Ruppert averaging with MCMC convergence diagnostics to make stochastic optimization in variational inference more robust or get a warning when it performs badly. These help to make automated use of VI safer in probabilistic programming frameworks. <LINK> Many VI methods use stochastic optimization either due to using random mini-batches of data or Monte Carlo to estimate expectations of the divergences. For example. For example, autodiff VI (ADVI) in Stan has stochastic target and gradients due to the latter. To fulfill Robbins-Monroe condition of reaching eventually the optimum, the step size is usually gradually decreased. Although this guarantees asymptotic convergence, it may take unfeasible amount of time and the last iteration in finite time can be far from the optimum. Under certain conditions, stochastic optimization with a fixed step size converges to a finite variance stationary process around the optimum. Average of the iterations converges towards the optimum faster. This is an old but under-used idea, aka Polyak–Ruppert averaging. Recently iterate averaging has been used also with name stochastic weight averaging (SWA) in context of deep learning. What we add is 1) a diagnostic for detecting when we have reached stationarity and can start averaging, and 2) a standard error estimate to decide when we can stop averaging or give up if the standard error is not decreasing (indicating violation of conditions). The diagnostics are familiar from MCMC convergence literature (e.g. Rhat, MCSE, and autocorrelation) and VI diagnostics literature (e.g. Pareto k). @lauretig Oops, forgot that. The repo will be public next week.",https://arxiv.org/abs/2009.00666,"We consider the problem of fitting variational posterior approximations using stochastic optimization methods. The performance of these approximations depends on (1) how well the variational family matches the true posterior distribution,(2) the choice of divergence, and (3) the optimization of the variational objective. We show that even in the best-case scenario when the exact posterior belongs to the assumed variational family, common stochastic optimization methods lead to poor variational approximations if the problem dimension is moderately large. We also demonstrate that these methods are not robust across diverse model types. Motivated by these findings, we develop a more robust and accurate stochastic optimization framework by viewing the underlying optimization algorithm as producing a Markov chain. Our approach is theoretically motivated and includes a diagnostic for convergence and a novel stopping rule, both of which are robust to noisy evaluations of the objective function. We show empirically that the proposed framework works well on a diverse set of models: it can automatically detect stochastic optimization failure or inaccurate variational approximation ","Robust, Accurate Stochastic Optimization for Variational Inference",9,"['Akash Dhaka, @AleexCatalina, @Michael_riis, @MansMeg, @jhhhuggins, and I have a new paper ""Robust, Accurate Stochastic Optimization for Variational Inference"" <LINK> <LINK>', 'tl;dr We combine Polyak–Ruppert averaging with MCMC convergence diagnostics to make stochastic optimization in variational inference more robust or get a warning when it performs badly. These help to make automated use of VI safer in probabilistic programming frameworks. https://t.co/3RUKPPdzfs', 'Many VI methods use stochastic optimization either due to using random mini-batches of data or Monte Carlo to estimate expectations of the divergences. For example. For example, autodiff VI (ADVI) in Stan has stochastic target and gradients due to the latter.', 'To fulfill Robbins-Monroe condition of reaching eventually the optimum, the step size is usually gradually decreased. Although this guarantees asymptotic convergence, it may take unfeasible amount of time and the last iteration in finite time can be far from the optimum.', 'Under certain conditions, stochastic optimization with a fixed step size converges to a finite variance stationary process around the optimum. Average of the iterations converges towards the optimum faster. This is an old but under-used idea, aka Polyak–Ruppert averaging.', 'Recently iterate averaging has been used also with name stochastic weight averaging (SWA) in context of deep learning.', 'What we add is 1) a diagnostic for detecting when we have reached stationarity and can start averaging, and 2) a standard error estimate to decide when we can stop averaging or give up if the standard error is not decreasing (indicating violation of conditions).', 'The diagnostics are familiar from MCMC convergence literature (e.g. Rhat, MCSE, and autocorrelation) and VI diagnostics literature (e.g. Pareto k).', '@lauretig Oops, forgot that. The repo will be public next week.']",20,09,1850
167,58,1329545666184441856,735269593443274753,Dr. Jennifer Burt,"New paper out today by me and the extended Lick-Carnegie Exoplanet team!! Three new planets orbiting two nearby stars -- two around HD 216520 & one around HD 190007) <LINK> These discoveries build on some early suggestions of Keplerian signals from the HIRES data release in Butler et al. 2017, which we then followed up with the Automated Planet Finder (my favorite robotic RV facility &lt;3) <LINK> All three of the new planets are in the super-Earth/sub-Neptune mass regime (10-16 Earth masses), and two of them are on relatively short periods: ~12 days for HD 190007 b, and 35 days for HD 216520 b. HD 216520 c, meanwhile, is further out with an orbital period of 154 days <LINK> It's a little surprising that these systems don't seem to contain a similar planets on nearby orbits... <LINK> B/c out of the 136 Kepler and K2 sub-Neptune mass exoplanets that have orbital periods less than 100 days, 85% are in multi-planet systems. And HD 190007 b & HD 216520 b sit in a similar mass/period regime <LINK> But our injection/recovery analysis suggests that there's not a whole lot of room left for additional planets to hide - certainly not within the factor of 2 in period/mass that we'd expect for a ""similar"" planet (which excludes HD 216520 c as well) <LINK> We also provided some updated orbital parameters to GJ 686 b and HD 180617 b using data from the APF & PFS -- all in good agreement with the original discoveries by Affers+2019, Lalitha+2019, and Kaminski+2018 <LINK> So all in all, we've got a nice set of new and improved planets, making use of data from both HIRES and the APF, that we're super excited to add to the RV planet landscape! <LINK>",https://arxiv.org/abs/2011.08867,"Analysis of new precision radial velocity (RV) measurements from the Lick Automated Planet Finder (APF) and Keck HIRES have yielded the discovery of three new exoplanet candidates orbiting two nearby K dwarfs not previously reported to have companions (HD 190007 & HD 216520). We also report new velocities from both the APF and the Planet Finder Spectrograph (PFS) for the previously reported planet host stars GJ 686 and HD 180617 and update the corresponding exoplanet orbital models. Of the newly discovered planets, HD 190007 b has a period of 11.72 days, an RV semi-amplitude of K = 5.64$\pm$0.55 m s$^{-1}$, a minimum mass of 16.46$\pm$1.66 $\rm M_{\oplus}$, and orbits the slightly metal-rich, active K4 dwarf star HD 190007 (d = 12.7 pc). HD 216520 b has an orbital period of 35.45 days, an RV semi-amplitude of K = 2.28$\pm$0.20 m s$^{-1}$, and a minimum mass of 10.26$\pm$0.99 $\rm M_{\oplus}$, while HD 216520 c has an orbital period of P = 154.43 days, an RV semi-amplitude of K = 1.29$\pm0.22$ m s$^{-1}$, and a minimum mass of 9.44$\pm$1.63 $\rm M_{\oplus}$. Both of these planets orbit the slightly metal-poor, inactive K0 dwarf star HD 216520 (d = 19.6 pc). We find that our updated best fit models for HD 180617 b and GJ 686 b are in good agreement with the previously published results. For HD 180617 b we obtain an orbital period of 105.91 days, an RV semi-amplitude of K = 2.696$\pm$0.22 m s$^{-1}$, and a minimum mass of 2.214$\pm$1.05 $\rm M_{\oplus}$. For GJ 686 b we find the orbital period to be 15.53 days, the RV semi-amplitude to be K = 3.00$\pm$0.18 m s$^{-1}$, and the minimum mass to be 6.624$\pm$0.432 $\rm M_{\oplus}$. Using an injection-recovery exercise, we find that HD 190007 b and HD 216520 b are unlikely to have additional planets with masses and orbital periods within a factor of two, in marked contrast to $\sim$85\% of planets in this mass and period range found with Kepler. ","A collage of small planets from the Lick Carnegie Exoplanet Survey :
  Exploring the super-Earth and sub-Neptune mass regime",8,"['New paper out today by me and the extended Lick-Carnegie Exoplanet team!! Three new planets orbiting two nearby stars -- two around HD 216520 &amp; one around HD 190007)  <LINK>', 'These discoveries  build on some early suggestions of Keplerian signals from the HIRES data release in Butler et al. 2017, which we then followed up with the Automated Planet Finder (my favorite robotic RV facility &lt;3) https://t.co/O1L9DFv7kq', 'All three of the new planets are in the super-Earth/sub-Neptune mass regime (10-16 Earth masses), and two of them are on relatively short periods: ~12 days for HD 190007 b, and 35 days for HD 216520 b. HD 216520 c, meanwhile, is further out with an orbital period of 154 days https://t.co/yDN1z6ogAW', ""It's a little surprising that these systems don't seem to contain a similar planets on nearby orbits... https://t.co/WHwegmxKNz"", 'B/c out of the 136 Kepler and K2 sub-Neptune mass exoplanets that have orbital periods less than 100 days, 85% are in multi-planet systems. And HD 190007 b &amp; HD 216520 b sit in a similar mass/period regime https://t.co/cBykgxXSyx', 'But our injection/recovery analysis suggests that there\'s not a whole lot of room left for additional planets to hide - certainly not within the factor of 2 in period/mass that we\'d expect for a ""similar"" planet (which excludes HD 216520 c as well) https://t.co/KGRymLzEls', 'We also provided some updated orbital parameters to GJ 686 b and HD 180617 b using data from the APF &amp; PFS -- all in good agreement with the original discoveries by Affers+2019, Lalitha+2019, and Kaminski+2018 https://t.co/1jbQOM3iWR', ""So all in all, we've got a nice set of new and improved planets, making use of data from both HIRES and the APF, that we're super excited to add to the RV planet landscape! https://t.co/t1aWxG70Do""]",20,11,1660
168,128,1428739477770489863,630560519,Dr Johanna Vos,"Delighted that our new paper, led by grad student Mary Anne Limbach, has been accepted for publication in ApJL! We explore the prospects of detecting exomoons orbiting isolated, planetary-mass companions <LINK> 🧵 Based on gas giant moons in our own Solar System as well as a variety of moon formation simulations, we expect that moons around gas giants are common. But the question we asked is - could we actually detect them using the transit method? We calculate the probability that at least one transiting companion exists in a sample. For a sample of 10 targets, the probability that at least one companion transits is 10% for Mercury-Sun analogs, 68% for Europa-Jupiter analogs, and 85% for Io-Jupiter analogs. Not bad! <LINK> Furthermore, we find that the transit probability of a habitable zone companion is at a maximum for isolated planetary-mass objects and brown dwarfs. Brown dwarfs and planetary-mass objects are in the sweet spot to host habitable, transiting companions. <LINK> We also looked at the photometric precision required to detect an exomoon using current facilities. More than 50% of the currently known isolated planetary-mass objects are bright enough to detect Titan or Ganymede-sized moons during one transit with JWST. <LINK> Finally, we note that cloud-driven variability is going to make it hard to draw conclusions on transit signals! As an example we explore a mysterious dimming event in the light curve of 2M1119. This signal may be due to a 1.7 R_earth exomoon or evolving variability. <LINK> There is a LOT more in the paper so please take a look! <LINK>",https://arxiv.org/abs/2108.08323,"All-sky imaging surveys have identified several dozen isolated planetary-mass objects (IPMOs), far away from any star. Here, we examine the prospects for detecting transiting moons around these objects. We expect transiting moons to be common, occurring around 10-15% of IPMOs, given that close-orbiting moons have a high geometric transit probability and are expected to be a common outcome of giant planet formation. IPMOs offer an advantage over other directly imaged planets in that high-contrast imaging is not necessary to detect the photometric transit signal. For at least 30 (>50%) of the currently known IPMOs, observations of a single transit with the James Webb Space Telescope would have low enough forecasted noise levels to allow for the detection of an Io-like or Titan-like moon. Intrinsic variability of the IPMOs will be an obstacle. Using archival time-series photometry of IPMOs with the Spitzer Space Telescope as a proof-of-concept, we found evidence for a fading event of 2MASS J1119-1137 AB that might have been caused by intrinsic variability, but is also consistent with a single transit of a habitable-zone 1.7$R_\oplus$ exomoon. Although the interpretation of this particular event is inconclusive, the characteristics of the data and the candidate signal suggest that Earth-sized habitable-zone exomoons around IPMOs are detectable with existing instrumentation. ",On the Detection of Exomoons Transiting Isolated Planetary-Mass Objects,7,"['Delighted that our new paper, led by grad student Mary Anne Limbach, has been accepted for publication in ApJL! We explore the prospects of detecting exomoons orbiting isolated, planetary-mass companions <LINK> 🧵', 'Based on gas giant moons in our own Solar System as well as a variety of moon formation simulations, we expect that moons around gas giants are common. But the question we asked is - could we actually detect them using the transit method?', 'We calculate the probability that at least one transiting companion exists in a sample. For a sample of 10 targets, the probability that at least one companion transits is 10% for Mercury-Sun analogs, 68% for Europa-Jupiter analogs, and 85% for Io-Jupiter analogs. Not bad! https://t.co/4SAGa6rmVS', 'Furthermore, we find that the transit probability of a habitable zone companion is at a maximum for isolated planetary-mass objects and brown dwarfs. Brown dwarfs and planetary-mass objects are in the sweet spot to host habitable, transiting companions. https://t.co/dxH4uOZ8uk', 'We also looked at the photometric precision required to detect an exomoon using current facilities. More than 50% of the currently known isolated planetary-mass objects are bright enough to detect Titan or Ganymede-sized moons during one transit with JWST. https://t.co/JcaYntF2wi', 'Finally, we note that cloud-driven variability is going to make it hard to draw conclusions on transit signals! As an example we explore a mysterious dimming event in the light curve of 2M1119. This signal may be due to a 1.7 R_earth exomoon or evolving variability. https://t.co/UFrxYyoZXa', 'There is a LOT more in the paper so please take a look! https://t.co/PgC4IvLFQa']",21,08,1594
169,136,1281486695515529216,994520594489204737,Jack Hare,"New arXiv paper, ""An Imaging Refractometer for Density Fluctuation Measurements in High Energy Density Plasmas"" <LINK>. This new diagnostic analyses a laser beam passing through #turbulent #Plasma, recording ray locations and ray deflections in orthogonal axes. <LINK> The idea is that the spectrum of ray deflections relates to the spectrum of density fluctuations within the turbulent plasma. These fluctuations were studied with digital Fourier transforms of shadowgraphy and schlieren images, but our new technique resolves much smaller scales. <LINK> We compare our diagnostic to existing methods using ray tracing techniques, and present data from an experiment which shows the exquisite detail we can capture. Next up: going beyond the power spectrum and looking for signatures of intermittency in magnetohydrodynamic turbulence. <LINK> Thanks to everyone who has helped with this paper, it's been a long journey. The data is from 2016, so it's taken a pandemic to finally analyse it. Along the way we've developed a really nice ray-tracing/ray-transfer-matrix code which we can use for all our optical diagnostics.",https://arxiv.org/abs/2007.04682,"We report on a recently developed laser-probing diagnostic which allows direct measurements of ray-deflection angles in one axis, whilst retaining imaging capabilities in the other axis. This allows us to measure the spectrum of angular deflections from a laser beam which passes though a turbulent high-energy-density plasma. This spectrum contains information about the density fluctuations within the plasma, which deflect the probing laser over a range of angles. %The principle of this diagnostic is described, along with our specific experimental realisation. We create synthetic diagnostics using ray-tracing to compare this new diagnostic with standard shadowgraphy and schlieren imaging approaches, which demonstrates the enhanced sensitivity of this new diagnostic over standard techniques. We present experimental data from turbulence behind a reverse shock in a plasma and demonstrate that this technique can measure angular deflections between 0.06 and 34 mrad, corresponding to a dynamic range of over 500. ","An Imaging Refractometer for Density Fluctuation Measurements in High
  Energy Density Plasmas",4,"['New arXiv paper, ""An Imaging Refractometer for Density Fluctuation Measurements in High Energy Density Plasmas"" <LINK>. This new diagnostic analyses a laser beam passing through #turbulent #Plasma, recording ray locations and ray deflections in orthogonal axes. <LINK>', 'The idea is that the spectrum of ray deflections relates to the spectrum of density fluctuations within the turbulent plasma. These fluctuations were studied with digital Fourier transforms of shadowgraphy and schlieren images, but our new technique resolves much smaller scales. https://t.co/CoHkahN2Cb', 'We compare our diagnostic to existing methods using ray tracing techniques, and present data from an experiment which shows the exquisite detail we can capture. Next up: going beyond the power spectrum and looking for signatures of intermittency in magnetohydrodynamic turbulence. https://t.co/uvxT9JaJW1', ""Thanks to everyone who has helped with this paper, it's been a long journey. The data is from 2016, so it's taken a pandemic to finally analyse it. Along the way we've developed a really nice ray-tracing/ray-transfer-matrix code which we can use for all our optical diagnostics.""]",20,07,1122
170,87,1183695437586452480,1107358308,Jack Turner,"Excited to release new paper w. @mpatacch on Gaussian Processes (GPs) for few-shot learning (with deep kernel transfer).   📝paper: <LINK>  💾 code: <LINK> (in @PyTorch and GPyTorch)  (1/3) GPs are a natural fit for few-shot because they work well in low data regime and have built-in uncertainty estimation. We apply this on standard few-shot benchmarks via deep kernel learning (@andrewgwils), using output feature maps of NN as input to GP (2/3) E.g. my favourite plot. Trained on random periodic head pose trajectories,Feature Transfer (FT) and GP are tested on flat rotation with Cutout noise. FT overfits, GP predicts correctly & acknowledges uncertainty on the noisy point. Results on std few-shot benchmarks in paper. <LINK>",https://arxiv.org/abs/1910.05199,"Recently, different machine learning methods have been introduced to tackle the challenging few-shot learning scenario that is, learning from a small labeled dataset related to a specific task. Common approaches have taken the form of meta-learning: learning to learn on the new problem given the old. Following the recognition that meta-learning is implementing learning in a multi-level model, we present a Bayesian treatment for the meta-learning inner loop through the use of deep kernels. As a result we can learn a kernel that transfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach has many advantages: is straightforward to implement as a single optimizer, provides uncertainty quantification, and does not require estimation of task-specific parameters. We empirically demonstrate that DKT outperforms several state-of-the-art algorithms in few-shot classification, and is the state of the art for cross-domain adaptation and regression. We conclude that complex meta-learning routines can be replaced by a simpler Bayesian model without loss of accuracy. ",Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels,3,"['Excited to release new paper w. @mpatacch on Gaussian Processes (GPs) for few-shot learning (with deep kernel transfer). \n\n   📝paper: <LINK>\n   💾 code: <LINK> (in @PyTorch and GPyTorch)  \n\n(1/3)', 'GPs are a natural fit for few-shot because they work well in low data regime and have built-in uncertainty estimation. We apply this on standard few-shot benchmarks via deep kernel learning (@andrewgwils), using output feature maps of NN as input to GP\n\n(2/3)', 'E.g. my favourite plot. Trained on random periodic head pose trajectories,Feature Transfer (FT) and GP are tested on flat rotation with Cutout noise. FT overfits, GP predicts correctly &amp; acknowledges uncertainty on the noisy point.  Results on std few-shot benchmarks in paper. https://t.co/UtPoX1k4yB']",19,10,730
171,151,1402184147762925569,481539448,Richard Alexander,"New paper, led by Alison Young, with @bec_nealon, @cwalshastrochem, @Alice_Centauri, and Christophe Pinte. Alison used hydrodynamics, chemistry, and radiative transfer models, to investigate how chemical changes can trace warps in planet-forming discs. <LINK> Protoplanetary discs (unlike discs around black holes or compact objects) are mainly heated by light from the star. So when a disc becomes warped, the misaligned (inner) part casts shadows that change the (outer) disc temperature - the disc has a ""hot side"" and a ""cold side"". <LINK> These azimuthal temperature variations affect the disc chemistry, and Alison showed that they cause variations in the chemical abundances. The chemistry therefore traces the warped disc structure - the dominant molecules differ on the ""bright side"" vs the ""dark side"". <LINK> These abundance and temperature variations are then observable in line emission, and we expect to see clear asymmetries in the emission maps made with ALMA. This gives us a new way to probe the structure of planet-forming discs.😀 <LINK> This is just a quick summary of a huge amount of work though, and there is *way* more in the paper than I can cover in a few tweets. So click through for all the details: <LINK>",https://arxiv.org/abs/2106.02660,"Circumstellar discs may become warped or broken into distinct planes if there is a stellar or planetary companion with an orbit that is misaligned with respect to the disc. There is mounting observational evidence for protoplanetary discs with misaligned inner discs and warps that may be caused by such interactions with a previously undetected companion, giving us a tantalising indication of possible planets forming there. Hydrodynamical and radiative transfer models indicate that the temperature varies azimuthally in warped discs due to the variable angle at which the disc surface faces the star and this impacts the disc chemistry. We perform chemical modelling based on a hydrodynamical model of a protoplanetary disc with an embedded planet orbiting at a 12$^{\circ}$ inclination to the disc. Even for this small misalignment, abundances of species including CO and HCO$^+$ vary azimuthally and this results in detectable azimuthal variations in submillimetre line emission. Azimuthal variations in line emission may therefore indicate the presence of an unseen embedded companion. Nonaxisymmetric chemical abundances should be considered when interpreting molecular line maps of warped or shadowed protoplanetary discs. ",Chemical signatures of a warped protoplanetary disc,5,"['New paper, led by Alison Young, with @bec_nealon, @cwalshastrochem, @Alice_Centauri, and Christophe Pinte. Alison used hydrodynamics, chemistry, and radiative transfer models, to investigate how chemical changes can trace warps in planet-forming discs.\n\n<LINK>', 'Protoplanetary discs (unlike discs around black holes or compact objects) are mainly heated by light from the star. So when a disc becomes warped, the misaligned (inner) part casts shadows that change the (outer) disc temperature - the disc has a ""hot side"" and a ""cold side"". https://t.co/pt7rVGNeWW', 'These azimuthal temperature variations affect the disc chemistry, and Alison showed that they cause variations in the chemical abundances. The chemistry therefore traces the warped disc structure - the dominant molecules differ on the ""bright side"" vs the ""dark side"". https://t.co/OFHafebw5n', 'These abundance and temperature variations are then observable in line emission, and we expect to see clear asymmetries in the emission maps made with ALMA. This gives us a new way to probe the structure of planet-forming discs.😀 https://t.co/8rXyUoL01S', 'This is just a quick summary of a huge amount of work though, and there is *way* more in the paper than I can cover in a few tweets. So click through for all the details:\nhttps://t.co/4E2q7qqSNh']",21,06,1234
172,130,1390237906384609280,2999702157,Anton Ilderton,"New paper on the arXiv by Robin Ekman, Tom Heinzl and... me! #resummation #radiationreaction <LINK> #AcademicChatter #AcademicTwitter @plym_math @PlymUni @HiggsCentre <LINK> And today we picked up our first citation! Thanks Greger! <LINK> <LINK> Ah, the feeling when two groups independently get the sequence {2, -20, 328, -7024, 179264, -5204416, 167270400....} Bingo!",https://arxiv.org/abs/2105.01640,"The Landau-Lifshitz equation is the first in an infinite series of approximations to the Lorentz-Abraham-Dirac equation obtained from `reduction of order'. We show that this series is divergent, predicting wildly different dynamics at successive perturbative orders. Iterating reduction of order ad infinitum in a constant crossed field, we obtain an equation of motion which is free of the erratic behaviour of perturbation theory. We show that Borel-Pad\'e resummation of the divergent series accurately reproduces the dynamics of this equation, using as little as two perturbative coefficients. Comparing with the Lorentz-Abraham-Dirac equation, our results show that for large times the optimal order of truncation typically amounts to using the Landau-Lifshitz equation, but that this fails to capture the resummed dynamics over short times. ","Reduction of order, resummation and radiation reaction",3,"['New paper on the arXiv by Robin Ekman, Tom Heinzl and... me! #resummation #radiationreaction\n<LINK>\n#AcademicChatter #AcademicTwitter \n@plym_math @PlymUni @HiggsCentre <LINK>', 'And today we picked up our first citation! Thanks Greger!\nhttps://t.co/d8z7jdPZC4 https://t.co/f9XouNAp59', 'Ah, the feeling when two groups independently get the sequence \n{2, -20, 328, -7024, 179264, -5204416, 167270400....}\nBingo!']",21,05,369
173,120,1206572429247733763,4844847993,Alexandre Bovet,Our work with @leoguti85 and Jean-Charles Delvenne on multi-scale anomaly detection in attributed networks was accepted at #AAAI20. <LINK>. We use a graph-signal processing framework to find anomalies and their contexts at all scales. My first CS conference 🙂 <LINK>,https://arxiv.org/abs/1912.04144,"Many social and economic systems can be represented as attributed networks encoding the relations between entities who are themselves described by different node attributes. Finding anomalies in these systems is crucial for detecting abuses such as credit card frauds, web spams or network intrusions. Intuitively, anomalous nodes are defined as nodes whose attributes differ starkly from the attributes of a certain set of nodes of reference, called the context of the anomaly. While some methods have proposed to spot anomalies locally, globally or within a community context, the problem remain challenging due to the multi-scale composition of real networks and the heterogeneity of node metadata. Here, we propose a principled way to uncover outlier nodes simultaneously with the context with respect to which they are anomalous, at all relevant scales of the network. We characterize anomalous nodes in terms of the concentration retained for each node after smoothing specific signals localized on the vertices of the graph. Besides, we introduce a graph signal processing formulation of the Markov stability framework used in community detection, in order to find the context of anomalies. The performance of our method is assessed on synthetic and real-world attributed networks and shows superior results concerning state of the art algorithms. Finally, we show the scalability of our approach in large networks employing Chebychev polynomial approximations. ",Multi-scale Anomaly Detection on Attributed Networks,1,['Our work with @leoguti85\n and Jean-Charles Delvenne on multi-scale anomaly detection in attributed networks was accepted at #AAAI20. <LINK>. \nWe use a graph-signal processing framework to find anomalies and their contexts at all scales.\nMy first CS conference 🙂 <LINK>'],19,12,266
174,146,1182102744032858112,907232486735958018,Jaki Noronha-Hostler,"New paper on the arxiv and my undergrad is the lead author! We make (baseline) predictions for integrated flow harmonics at the beam energy scan using linear+cubic response.   <LINK> The idea was the following: we know at the lowest beam energies that the physics has to change but it's complicated since both the initial conditions and medium properties are changing at the same time. We wanted a way to separate those differences. Also, we wanted to be able to test when the physics breaks down (using the assumptions of high energy collisions). Basically, if we use the high-energy paradigm we expect that the initial conditions shouldn't change with beam energy (nor the flow fluctuations). This is the paper I was talking about here: <LINK>",https://arxiv.org/abs/1910.03677,"Currently the RHIC Beam Energy Scan is exploring a new region of the Quantum Chromodynamic phase diagram at large baryon densities that approaches nuclear astrophysics regimes. This provides an opportunity to study relativistic hydrodynamics in a regime where the net conserved charges of baryon number, strangeness, and electric charge play a role, which will significantly change the theoretical approach to simulating the baryon-dense Quark-Gluon Plasma. Here we detail many of the important changes needed to adapt both initial conditions and the medium to baryon-rich matter. Then, we make baseline predictions for the elliptical flow and fluctuations based on extrapolating the physics at LHC and top RHIC energies to support future analyses of where and how the new baryon-dense physics causes these extrapolations to break down. First we compare eccentricities across beam energies, exploring their underlying assumptions; we find the the extrapolated initial state is predicted to be nearly identical to that at AuAu $\sqrt{s_{NN}}=200$ GeV. Then the final flow harmonic predictions are based on linear+cubic response. We discuss preliminary STAR results in order to determine the implications that they have for linear+cubic response coefficients at the lowest beam energy of AuAu $\sqrt{s_{NN}}=7$ GeV. ","Baseline predictions of elliptic flow and fluctuations at the RHIC Beam
  Energy Scan using response coefficients",4,"['New paper on the arxiv and my undergrad is the lead author! We make (baseline) predictions for integrated flow harmonics at the beam energy scan using linear+cubic response.   \n\n<LINK>', ""The idea was the following: we know at the lowest beam energies that the physics has to change but it's complicated since both the initial conditions and medium properties are changing at the same time. We wanted a way to separate those differences."", ""Also, we wanted to be able to test when the physics breaks down (using the assumptions of high energy collisions).  Basically, if we use the high-energy paradigm we expect that the initial conditions shouldn't change with beam energy (nor the flow fluctuations)."", 'This is the paper I was talking about here: \nhttps://t.co/BHlydrSurO']",19,10,745
175,241,1371468428817674243,471165766,Ion Errea,"Today we post a beautiful preprint, where we find a strong correlation between the chemical bonding network and Tc in hydrogen-rich syperconductors. It's important for setting up search directions in the future. Superb work by @Frances89010669! #SuperH <LINK> <LINK>",https://arxiv.org/abs/2103.07320,"Recent experimental discoveries show that hydrogen-rich compounds can reach room temperature superconductivity, at least at high pressures. Also that there exist metallic hydrogen-abundant systems with critical temperatures of few Kelvin, or even with no trace of superconductivity at all. By analyzing through first-principles calculations the structural and electronic properties of more than one hundred compounds predicted to be superconductors in the literature, we determine that the capacity of creating a bonding network of connected localized units is the key to enhance the critical temperature in hydrogen-based superconductors, explaining the large variety of critical temperatures of superconducting hydrogen-rich materials. We define a magnitude named as the {\it networking value}, which correlates well with the predicted critical temperature, much better than any other descriptor analyzed thus far. This magnitude can be easily calculated for any compound by analyzing isosurfaces of the electron localization function. By classifying the studied compounds according to their bonding nature, we observe that the {\it networking value} correlates with the critical temperature for all bonding types. Our analysis also highlights that systems with weakened covalent bonds are the most promising candidates for reaching high critical temperatures. The discovery of the positive correlation between superconductivity and the bonding network offers the possibility of screening easily hydrogen-based compounds and, at the same time, sets clear paths for chemically engineering better superconductors. ","Strong correlation between bonding network and critical temperature in
  hydrogen-based superconductors",1,"[""Today we post a beautiful preprint, where we find a strong correlation between the chemical bonding network and Tc in hydrogen-rich syperconductors. It's important for setting up search directions in the future. Superb work by @Frances89010669! #SuperH <LINK> <LINK>""]",21,03,266
176,176,1326125319078686721,1667135348,Alexander Terenin,"Pathwise Conditioning of Gaussian Processes We study implications of the pathwise formula (f|y)(.) = f(.) + K_{(.)x} K_{xx}^{-1} (y - f(x)) on Gaussian processes, following our outstanding-paper-honorable-mention-winning ICML paper. Check it out! <LINK> @mpd37 <LINK>",https://arxiv.org/abs/2011.04026,"As Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning. ",Pathwise Conditioning of Gaussian Processes,1,"['Pathwise Conditioning of Gaussian Processes\n\nWe study implications of the pathwise formula (f|y)(.) = f(.) + K_{(.)x} K_{xx}^{-1} (y - f(x)) on Gaussian processes, following our outstanding-paper-honorable-mention-winning ICML paper. Check it out!\n\n<LINK>\n\n@mpd37 <LINK>']",20,11,267
177,5,723001334908100608,56872711,Tejas Kulkarni,"check our new paper on - Hierarchical Deep RL: Integrating temporal abstraction and intrinsic motivation"" <LINK> @iandanforth agreed re saliency. Ext rewards have always bothered me. But they seem appropriate to represent unalterable rewards w.r.t agent @Miles_Brundage if you chop top layer, model becomes DQN. Need flexible memory and qnet to learn objecty things to exploit top layer",https://arxiv.org/abs/1604.06057,"Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'. ","Hierarchical Deep Reinforcement Learning: Integrating Temporal
  Abstraction and Intrinsic Motivation",3,"['check our new paper on - Hierarchical Deep RL: Integrating temporal abstraction and intrinsic motivation"" <LINK>', '@iandanforth agreed re saliency. Ext rewards have always bothered me. But they seem appropriate to represent unalterable rewards w.r.t agent', '@Miles_Brundage if you chop top layer, model becomes DQN. Need flexible memory and qnet to learn objecty things to exploit top layer']",16,04,386
178,77,1006308638540132352,2800204849,Andrew Gordon Wilson,"Our new paper, Probabilistic FastText for Multi-Sense Word Embeddings, is appearing as an oral at #ACL2018, with code! <LINK> We learn density embeddings that account for sub-word structure and multiple senses. Joint with Ben Athiwaratkun and @AnimaAnandkumar! <LINK>",https://arxiv.org/abs/1806.02901,"We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share statistical strength across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, the proposed model is the first to achieve multi-sense representations while having enriched semantics on rare words. ",Probabilistic FastText for Multi-Sense Word Embeddings,1,"['Our new paper, Probabilistic FastText for Multi-Sense Word Embeddings, is appearing as an oral at #ACL2018, with code! <LINK>\nWe learn density embeddings that account for sub-word structure and multiple senses. Joint with Ben Athiwaratkun and @AnimaAnandkumar! <LINK>']",18,06,267
179,87,1273878952520507392,762420558,Ciaran O'Hare,"New paper on solar axions. Given the recent XENON excitement, thought it timely to push this one out too: <LINK> <LINK> We look at a new source of axions from the Sun (aside from the usual Primakoff and ABC fluxes) which are produced from the conversion of longitudinal plasmons. This has the side effect of the expected axion signal being very sensitive to the magnetic field of the Sun. So we took a look at whether the planned next-gen helioscope #IAXO which will be hosted @desy could serve a dual-purpose as an instrument to measure the solar B-field. This of course requires that solar axions are discovered first (maybe they already have?....) Nevertheless the prospects look quite good we think. The flux effectively probes the Sun from the inside out: the better the energy threshold the farther out in radius you can probe. The code is available for you to have a look at here: <LINK>",https://arxiv.org/abs/2006.10415,"Axion helioscopes search for solar axions and axion-like particles via inverse Primakoff conversion in strong laboratory magnets pointed at the Sun. Anticipating the detection of solar axions, we determine the potential for the planned next-generation helioscope, the International Axion Observatory (IAXO), to measure or constrain the solar magnetic field. To do this we consider a previously neglected component of the solar axion flux at sub-keV energies arising from the conversion of longitudinal plasmons. This flux is sensitively dependent to the magnetic field profile of the Sun, with lower energies corresponding to axions converting into photons at larger solar radii. If the detector technology eventually installed in IAXO has an energy resolution better than 200 eV, then solar axions could become an even more powerful messenger than neutrinos of the magnetic field in the core of the Sun. For energy resolutions better than 10 eV, IAXO could access the inner 70% of the Sun and begin to constrain the field at the tachocline: the boundary between the radiative and convective zones. The longitudinal plasmon flux from a toroidal magnetic field also has an additional 2% geometric modulation effect which could be used to measure the angular dependence of the magnetic field. ",Axion helioscopes as solar magnetometers,5,"['New paper on solar axions. Given the recent XENON excitement, thought it timely to push this one out too: <LINK> <LINK>', 'We look at a new source of axions from the Sun (aside from the usual Primakoff and ABC fluxes) which are produced from the conversion of longitudinal plasmons. This has the side effect of the expected axion signal being very sensitive to the magnetic field of the Sun.', 'So we took a look at whether the planned next-gen helioscope #IAXO which will be hosted @desy could serve a dual-purpose as an instrument to measure the solar B-field. This of course requires that solar axions are discovered first (maybe they already have?....)', 'Nevertheless the prospects look quite good we think. The flux effectively probes the Sun from the inside out: the better the energy threshold the farther out in radius you can probe.', 'The code is available for you to have a look at here: https://t.co/7Qb24OxEhn']",20,06,894
180,92,1425366685435973636,1339508444764786694,Gregor Kasieczka,"New paper today: ""Symmetries, Safety, and Self-Supervision"" (arXiv: <LINK>). We - driven by excellent Heidelberg people including @LorenzVogel - look at how known physical symmetries can be used to learn better representations. 1/3 We use contrastive learning a la #SimCLR and include translation, rotations, and soft+collinear emissions. This figure shows how well rotations are learned. Left is without including rotations, right is with. s(z,z')=1 &lt;-&gt; identical representations 2/3 <LINK> The goal is to have a better input for #unsupervised learning (coming next..) but we can already test how well the learned representation does as input to a linear classifier. Spoiler: Pretty well (curve is for top tagging w/ a linear network) 3/3 <LINK>",https://arxiv.org/abs/2108.04253,"Collider searches face the challenge of defining a representation of high-dimensional data such that physical symmetries are manifest, the discriminating features are retained, and the choice of representation is new-physics agnostic. We introduce JetCLR to solve the mapping from low-level data to optimized observables though self-supervised contrastive learning. As an example, we construct a data representation for top and QCD jets using a permutation-invariant transformer-encoder network and visualize its symmetry properties. We compare the JetCLR representation with alternative representations using linear classifier tests and find it to work quite well. ","Symmetries, Safety, and Self-Supervision",3,"['New paper today: ""Symmetries, Safety, and Self-Supervision"" (arXiv: <LINK>). We - driven by excellent Heidelberg people including @LorenzVogel - look at how known physical symmetries can be used to learn better representations. 1/3', ""We use contrastive learning a la #SimCLR and include translation, rotations, and soft+collinear emissions. This figure shows how well rotations are learned. Left is without including rotations, right is with. s(z,z')=1 &lt;-&gt; identical representations 2/3 https://t.co/zFaYjFEiEk"", 'The goal is to have a better input for #unsupervised learning (coming next..) but we can already test how well the learned representation does as input to a linear classifier. Spoiler: Pretty well (curve is for top tagging w/ a linear network) 3/3 https://t.co/YyzlA9srTG']",21,08,752
181,221,1247441503565348865,776322668509429761,Svenja Boberg,"Our study on “Pandemic Populism” is out today with my awesome colleagues @Kudusch, @thorstenquandt & @Lenafrescamente! We analyzed Corona-related Fb-posts of alternative news media and the actors, topics, fake news and conspiracy theories they addressed: <LINK>",https://arxiv.org/abs/2004.02566,"The COVID-19 pandemic has not only had severe political, economic, and societal effects, it has also affected media and communication systems in unprecedented ways. While traditional journalistic media has tried to adapt to the rapidly evolving situation, alternative news media on the Internet have given the events their own ideological spin. Such voices have been criticized for furthering societal confusion and spreading potentially dangerous ""fake news"" or conspiracy theories via social media and other online channels. The current study analyzes the factual basis of such fears in an initial computational content analysis of alternative news media's output on Facebook during the early Corona crisis, based on a large German data set from January to the second half of March 2020. Using computational content analysis, methods, reach, interactions, actors, and topics of the messages were examined, as well as the use of fabricated news and conspiracy theories. The analysis revealed that the alternative news media stay true to message patterns and ideological foundations identified in prior research. While they do not spread obvious lies, they are predominantly sharing overly critical, even anti-systemic messages, opposing the view of the mainstream news media and the political establishment. With this pandemic populism, they contribute to a contradictory, menacing, and distrusting worldview, as portrayed in detail in this analysis. ","Pandemic Populism: Facebook Pages of Alternative News Media and the
  Corona Crisis -- A Computational Content Analysis",1,"['Our study on “Pandemic Populism”  is out today with my awesome colleagues @Kudusch, @thorstenquandt &amp; @Lenafrescamente! We analyzed Corona-related Fb-posts of alternative news media and the actors, topics, fake news and conspiracy theories they addressed: <LINK>']",20,04,261
182,18,978941181882261504,96779364,Arnab Bhattacharyya,"New paper (<LINK> ) showing that the so-called ""Even Set"" problem (aka, finding the minimum distance of binary linear codes) is W[1]-hard (assuming Gap-ETH). I'd been obsessed over this problem the last few years, so really satisfying to get a solid result!",https://arxiv.org/abs/1803.09717,"The $k$-Even Set problem is a parameterized variant of the Minimum Distance Problem of linear codes over $\mathbb F_2$, which can be stated as follows: given a generator matrix $\mathbf A$ and an integer $k$, determine whether the code generated by $\mathbf A$ has distance at most $k$. Here, $k$ is the parameter of the problem. The question of whether $k$-Even Set is fixed parameter tractable (FPT) has been repeatedly raised in literature and has earned its place in Downey and Fellows' book (2013) as one of the ""most infamous"" open problems in the field of Parameterized Complexity. In this work, we show that $k$-Even Set does not admit FPT algorithms under the (randomized) Gap Exponential Time Hypothesis (Gap-ETH) [Dinur'16, Manurangsi-Raghavendra'16]. In fact, our result rules out not only exact FPT algorithms, but also any constant factor FPT approximation algorithms for the problem. Furthermore, our result holds even under the following weaker assumption, which is also known as the Parameterized Inapproximability Hypothesis (PIH) [Lokshtanov et al.'17]: no (randomized) FPT algorithm can distinguish a satisfiable 2CSP instance from one which is only $0.99$-satisfiable (where the parameter is the number of variables). We also consider the parameterized $k$-Shortest Vector Problem (SVP), in which we are given a lattice whose basis vectors are integral and an integer $k$, and the goal is to determine whether the norm of the shortest vector (in the $\ell_p$ norm for some fixed $p$) is at most $k$. Similar to $k$-Even Set, this problem is also a long-standing open problem in the field of Parameterized Complexity. We show that, for any $p > 1$, $k$-SVP is hard to approximate (in FPT time) to some constant factor, assuming PIH. Furthermore, for the case of $p = 2$, the inapproximability factor can be amplified to any constant. ","Parameterized Intractability of Even Set and Shortest Vector Problem
  from Gap-ETH",1,"['New paper (<LINK> ) showing that the so-called ""Even Set"" problem (aka, finding the minimum distance of binary linear codes) is W[1]-hard (assuming Gap-ETH). I\'d been obsessed over this problem the last few years, so really satisfying to get a solid result!']",18,03,257
183,127,1272804855120302082,711950336,Olof Mogren,"Our new paper: ""Adversarial representation learning for synthetic replacement of private attributes"". Unlike traditional adv. methods, we sample a synthetic value for the sensitive attribute and improve utility while maintaining a stronger privacy. @EduNiw <LINK> <LINK> @dgillblad",https://arxiv.org/abs/2006.08039,"Data privacy is an increasingly important aspect of many real-world Data sources that contain sensitive information may have immense potential which could be unlocked using the right privacy enhancing transformations, but current methods often fail to produce convincing output. Furthermore, finding the right balance between privacy and utility is often a tricky trade-off. In this work, we propose a novel approach for data privatization, which involves two steps: in the first step, it removes the sensitive information, and in the second step, it replaces this information with an independent random sample. Our method builds on adversarial representation learning which ensures strong privacy by training the model to fool an increasingly strong adversary. While previous methods only aim at obfuscating the sensitive information, we find that adding new random information in its place strengthens the provided privacy and provides better utility at any given level of privacy. The result is an approach that can provide stronger privatization on image data, and yet be preserving both the domain and the utility of the inputs, entirely independent of the downstream task. ","Adversarial representation learning for synthetic replacement of private
  attributes",2,"['Our new paper: ""Adversarial representation learning for synthetic replacement of private attributes"". Unlike traditional adv. methods, we sample a synthetic value for the sensitive attribute and improve utility while maintaining a stronger privacy. @EduNiw <LINK> <LINK>', '@dgillblad']",20,06,281
184,209,1448459267346821127,929633835309981698,mmatsuo,"Our paper is now on arXiv😊We propose spin-motive force driven by a surface acoustic wave, resulting in both dc & second harmonic voltage. In contrast to the conventional ones, it requires no sophisticated device structures or strong spin-orbit materials. <LINK>",https://arxiv.org/abs/2110.06552,"The spin-motive force (SMF) in a simple ferromagnetic monolayer caused by a surface acoustic wave is studied theoretically via spin-vorticity coupling (SVC). The SMF has two mechanisms. The first is the SVC-driven SMF, which produces the first harmonic electromotive force, and the second is the interplay between the SVC and the magentoelastic coupling, which produces the d.c. and second harmonic electromotive forces. We show that these electric voltages induced by a Rayleigh-type surface acoustic wave can be detected in polycrystalline nickel. No sophisticated device structures, non-collinear magnetic structures, or strong spin-orbit materials are used in our approach. Consequently, it is intended to broaden the spectrum of SMF applications considerably. ",Spin elastodynamic motive force,1,"['Our paper is now on arXiv😊We propose spin-motive force driven by a surface acoustic wave, resulting in both dc &amp; second harmonic voltage. In contrast to the conventional ones, it requires no sophisticated device structures or strong spin-orbit materials. \n<LINK>']",21,10,261
185,91,1491715766147862530,1397091478460116993,Laura Colzi,"Check out our new paper: Colzi et al. (2022), ApJL in press, arXiv:2202.0411 @L_Colzi @ryvendel  <LINK> Thanks to deuterated molecules we have spotted the presence of two different gas components towards the Galactic Centre source G+0.693-0.027. <LINK>",https://arxiv.org/abs/2202.04111,"The Central Molecular Zone (CMZ) contains most of the mass of our Galaxy but its star formation rate is one order of magnitude lower than in the Galactic disc. This is likely related to the fact that the bulk of the gas in the CMZ is in a warm ($>$100 K) and turbulent phase with little material in the pre-stellar phase. We present in this Letter observations of deuterium fractionation (D/H ratios) of HCN, HNC, HCO$^{+}$, and N$_{2}$H$^{+}$ towards the CMZ molecular cloud G+0.693-0.027. These observations clearly show, for the first time, the presence of a colder, denser, and less turbulent narrow component, with a line width of $\sim$9 km s$^{-1}$, in addition to the warm, less dense and turbulent broad component with a line width of $\sim$20 km s$^{-1}$. The very low D/H ratio $\le$6$\times$10$^{-5}$ for HCO$^{+}$ and N$_{2}$H$^{+}$, close to the cosmic value ($\sim$2.5$\times$10$^{-5}$), and the high D/H ratios $>$4$\times$10$^{-4}$ for HCN and HNC derived for the broad component, confirm the presence of high-temperatures deuteration routes for nitriles. For the narrow component we have derived D/H ratios $>$10$^{-4}$ and excitation temperatures of $7$ K for all molecules, suggesting kinetic temperatures $\le$30 K and H$_2$ densities $\ge$5$\times$10$^{4}$ cm$^{-3}$, at least one order of magnitude larger than for the broad component. The method presented in this Letter allows to identify clouds on the verge of star formation, i.e. under pre-stellar conditions, towards the CMZ. This method can also be used for the identification of such clouds in external galaxies. ","Deuterium fractionation as a multi-phase component tracer in the
  Galactic Centre",1,"['Check out our new paper: Colzi et al. (2022), ApJL in press, arXiv:2202.0411 @L_Colzi @ryvendel  \n<LINK>\nThanks to deuterated molecules we have spotted the presence of two different gas components towards the Galactic Centre source G+0.693-0.027. <LINK>']",22,02,252
186,152,1445007474327793665,732494566545203201,David Klindt,"New paper on score-based generative classifiers (SBGCs) <LINK> Diffusion models have produced impressive results <LINK> We show how they can be used as classifiers on CIFAR-10. Work w/ @zimmerrol @schott_lukas @YsongStanford @adric_dunn (1/6) While previous methods have shown a trade-off between generative and classification performance, our SBGC model achieves new state-of-the-art performances both in likelihoods and classification accuracy for generative classifiers on CIFAR-10. (2/6) <LINK> In the past, generative classifiers (analysis-by-synthesis) have been shown to increase adversarial robustness on MNIST <LINK> However, so far these results have not been extended to complex natural image datasets such as CIFAR-10. (3/6) Previous work showed that interpolating between images increases likelihoods, suggesting model failure on out-of-distribution data <LINK> @jh_jacobsen By contrast, our SBGC model correctly produces convex interpolation curves. (4/6) <LINK> Nevertheless, we find that our model spectacularly fails against gradient-based adversarial attacks. We argue that SBGCs have no structural advantage over discriminative classifiers and that analysis-by-synthesis alone is not sufficient for out-of-distribution robustness. (5/6) <LINK> Still, our work shows that SBGCs can achieve very competitive likelihoods and classification accuracies which encourage further research! Thanks for fun discussions and feedback @poolio @wgrathwohl @yash_j_sharma @wielandbr @dylanpaiton @eero_simoncelli (6/6)",http://arxiv.org/abs/2110.00473,"The tremendous success of generative models in recent years raises the question whether they can also be used to perform classification. Generative models have been used as adversarially robust classifiers on simple datasets such as MNIST, but this robustness has not been observed on more complex datasets like CIFAR-10. Additionally, on natural image datasets, previous results have suggested a trade-off between the likelihood of the data and classification accuracy. In this work, we investigate score-based generative models as classifiers for natural images. We show that these models not only obtain competitive likelihood values but simultaneously achieve state-of-the-art classification accuracy for generative classifiers on CIFAR-10. Nevertheless, we find that these models are only slightly, if at all, more robust than discriminative baseline models on out-of-distribution tasks based on common image corruptions. Similarly and contrary to prior results, we find that score-based are prone to worst-case distribution shifts in the form of adversarial perturbations. Our work highlights that score-based generative models are closing the gap in classification accuracy compared to standard discriminative models. While they do not yet deliver on the promise of adversarial and out-of-domain robustness, they provide a different approach to classification that warrants further research. ",Score-Based Generative Classifiers,6,"['New paper on score-based generative classifiers (SBGCs) <LINK>\n\nDiffusion models have produced impressive results <LINK>\n\nWe show how they can be used as classifiers on CIFAR-10.\n\nWork w/ @zimmerrol @schott_lukas @YsongStanford @adric_dunn\n\n(1/6)', 'While previous methods have shown a trade-off between generative and classification performance, our SBGC model achieves new state-of-the-art performances both in likelihoods and classification accuracy for generative classifiers on CIFAR-10.\n\n(2/6) https://t.co/iRl9XCTRAK', 'In the past, generative classifiers (analysis-by-synthesis) have been shown to increase adversarial robustness on MNIST https://t.co/OSMjtHjHX1\n\nHowever, so far these results have not been extended to complex natural image datasets such as CIFAR-10.\n\n(3/6)', 'Previous work showed that interpolating between images increases likelihoods, suggesting model failure on out-of-distribution data https://t.co/WBwvfWUSsK @jh_jacobsen\n\nBy contrast, our SBGC model correctly produces convex interpolation curves.\n\n(4/6) https://t.co/KWWUDkRBSI', 'Nevertheless, we find that our model spectacularly fails against gradient-based adversarial attacks.\n\nWe argue that SBGCs have no structural advantage over discriminative classifiers and that analysis-by-synthesis alone is not sufficient for out-of-distribution robustness.\n\n(5/6) https://t.co/TCZTbPiuIy', 'Still, our work shows that SBGCs can achieve very competitive likelihoods and classification accuracies which encourage further research!\n\nThanks for fun discussions and feedback @poolio @wgrathwohl @yash_j_sharma @wielandbr @dylanpaiton @eero_simoncelli\n\n(6/6)']",21,10,1522
187,1,1326946705233772544,1161312102486667264,Keith Burghardt,"Whatever the outcome, affirmative action can be a critical tool to improve policies. We discuss questions that surround fair policies in our new paper <LINK> with Yuzi He (first author) and @KristinaLerman. Come see our presentation at the AFLCI NeurIPS Workshop! <LINK> Also wanted to add Fiona (Siyi) Guo was an excellent co-author on this paper too. I ran out of space in the previous tweet!",https://arxiv.org/abs/2010.16409,"Explicit and implicit bias clouds human judgement, leading to discriminatory treatment of minority groups. A fundamental goal of algorithmic fairness is to avoid the pitfalls in human judgement by learning policies that improve the overall outcomes while providing fair treatment to protected classes. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to fairness constraints. We define two measures of treatment bias and infer best treatment assignment that minimizes the bias while optimizing overall outcome. We demonstrate that there is a dilemma of balancing fairness and overall benefit; however, allowing preferential treatment to protected classes in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing student outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve student test scores. Our framework provides a principled way to learn fair treatment policies in real-world settings. ",Inherent Trade-offs in the Fair Allocation of Treatments,2,"['Whatever the outcome, affirmative action can be a critical tool to improve policies. We discuss questions that surround fair policies in our new paper\n<LINK>\nwith Yuzi He (first author) and @KristinaLerman. Come see our presentation at the AFLCI NeurIPS Workshop! <LINK>', 'Also wanted to add Fiona (Siyi) Guo was an excellent co-author on this paper too. I ran out of space in the previous tweet!']",20,10,394
188,5,1477313568685056001,586538662,Dotan,"Our recent on Autonomous dozers (submitted to ICRA) is nowavailable on Arxiv.  Paper: <LINK> Video: <LINK> TL:DR We created a new method for autonomous bulldozers to preform the grading task using RL and BC algorithms. 👇 2/2 Our trained agent, AGPNet, reaches human-level performance and outperforms current state-of-the-art machine learning methods for the autonomous grading task. In addition, our agent is capable of generalizing from random scenarios to unseen real world problems. 3/3 Finally, we show our policy which trained in simulation on a real prototype dozer we built in our Lab.",https://arxiv.org/abs/2112.10877,"In this work, we establish heuristics and learning strategies for the autonomous control of a dozer grading an uneven area studded with sand piles. We formalize the problem as a Markov Decision Process, design a simulation which demonstrates agent-environment interactions and finally compare our simulator to a real dozer prototype. We use methods from reinforcement learning, behavior cloning and contrastive learning to train a hybrid policy. Our trained agent, AGPNet, reaches human-level performance and outperforms current state-of-the-art machine learning methods for the autonomous grading task. In addition, our agent is capable of generalizing from random scenarios to unseen real world problems. ",AGPNet -- Autonomous Grading Policy Network,3,"['Our recent on Autonomous dozers (submitted to ICRA) is nowavailable on Arxiv. \n\nPaper: <LINK>\nVideo: <LINK>\n\nTL:DR We created a new method for autonomous bulldozers to preform the grading task using RL and BC algorithms. 👇', '2/2 Our trained agent, AGPNet, reaches human-level performance and outperforms current state-of-the-art machine learning methods for the autonomous grading task. In addition, our agent is capable of generalizing from random scenarios to unseen real world problems.', '3/3 Finally, we show our policy which trained in simulation on a real prototype dozer we built in our Lab.']",21,12,592
189,24,1075175442808799243,710610891058716673,Jan Leike,Multiparty computation is awesome because it lets multiple parties train a model without seeing the weights. But there are fundamental limits to making it scalable: &gt;24x overhead! Our new paper addresses this problem. <LINK> w/ @MiljanMartic @iamtrask et al. <LINK>,https://arxiv.org/abs/1812.05979,"Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive. ",Scaling shared model governance via model splitting,1,['Multiparty computation is awesome because it lets multiple parties train a model without seeing the weights.\n\nBut there are fundamental limits to making it scalable: &gt;24x overhead!\n\nOur new paper addresses this problem.\n\n<LINK>\n\nw/ @MiljanMartic @iamtrask et al. <LINK>'],18,12,268
190,10,1377782750384545794,14280661,Ben Goertzel,General Theory of General Intelligence: A Pragmatic Patternist Perspective -- new review paper by moi -- <LINK> -- only semi-technical -- starts w/ high level ontological and mathematical perspectives and ends up at the level of human cog sci and OpenCog Hyperon,https://arxiv.org/abs/2103.15100,"A multi-decade exploration into the theoretical foundations of artificial and natural general intelligence, which has been expressed in a series of books and papers and used to guide a series of practical and research-prototype software systems, is reviewed at a moderate level of detail. The review covers underlying philosophies (patternist philosophy of mind, foundational phenomenological and logical ontology), formalizations of the concept of intelligence, and a proposed high level architecture for AGI systems partly driven by these formalizations and philosophies. The implementation of specific cognitive processes such as logical reasoning, program learning, clustering and attention allocation in the context and language of this high level architecture is considered, as is the importance of a common (e.g. typed metagraph based) knowledge representation for enabling ""cognitive synergy"" between the various processes. The specifics of human-like cognitive architecture are presented as manifestations of these general principles, and key aspects of machine consciousness and machine ethics are also treated in this context. Lessons for practical implementation of advanced AGI in frameworks such as OpenCog Hyperon are briefly considered. ","The General Theory of General Intelligence: A Pragmatic Patternist
  Perspective",1,['General Theory of General Intelligence: A Pragmatic Patternist Perspective -- new review paper by moi -- <LINK> -- only semi-technical -- starts w/ high level ontological and mathematical perspectives and ends up at the level of human cog sci and OpenCog Hyperon'],21,03,262
191,76,1426179742957543426,1003652696723873792,Max Gaspari,"New paper on AGN feeding & feedback, done with a brilliant young PD (F. Maccagni)! One of the best probes of chaotic cold accretion (raining on galaxies/BHs), with many multiphase thermo-kinematical diagnostics (e.g. k-plot). #astronomy #BlackHoleWeather <LINK>",https://arxiv.org/abs/2108.05247,"We present a multi-wavelength study of the gaseous medium surrounding the nearby active galactic nucleus (AGN) Fornax A. Using MeerKAT, ALMA and MUSE observations we reveal a complex distribution of the atomic (HI), molecular (CO), and ionised gas in its centre and along the radio jets. By studying the multi-scale kinematics of the multi-phase gas, we reveal the presence of concurrent AGN feeding and feedback phenomena. Several clouds and an extended 3 kpc filament -- perpendicular to the radio jets and the inner disk ($r\lesssim 4.5$ kpc) -- show highly-turbulent kinematics, which likely induces nonlinear condensation and subsequent Chaotic Cold Accretion (CCA) onto the AGN. In the wake of the radio jets and in an external ($r\gtrsim 4.5$ kpc) ring, we identify an entrained massive ($\sim$ $10^7$ M$_\odot$) multi-phase outflow ($v_{\rm OUT}\sim 2000$ km s$^{-1}$). The rapid flickering of the nuclear activity of Fornax A ($\sim$ 3 Myr) and the gas experiencing turbulent condensation raining onto the AGN provide quantitative evidence that a recurrent, tight feeding and feedback cycle may be self-regulating the activity of Fornax A, in agreement with CCA simulations. To date, this is one of the most in-depth probes of such a mechanism, paving the way to apply these precise diagnostics to a larger sample of nearby AGN hosts and their multi-phase ISM. ","AGN feeding and feedback in Fornax A: kinematical analysis of the
  multi-phase ISM",1,"['New paper on AGN feeding &amp; feedback, done with a brilliant young PD (F. Maccagni)! \nOne of the best probes of chaotic cold accretion (raining on galaxies/BHs), with many multiphase thermo-kinematical diagnostics (e.g. k-plot). \n#astronomy #BlackHoleWeather\n<LINK>']",21,08,261
192,36,1055514818940039168,93358739,Mandar Joshi,"New paper! pair2vec: We train word pair embeddings from predictive contexts, and use them to inject background knowledge into QA & NLI models. We get significant gains on top of ELMo, including SotA on the adversarial Glockner NLI dataset (93.4). <LINK> (1/n) <LINK> Why embed word *pairs*? Because QA & NLI often involve reasoning over implied relations (e.g. lexical or common sense) between pairs, and relying solely on end-task data for this background knowledge often results in poor generalization. (2/n) <LINK> We train pair representations on Wikipedia by maximizing the PMI between pairs and contexts. The intuition is that contexts (e.g., ""X is a famous Y"") for a pair (X, Y) often encode their relation (e.g. profession). (3/n) Embedding word pairs close to their predictive contexts helps us capture such relations. During end-task training, we add them to the cross-sentence attention layer of existing ELMo-equipped models. (4/n) <LINK> Our experiments show a gain of 2.7 F1 on SQuAD 2.0 and 1.3% on MultiNLI. The models do particularly well on adversarial datasets with 6-7% gains on adversarial SQuAD and 8.8% on the Glockner NLI test set. (5/n) <LINK> Our analysis shows that pair2vec is complementary to single-word embeddings. Interpolating pair2vec with fastText significantly improves encyclopedic and lexical-semantic word analogies. (6/n) <LINK> Joint work with @eunsolc, @omerlevy_, @dsweld, and @LukeZettlemoyer. Here's the link to the paper again -- <LINK> (7/7) @seb_ruder @eunsolc @omerlevy_ @dsweld @LukeZettlemoyer Thanks Sebastian :)",https://arxiv.org/abs/1810.08854,"Reasoning about implied relationships (e.g., paraphrastic, common sense, encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. Our pairwise embeddings are computed as a compositional function on word representations, which is learned by maximizing the pointwise mutual information (PMI) with the contexts in which the two words co-occur. We add these representations to the cross-sentence attention layer of existing inference models (e.g. BiDAF for QA, ESIM for NLI), instead of extending or replacing existing word embeddings. Experiments show a gain of 2.7% on the recently released SQuAD2.0 and 1.3% on MultiNLI. Our representations also aid in better generalization with gains of around 6-7% on adversarial SQuAD datasets, and 8.8% on the adversarial entailment test set by Glockner et al. (2018). ","pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence
  Inference",8,"['New paper! pair2vec: We train word pair embeddings from predictive contexts, and use them to inject background knowledge into QA &amp; NLI models. We get significant gains on top of ELMo, including SotA on the adversarial Glockner NLI dataset (93.4). <LINK> (1/n) <LINK>', 'Why embed word *pairs*? Because QA &amp; NLI often involve reasoning over implied relations (e.g. lexical or common sense) between pairs, and relying solely on end-task data for this background knowledge often results in poor generalization. (2/n) https://t.co/MsmiMiYba2', 'We train pair representations on Wikipedia by maximizing the PMI between pairs and contexts. The intuition is that contexts (e.g.,  ""X is a famous Y"") for a pair (X, Y) often encode their relation (e.g. profession). (3/n)', 'Embedding word pairs close to their predictive contexts helps us capture such relations. During end-task training, we add them to the cross-sentence attention layer of existing ELMo-equipped models. (4/n) https://t.co/llgml67CDm', 'Our experiments show a gain of 2.7 F1 on SQuAD 2.0 and 1.3% on MultiNLI. The models do particularly well on adversarial datasets with 6-7% gains on adversarial SQuAD and 8.8% on the Glockner NLI test set. (5/n) https://t.co/UQVjIn5U1W', 'Our analysis shows that pair2vec is complementary to single-word embeddings. Interpolating pair2vec with fastText significantly improves encyclopedic and lexical-semantic word analogies. (6/n) https://t.co/tVOpomKP8x', ""Joint work with @eunsolc, @omerlevy_, @dsweld, and @LukeZettlemoyer. Here's the link to the paper again -- https://t.co/bxvaNccaff (7/7)"", '@seb_ruder @eunsolc @omerlevy_ @dsweld @LukeZettlemoyer Thanks Sebastian :)']",18,10,1564
193,139,1486351835489312783,109571291,Daniel González,"Fresh from the arXiv: We study the non-equilibrium dynamics of a simple lattice gauge theory, where local symmetry constraints can prevent thermalization. We show, in particular, how quantum many-body scars appear in the deconfined phase of the theory 👇🏽 <LINK>",https://arxiv.org/abs/2201.10260,"The weak ergodicity breaking induced by quantum many-body scars (QMBS) represents an intriguing concept that has received great attention in recent years due to its relation to unusual non-equilibrium behaviour. Here we reveal that this phenomenon can occur in a previously unexplored regime of a lattice gauge theory, where QMBS emerge due to the presence of an extensive number of local constraints. In particular, by analyzing the gauged Kitaev model, we provide an example where QMBS appear in a regime where charges are deconfined. By means of both numerical and analytical approaches, we find a variety of scarred states far away from the regime where the model is integrable. The presence of these states is revealed both by tracing them directly from the analytically reachable limit, as well as by quantum quenches showing persistent oscillations for specific initial states. ",Scar States in Deconfined $\mathbb{Z}_2$ Lattice Gauge Theories,1,"['Fresh from the arXiv:\n\nWe study the non-equilibrium dynamics of a simple lattice gauge theory, where local symmetry constraints can prevent thermalization. We show, in particular, how quantum many-body scars appear in the deconfined phase of the theory 👇🏽\n\n<LINK>']",22,01,261
194,179,1338603526511947777,209022852,David Yllanes,"Our latest preprint is up on the @arxiv. We study the effect of thermal fluctuations on buckling. With a mean-field theory and simulations, we quantify the T dependence of the buckling threshold, according to the renormalisation of the elastic constants. <LINK> <LINK>",https://arxiv.org/abs/2012.06565,"Understanding thin sheets, ranging from the macro to the nanoscale, can allow control of mechanical properties such as deformability. Out-of-plane buckling due to in-plane compression can be a key feature in designing new materials. While thin-plate theory can predict critical buckling thresholds for thin frames and nanoribbons at very low temperatures, a unifying framework to describe the effects of thermal fluctuations on buckling at more elevated temperatures presents subtle difficulties. We develop and test a theoretical approach that includes both an in-plane compression and an out-of-plane perturbing field to describe the mechanics of thermalised ribbons above and below the buckling transition. We show that, once the elastic constants are renormalised to take into account the ribbon's width (in units of the thermal length scale), we can map the physics onto a mean-field treatment of buckling, provided the length is short compared to a ribbon persistence length. Our theoretical predictions are checked by extensive molecular dynamics simulations of thin thermalised ribbons under axial compression. ",Thermal buckling and symmetry breaking in thin ribbons under compression,1,"['Our latest preprint is up on the @arxiv.\n\nWe study the effect of thermal fluctuations on buckling. With a mean-field theory and simulations, we quantify the T dependence of the buckling threshold, according to the renormalisation of the elastic constants.\n\n<LINK> <LINK>']",20,12,268
195,80,1060116932836409344,1230746966,Adam Płoszaj,"More flight connections and proximity of airport increase the number of coauthored scientific papers. More in my & @katycns & @everyxs new paper ""The impact of air transport availability on research collaboration: A case study of four universities"": <LINK> <LINK>",https://arxiv.org/abs/1811.02106,This paper analyzes the impact of air transport connectivity and accessibility on scientific collaboration. ,"The impact of air transport availability on research collaboration: A
  case study of four universities",1,"['More flight connections and proximity of airport increase the number of coauthored scientific papers. More in my &amp; @katycns &amp; @everyxs new paper ""The impact of air transport availability on research collaboration: A case study of four universities"": <LINK> <LINK>']",18,11,263
196,142,1456977009112858633,955103299,Marina Radulaski,"In the final figure in #Rlab new perspective paper, we compile a recipe for deploying integrated silicon carbide color center photonics for measurement based quantum computing. Prepared in an insightful illustration by @PrantaSaha01  <LINK> <LINK> <LINK>",https://arxiv.org/abs/2111.00136,"Color centers in wide band gap semiconductors are prominent candidates for solid-state quantum technologies due to their attractive properties including optical interfacing, long coherence times, spin-photon and spin-spin entanglement, as well as the potential for scalability. Silicon carbide color centers integrated into photonic devices span a wide range of applications in quantum information processing, in a material platform with quantum-grade wafer availability and advanced processing capabilities. Recent progress in emitter generation and characterization, nanofabrication, device design, and quantum optical studies have amplified the scientific interest in this platform. We provide a conceptual and quantitative analysis of the role of silicon carbide integrated photonics in three key application areas: quantum networking, simulation, and computing. ",Quantum Information Processing With Integrated Silicon Carbide Photonics,1,"['In the final figure in #Rlab new perspective paper, we compile a recipe for deploying integrated silicon carbide color center photonics for measurement based quantum computing. Prepared in an insightful illustration by @PrantaSaha01 \n\n<LINK> <LINK> <LINK>']",21,11,254
197,14,1466003548144082944,456819625,Lawrence Bull,"Our new paper - using a mixture of interpretable #GaussianProcesses to model overlapping power trends in wind farm data <LINK> <LINK> Rather than removing data, we automatically model different power relationships #windenergy #machinelearning <LINK> Thanks to everyone involved! Those on Twitter: @drgTim @dervilisTheDRG @lizzyintheDRG",http://arxiv.org/abs/2111.15496,"Power curves capture the relationship between wind speed and output power for a specific wind turbine. Accurate regression models of this function prove useful in monitoring, maintenance, design, and planning. In practice, however, the measurements do not always correspond to the ideal curve: power curtailments will appear as (additional) functional components. Such multivalued relationships cannot be modelled by conventional regression, and the associated data are usually removed during pre-processing. The current work suggests an alternative method to infer multivalued relationships in curtailed power data. Using a population-based approach, an overlapping mixture of probabilistic regression models is applied to signals recorded from turbines within an operational wind farm. The model is shown to provide an accurate representation of practical power data across the population. ","Bayesian Modelling of Multivalued Power Curves from an Operational Wind
  Farm",2,"['Our new paper - using a mixture of interpretable #GaussianProcesses to model overlapping power trends in wind farm data\n\n<LINK>\n<LINK>\n\nRather than removing data, we automatically model different power relationships\n#windenergy #machinelearning <LINK>', 'Thanks to everyone involved! Those on Twitter: @drgTim @dervilisTheDRG @lizzyintheDRG']",21,11,335
198,90,1039751045591826432,982515235285274624,R. Mor,"Besançon Galaxy Model is ready for multi-parameter inference: BGM FASt (<LINK>). Here it is a Corner plot resultig to study the IMF, the SFH and the density at the Solar Neighbourhood with Tycho-2 data. We are working really hard to apply our method to #GaiaDR2!! <LINK>",https://arxiv.org/abs/1809.03511,"We develop a new theoretical framework to generate Besan\c{c}on Galaxy Model fast approximate simulations (BGM FASt) to address fundamental questions of the Galactic structure and evolution performing multi-parameter inference. As a first application of our strategy we simultaneously infer the IMF, the star formation history and the stellar mass density in the Solar Neighbourhood. The BGM FASt strategy is based on a reweighing scheme, that uses a specific pre-sampled simulation, and on the assumption that the distribution function of the generated stars in the Galaxy can be described by an analytical expression. To validate BGM FASt we execute a set of tests. Finally, we use BGM FASt with an approximate Bayesian computation algorithm to obtain the posterior PDF of the inferred parameters, by comparing synthetic versus Tycho-2 colour-magnitude diagrams. Results: The validation shows a very good agreement between BGM FASt and the standard BGM, with BGM FASt being $\approx 10^4$ times faster. By analysing Tycho-2 data we obtain a thin disc star formation history decreasing in time and a present rate of $1.2 \pm 0.2 M_\odot/yr$. The resulting total stellar mass density in the Solar Neighbourhood is $0.051_{-0.005}^{+0.002} M_\odot/pc^3$ and the local dark matter density is $0.012 \pm 0.001 M_\odot/pc^3$. For the composite IMF we obtain a slope of $\alpha_2={2.1}_{-0.3}^{+0.1}$ in the mass range between $0.5 M_\odot$ and $1.53M_\odot$. The results of the slope at the high mass range are trustable up to $4M_\odot$ and highly depend on the choice of the extinction map (obtaining $\alpha_3={2.9}_{-0.2}^{+0.2}$ and $\alpha_3={3.7}_{-0.2}^{+0.2}$ respectively, for two different extinction maps). Systematic uncertainties are not included. Conclusions: The good performance of BGM FASt demonstrates that it is a very valuable tool to perform multi-parameter inference using Gaia data releases. ","BGM FASt: Besan\c{c}on Galaxy Model for Big Data. Simultaneous inference
  of the IMF, SFH and density in the Solar Neighbourhood",1,"['Besançon Galaxy Model is ready for multi-parameter inference: BGM FASt (<LINK>). Here it is a Corner plot resultig to study the IMF, the SFH and the density at the Solar Neighbourhood with Tycho-2 data. We are working really hard to apply our method to #GaiaDR2!! <LINK>']",18,09,270
199,70,1285826616510353409,822867138,Bradley Kavanagh,"New paper with @ultra_wimp: Primordial Black Holes as a dark matter candidate (<LINK>) Can black holes formed in the early Universe constitute the Dark Matter? Possibly. Did the world need another PBH review? Absolutely. <LINK> We give an overview of how such PBHs can form, focusing on the collapse of large density fluctuations in the early Universe.  Then summarise current straights on the PBH abundance (as a fraction of the Dark Matter abundance in the Universe): <LINK> We digitised *a lot* of PBH bounds. You can find them all on line, along with code for generating the PBH abundance plots: <LINK> Special thanks to @adamcoogan1 for constantly letting me know whenever a new PBH paper came out... I'm very grateful to @ultra_wimp for bringing me on board with this.  She did the lion's share of the work, but I was happy to help out with a fresh pair of eyes and my superhuman ability to click on lines in plots to digitise them. @ultra_wimp Oh I forgot to tell you, in the last read through, I corrected some ""f_{\rm co}"" to ""f_{\rm CO}"", so I consider my contribution invaluable. @cajohare @ultra_wimp <LINK>",https://arxiv.org/abs/2007.10722,"The detection of gravitational waves from mergers of tens of Solar mass black hole binaries has led to a surge in interest in Primordial Black Holes (PBHs) as a dark matter candidate. We aim to provide a (relatively) concise overview of the status of PBHs as a dark matter candidate, circa Summer 2020. First we review the formation of PBHs in the early Universe, focusing mainly on PBHs formed via the collapse of large density perturbations generated by inflation. Then we review the various current and future constraints on the present day abundance of PBHs. We conclude with a discussion of the key open questions in this field. ",Primordial Black Holes as a dark matter candidate,6,"['New paper with @ultra_wimp: Primordial Black Holes as a dark matter candidate (<LINK>)\n\nCan black holes formed in the early Universe constitute the Dark Matter? Possibly.\n\nDid the world need another PBH review? Absolutely. <LINK>', 'We give an overview of how such PBHs can form, focusing on the collapse of large density fluctuations in the early Universe. \n\nThen summarise current straights on the PBH abundance (as a fraction of the Dark Matter abundance in the Universe): https://t.co/7TOOv2tu7U', 'We digitised *a lot* of PBH bounds. You can find them all on line, along with code for generating the PBH abundance plots: https://t.co/HsCap8Rvup\n\nSpecial thanks to @adamcoogan1 for constantly letting me know whenever a new PBH paper came out...', ""I'm very grateful to @ultra_wimp for bringing me on board with this. \n\nShe did the lion's share of the work, but I was happy to help out with a fresh pair of eyes and my superhuman ability to click on lines in plots to digitise them."", '@ultra_wimp Oh I forgot to tell you, in the last read through, I corrected some ""f_{\\rm co}"" to ""f_{\\rm CO}"", so I consider my contribution invaluable.', '@cajohare @ultra_wimp https://t.co/fQI4tB1h0W']",20,07,1119
200,194,1417856009134018561,1256206943938584576,KavehDelfanazari,"In this pre-print, we propose voltage-controlled photonic integrated circuits with hybrid graphene-superconductor resonator arrays: active control of electromagnetic induced transparency: <LINK> @KDelfanazari @UofG_ENE @cambridge_ee @JoyceGroup_ @TheHofmannGroup",https://arxiv.org/abs/2107.03677,"Metamaterial photonic integrated circuits with arrays of hybrid graphene-superconductor coupled split-ring resonators (SRR) capable of modulating and slowing down terahertz (THz) light are introduced and proposed. The hybrid device optical responses, such as electromagnetic induced transparency (EIT) and group delay, can be modulated in several ways. First, it is modulated electrically by changing the conductivity and carrier concentrations in graphene. Alternatively, the optical response can be modified by acting on the device temperature sensitivity, by switching Nb from a lossy normal phase to a low-loss quantum mechanical phase below the transition temperature (Tc) of Nb. Maximum modulation depths of 57.3 % and 97.61 % are achieved for EIT and group delay at the THz transmission window, respectively. A comparison is carried out between the Nb-graphene-Nb coupled SRR-based devices with those of Au-graphene-Au SRRs and a significant enhancement of the THz transmission, group delay, and EIT responses are observed when Nb is in the quantum mechanical phase. Such hybrid devices with their reasonably large and tunable slow light bandwidth pave the way for the realization of active optoelectronic modulators, filters, phase shifters, and slow light devices for applications in chip-scale quantum communication and quantum processing. ","Active terahertz modulator and slow light metamaterial devices with
  hybrid graphene-superconductor photonic integrated circuits",1,"['In this pre-print, we propose voltage-controlled photonic integrated circuits with hybrid graphene-superconductor resonator arrays: active control of electromagnetic induced transparency:\n<LINK>\n@KDelfanazari @UofG_ENE @cambridge_ee @JoyceGroup_ @TheHofmannGroup']",21,07,262
201,232,1374257869038698497,760022547895377920,Federico Errica,"Really excited for this work! We perform a preliminary study on catastrophic forgetting for DGNs (deep graph nets). See you at WWW '21 GLB Workshop! Joint work with @Cossu94 @acarta7 and Davide Bacciu. <LINK> And, do not forget to check out our codebase to reproduce the results! It's an extension of PyDGN for Continual learning experiments! Apparently, reviewers appreciated it very much ;) <LINK>",https://arxiv.org/abs/2103.11750,"In this work, we study the phenomenon of catastrophic forgetting in the graph representation learning scenario. The primary objective of the analysis is to understand whether classical continual learning techniques for flat and sequential data have a tangible impact on performances when applied to graph data. To do so, we experiment with a structure-agnostic model and a deep graph network in a robust and controlled environment on three different datasets. The benchmark is complemented by an investigation on the effect of structure-preserving regularization techniques on catastrophic forgetting. We find that replay is the most effective strategy in so far, which also benefits the most from the use of regularization. Our findings suggest interesting future research at the intersection of the continual and graph representation learning fields. Finally, we provide researchers with a flexible software framework to reproduce our results and carry out further experiments. ","Catastrophic Forgetting in Deep Graph Networks: an Introductory
  Benchmark for Graph Classification",2,"[""Really excited for this work! We perform a preliminary study on catastrophic forgetting for DGNs (deep graph nets). See you at WWW '21 GLB Workshop!\nJoint work with @Cossu94 @acarta7 and Davide Bacciu.  <LINK>"", ""And, do not forget to check out our codebase to reproduce the results! It's an extension of PyDGN for Continual learning experiments! Apparently, reviewers appreciated it very much ;)\nhttps://t.co/XtLneaYy4N""]",21,03,399
202,47,1471592968712818692,110103071,Andrej Risteski,"New paper on the landscape and training dynamics of VAEs when trained on data supported on low-dimensional manifolds. Joint work with Fred Koehler, @thebigmehtaphor and Chenghui Zhou. <LINK> <LINK> Story starts w a very nice paper by Dai-Wipf <LINK>, who propose a 2-stage training algo for VAEs. Stage 1: Heuristic analysis of the standard VAE loss suggests optimal generator is supptd on the right manifold. Stage 2: Recovers the right density on manifold. Via a combination of theory+empirics, we show the full picture for Stage 1 is subtle. Even in the linear case---i.e. data is generated by a linear generator, and trained gen/enc are linear--- there exist minima s.t. generator support is a superset of the ground truth manifold. Training dynamics matter: in the linear case, we prove implicit bias of the GD dynamics is such that the generator recovers the right support. In the nonlinear case, via simulations, we show VAE training frequently recovers bad minima --- even for simple low-dim manifolds. Feels like there is a lot more to the story: e.g. maybe under more data distribution assumptions, something like this could work? Sample quality does improve in D-W --- is there a sense in which it ""partially"" works? Is there a ""multi-stage"" version of the strategy in D-W?",https://arxiv.org/abs/2112.06868,"Variational Autoencoders (VAEs) are one of the most commonly used generative models, particularly for image data. A prominent difficulty in training VAEs is data that is supported on a lower dimensional manifold. Recent work by Dai and Wipf (2019) suggests that on low-dimensional data, the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. In this paper, via a combination of theoretical and empirical results, we show that the story is more subtle. Precisely, we show that for linear encoders/decoders, the story is mostly true and VAE training does recover a generator with support equal to the ground truth manifold, but this is due to the implicit bias of gradient descent rather than merely the VAE loss itself. In the nonlinear case, we show that the VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold. ","Variational autoencoders in the presence of low-dimensional data:
  landscape and implicit bias",5,"['New paper on the landscape and training dynamics of VAEs when trained on data supported on low-dimensional manifolds. Joint work with Fred Koehler, @thebigmehtaphor and Chenghui Zhou.  <LINK> <LINK>', 'Story starts w a very nice paper by Dai-Wipf https://t.co/Z8KW3CjEbf, who propose a 2-stage training algo for VAEs. Stage 1: Heuristic analysis of the standard VAE loss suggests optimal generator is supptd on the right manifold. Stage 2: Recovers the right density on manifold.', 'Via a combination of theory+empirics, we show the full picture for Stage 1 is subtle. Even in the linear case---i.e. data is generated by a linear generator, and trained gen/enc are linear--- there exist minima s.t. generator support is a superset of the ground truth manifold.', 'Training dynamics matter: in the linear case, we prove implicit bias of the GD dynamics is such that the generator recovers the right support. In the nonlinear case, via simulations, we show VAE training frequently recovers bad minima --- even for simple low-dim manifolds.', 'Feels like there is a lot more to the story: e.g. maybe under more data distribution assumptions, something like this could work?  Sample quality does improve in D-W --- is there a sense in which it ""partially"" works? Is there a ""multi-stage"" version of the strategy in D-W?']",21,12,1284
203,25,1377169942252978181,481539448,Richard Alexander,"Our new paper on HD143006, led by @GBallabio, is out today. We found that the complex morphology of this system can be explained by a giant planet in the disc around a misaligned binary star - potentially the first known misaligned circumbinary planet. <LINK> <LINK> This project stems from an idea @GBallabio & @bec_nealon came up with about 18 months ago... HD143006 is maybe the weirdest protoplanetary disc in the @almaobs DSHARP survey. Based on sub-mm and near-IR observations, the disc appears to look like this (fig from Pérez+ 2018). <LINK> Giulia used @DiRAC_HPC simulations to show that neither a binary star nor a planet can explain all of the observed features. But a (co-planar) gas-giant planet orbiting in the disc around a *misaligned* binary is consistent with both the VLT/SPHERE and ALMA observations. <LINK> Our synthetic observations also match the disc's kinematics. The CO channel maps from @almaobs (white contours) show a ""kink"" that may be due to a planet in the disc, and the model (colour scale) matches *beautifully*. 🤩 <LINK> As with any misaligned system, *everything* precesses (movie below). That means the observed features in the disc are expected to move, perhaps in as little as 5-10yr. Which means we can test the model... <LINK> If we're right, HD143006 would be the first known example of a circumbinary planet (a ""Tatooine"") whose orbit is not co-planar with the binary star. And we can potentially confirm it in just a few years' time. 🔭 This has been one of the most fun projects I've been involved in, and credit goes to the whole team. @GBallabio & @bec_nealon did the heavy lifting, with help from me, @nomadastro, Christophe Pinte & @danprice_astro.👍🏻 I'll finish with another movie.😀 <LINK>",https://arxiv.org/abs/2103.16213,"Misalignments within protoplanetary discs are now commonly observed, and features such as shadows in scattered light images indicate departure from a co-planar geometry. VLT/SPHERE observations of the disc around HD 143006 show a large-scale asymmetry, and two narrow dark lanes which are indicative of shadowing. ALMA observations also reveal the presence of rings and gaps in the disc, along with a bright arc at large radii. We present new hydrodynamic simulations of HD 143006, and show that a configuration with both a strongly inclined binary and an outer planetary companion is the most plausible to explain the observed morphological features. We compute synthetic observations from our simulations, and successfully reproduce both the narrow shadows and the brightness asymmetry seen in IR scattered light. Additionally, we reproduce the large dust observed in the mm continuum, due to a 10 Jupiter mass planet detected in the CO kinematics. Our simulations also show the formation of a circumplanetary disc, which is misaligned with respect to the outer disc. The narrow shadows cast by the inner disc and the planet-induced ""kink"" in the disc kinematics are both expected to move on a time-scale of $\sim$ 5-10 years, presenting a potentially observable test of our model. If confirmed, HD 143006 would be the first known example of a circumbinary planet on a strongly misaligned orbit. ",HD 143006: circumbinary planet or misaligned disc?,7,"['Our new paper on HD143006, led by @GBallabio, is out today. We found that the complex morphology of this system can be explained by a giant planet in the disc around a misaligned binary star - potentially the first known misaligned circumbinary planet.\n<LINK> <LINK>', 'This project stems from an idea @GBallabio &amp; @bec_nealon came up with about 18 months ago...\n\nHD143006 is maybe the weirdest protoplanetary disc in the @almaobs DSHARP survey. Based on sub-mm and near-IR observations, the disc appears to look like this (fig from Pérez+ 2018). https://t.co/OnU2wbRDl5', 'Giulia used @DiRAC_HPC simulations to show that neither a binary star nor a planet can explain all of the observed features. But a (co-planar) gas-giant planet orbiting in the disc around a *misaligned* binary is consistent with both the VLT/SPHERE and ALMA observations. https://t.co/7HZeicyMPW', 'Our synthetic observations also match the disc\'s kinematics. The CO channel maps from @almaobs (white contours) show a ""kink"" that may be due to a planet in the disc, and the model (colour scale) matches *beautifully*. 🤩 https://t.co/97SvWi5nXa', 'As with any misaligned system, *everything* precesses (movie below). That means the observed features in the disc are expected to move, perhaps in as little as 5-10yr. Which means we can test the model...\nhttps://t.co/YGiGV1H6AY', 'If we\'re right, HD143006 would be the first known example of a circumbinary planet (a ""Tatooine"") whose orbit is not co-planar with the binary star. And we can potentially confirm it in just a few years\' time. 🔭', ""This has been one of the most fun projects I've been involved in, and credit goes to the whole team. @GBallabio &amp; @bec_nealon did the heavy lifting, with help from me, @nomadastro, Christophe Pinte &amp; @danprice_astro.👍🏻\n\nI'll finish with another movie.😀\nhttps://t.co/SiRMF1kbay""]",21,03,1739
204,173,1400478101855817730,171674815,Mark Marley,"I want to highlight our new paper, led by @exoEhsan, with @NatashaBatalha and @ChannonVisscher about Lithium in brown dwarfs: <LINK> <LINK> In low mass stars and the most massive brown dwarfs Lithium is lost to fusion in the core. Li is a little bit easier to 'burn' than H but not as easy as deuterium. So while the minimum mass to steadily burn H is around 75 Jupiter masses, the limit for Li burning is around 65 M_J. So brown dwarfs and stars older than around 250 Myr and more massive than 65 MJ do not have Li visible in their atmospheres. There is a strong Li line in the optical that makes Li detection feasible. This led Rafael Rebolo in the 90s to suggest the ""Li test"" as a way of distinguishing brown dwarfs from stars. If you could detect Li you knew you had to have a brown dwarf on your hand since it would be below 65 MJ. For objects which were hot enough to be either this was helpful If you have a cooler object, say one that has CH4, you KNOW you don't have a star on your hands, so the Li test seems less important. Also atomic Li goes into various other molecules and is removed, thus muddying the waters. However there aren't many spectral gravity indicators for brown dwarfs and it would be nice to have some marker for the most massive objects, those between ~65 MJ and ~75 MJ. Missing Li would serve nicely, but you have the chemistry removing Li as well as the nuclear fires. So the question becomes, can we detect the other Li molecules that show up at lower Teff? If so then we have a new lithium test for identifying the most massive brown dwarfs. But we need molecular opacities for species such as LiH, LiF, LiOH, and LiCl. This is where Ehsan comes in. As part of his NPP postdoc at Ames Ehsan computed himself some of these opacities and compiled others so that we could investigate the detectability of the various Li species, allowing us to follow the lithium through the brown dwarf cooling sequence You can read the paper for details, but some of these other Li species should be detectable in the mid-IR. Unfortunately the molecular signatures are more subtle than the atomic Li feature, but the 30m telescopes and perhaps JWST should be able to search for them. I'm hopeful that someday Li could be used to resolve some puzzles, such as the mass of Gl 229 B. Special thanks to Channon for handling all of the Li-species chemistry for us. Our spectra were computed with PICASO.",https://arxiv.org/abs/2106.00781,"Lithium is an important element for the understanding of ultracool dwarfs because it is lost to fusion at masses above $\sim 68\, M_{\rm J}$. Hence, the presence or absence of atomic Li has served as an indicator of the nearby H-burning boundary at about $75\,M_{\rm J}$ between brown-dwarfs and very low-mass stars. Historically the ""Lithium test"", a search for the presence and strength of the Li line at 670.8 nm, has been a marker if an object has a substellar mass with stellar-like spectral energy distribution (e.g., a late-type M dwarf). While the Li test could in principle also be used to distinguish masses of later-type L-T dwarfs, Li is predominantly no longer found as an atomic gas, but rather a molecular species such as LiH, LiF, LiOH, and LiCl in their cooler atmospheres. L- and T-type brown dwarfs are also quite faint at 670 nm and thus challenging targets for high resolution spectroscopy. But only recently have experimental molecular line lists become available for the molecular Li species, allowing molecular Li mass discrimination. In this study, we generated the latest opacity of each of these Li-bearing molecules and performed thermochemical equilibrium atmospheric composition calculation of the abundance of these molecules. Finally, we computed thermal emission spectra for a series of radiative-convective equilibrium models of cloudy and cloudless brown dwarf atmospheres (with $T_{\rm eff}=$ 500--2400~K, and $\log g$=4.0, 4.5, 5.0) to understand where the presence or absence of atmospheric lithium-bearing species is most easily detected as a function of brown dwarf mass and age. After atomic Li, the best spectral signatures were found to be LiF at $10.5-12.5$~\micron and LiCl at $14.5-18.5$ $\micron$. LiH also shows a narrow feature at $\sim 9.38$ $\micron$. ","Following the Lithium: Tracing Li-bearing Molecules Across Age, Mass,
  and Gravity in Brown Dwarfs",10,"['I want to highlight our new paper, led by @exoEhsan, with @NatashaBatalha and @ChannonVisscher about Lithium in brown dwarfs: <LINK> <LINK>', ""In low mass stars and the most massive brown dwarfs Lithium is lost to fusion in the core. Li is a little bit easier to 'burn' than H but not as easy as deuterium. So while the minimum mass to steadily burn H is around 75 Jupiter masses, the limit for Li burning is around 65 M_J."", 'So brown dwarfs and stars older than around 250 Myr and more massive than 65 MJ do not have Li visible in their atmospheres. There is a strong Li line in the optical that makes Li detection feasible.', 'This led Rafael Rebolo in the 90s to suggest the ""Li test"" as a way of distinguishing brown dwarfs from stars. If you could detect Li you knew you had to have a brown dwarf on your hand since it would be below 65 MJ. For objects which were hot enough to be either this was helpful', ""If you have a cooler object, say one that has CH4, you KNOW you don't have a star on your hands, so the Li test seems less important. Also atomic Li goes into various other molecules and is removed, thus muddying the waters."", ""However there aren't many spectral gravity indicators for brown dwarfs and it would be nice to have some marker for the most massive objects, those between ~65 MJ and ~75 MJ. Missing Li would serve nicely, but you have the chemistry removing Li as well as the nuclear fires."", 'So the question becomes, can we detect the other Li molecules that show up at lower Teff? If so then we have a new lithium test for identifying the most massive brown dwarfs. But we need molecular opacities for species such as LiH, LiF, LiOH, and LiCl.', 'This is where Ehsan comes in. As part of his NPP postdoc at Ames Ehsan computed himself some of these opacities and compiled others so that we could investigate the detectability of the various Li species, allowing us to follow the lithium through the brown dwarf cooling sequence', 'You can read the paper for details, but some of these other Li species should be detectable in the mid-IR. Unfortunately the molecular signatures are more subtle than the atomic Li feature, but the 30m telescopes and perhaps JWST should be able to search for them.', ""I'm hopeful that someday Li could be used to resolve some puzzles, such as the mass of Gl 229 B. Special thanks to Channon for handling all of the Li-species chemistry for us. Our spectra were computed with PICASO.""]",21,06,2415
205,85,973203630412173312,308587014,Robert Feldt,"Our mega-study (many, many authors :)) on artificial creativity now also on arxiv (in submission since last spring but we want to be able to also ref it during this (long) process...) <LINK> In particular, I'm very happy to be co-author with Karl Sims whose classic work on evolving virtual creatures helped me dare to apply artificial evolution in SW Engineering in my PhD work in late 90's: <LINK>",https://arxiv.org/abs/1803.03453,"Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems. ","The Surprising Creativity of Digital Evolution: A Collection of
  Anecdotes from the Evolutionary Computation and Artificial Life Research
  Communities",2,"['Our mega-study (many, many authors :)) on artificial creativity now also on arxiv (in submission since last spring but we want to be able to also ref it during this (long) process...)\n<LINK>', ""In particular, I'm very happy to be co-author with Karl Sims whose classic work on evolving virtual creatures helped me dare to apply artificial evolution in SW Engineering in my PhD work in late 90's:\nhttps://t.co/C2NMFhhuwq""]",18,03,399
206,129,1140965195998777345,954335153723183104,Luciano da F. Costa,"Consonance/dissonance are important properties of sound, and music is characterized by creative application of respective patterns. How would combinations of sounds be affected by amplification? We study this problem in this recent work: <LINK> <LINK>",https://arxiv.org/abs/1906.06559,"After briefly revising the concepts of consonance/dissonance, a respective mathematic-computational model is described, based on Helmholtz's consonance theory and also considering the partials intensity. It is then applied to characterize five scale temperaments, as well as some minor and major triads and electronic amplification. In spite of the simplicity of the described model, a surprising agreement is often observed between the obtained consonances/dissonances and the typically observed properties of scales and chords. The representation of temperaments as graphs where links correspond to consonance (or dissonance) is presented and used to compare distinct temperaments, allowing the identification of two main groups of scales. The interesting issue of nonlinearities in electronic music amplification is also addressed while considering quadratic distortions, and it is shown that such nonlinearities can have drastic effect in changing the original patterns of consonance and dissonance. ","Modeling Consonance and its Relationships with Temperament, Harmony, and
  Electronic Amplification",1,"['Consonance/dissonance are important properties of sound, and music is characterized by creative application of respective patterns.  How would combinations of sounds be affected by amplification?  We study this problem in this recent work:\n\n<LINK> <LINK>']",19,06,251
207,205,1252250966205894658,1251488051337068547,Alexandru Topîrceanu,"Our study of centralized and decentralized isolation strategies on the #COVID19 dynamics is here: <LINK> We adapt a custom SICARS epidemic model and use #NetSci #NetworkScience to quantify how essential isolation is. Two specific states of SICARS allow for understanding centralized C (government-imposed) and decentralized D (self-imposed) isolation (through social distancing) and their combined effects on limiting the number of infected people. <LINK> We also test what happens when the #COVID19 isolation measures are not applied immediately (like in Singapore, Hong Kong, China), but with a delay (like US, UK, Europe), or reactively (“waiting for it to happen”) instead of proactively (“let’s be prepared”). We are able to show that: • D isolation helps, but is insufficient on its own: -42% less infected. • C isolation is a must (reduces maximum degree in the network, hence it disables hubs or superspreaders from spreading the virus): -76% less infected. • The best response strategy is a hybrid one (C+D). That’s why it’s important to respect the government-imposed quarantine, but also be responsible and #StayHome : -87% less infected! <LINK> • If applied too late after the outbreak of the pandemic, even the combined (C+D) strategy loses its effectiveness significantly: +41% more infected (for a delay of 20 days), or 4x times more infected (for a delay of 50 days)! <LINK> • All isolation strategies applied proactively (at outbreak onset) are more effective than applied reactively (in response to the epidemic dynamics) even by progressively increasing the isolation severity. <LINK> • A higher patient relapse rate (e.g., 10%) than currently estimated for COVID-19 (&lt;1%) would not alter our conclusions on the effectiveness of the isolation strategies significantly. Find all the details in our #NetSci #COVID19 paper on arXiv: <LINK> <LINK>",https://arxiv.org/abs/2004.04222,"The infectious diseases are spreading due to human interactions enabled by various social networks. Therefore, when a new pathogen such as SARS-CoV-2 causes an outbreak, the non-pharmaceutical isolation strategies (e.g., social distancing) are the only possible response to disrupt its spreading. To this end, we introduce the new epidemic model (SICARS) and compare the centralized (C), decentralized (D), and combined (C+D) social distancing strategies, and analyze their efficiency to control the dynamics of COVID-19 on heterogeneous complex networks. Our analysis shows that the centralized social distancing is necessary to minimize the pandemic spreading. The decentralized strategy is insufficient when used alone, but offers the best results when combined with the centralized one. Indeed, the (C+D) is the most efficient isolation strategy at mitigating the network superspreaders and reducing the highest node degrees to less than 10% of their initial values. Our results also indicate that stronger social distancing, e.g., cutting 75% of social ties, can reduce the outbreak by 75% for the C isolation, by 33% for the D isolation, and by 87% for the (C+D) isolation strategy. Finally, we study the impact of proactive versus reactive isolation strategies, as well as their delayed enforcement. We find that the reactive response to the pandemic is less efficient, and delaying the adoption of isolation measures by over one month (since the outbreak onset in a region) can have alarming effects; thus, our study contributes to an understanding of the COVID-19 pandemic both in space and time. We believe our investigations have a high social relevance as they provide insights into understanding how different degrees of social distancing can reduce the peak infection ratio substantially; this can make the COVID-19 pandemic easier to understand and control over an extended period of time. ","Centralized and decentralized isolation strategies and their impact on
  the COVID-19 pandemic dynamics",8,"['Our study of centralized and decentralized isolation strategies on the #COVID19 dynamics is here: <LINK> We adapt a custom SICARS epidemic model and use #NetSci #NetworkScience to quantify how essential isolation is.', 'Two specific states of SICARS allow for understanding centralized C (government-imposed) and decentralized D (self-imposed) isolation (through social distancing) and their combined effects on limiting the number of infected people. https://t.co/PvzewrnBRs', 'We also test what happens when the #COVID19  isolation measures are not applied immediately (like in Singapore, Hong Kong, China), but with a delay (like US, UK, Europe), or reactively (“waiting for it to happen”) instead of proactively (“let’s be prepared”).', 'We are able to show that:\n• D isolation helps, but is insufficient on its own: -42% less infected. \n• C isolation is a must (reduces maximum degree in the network, hence it disables hubs or superspreaders from spreading the virus): -76% less infected.', '• The best response strategy is a hybrid one (C+D). That’s why it’s important to respect the government-imposed quarantine, but also be responsible and #StayHome : -87% less infected! https://t.co/oV5f2yMzOO', '• If applied too late after the outbreak of the pandemic, even the combined (C+D) strategy loses its effectiveness significantly: +41% more infected (for a delay of 20 days), or 4x times more infected (for a delay of 50 days)! https://t.co/0Iv7m6J8jW', '• All isolation strategies applied proactively (at outbreak onset) are more effective than applied reactively (in response to the epidemic dynamics) even by progressively increasing the isolation severity. https://t.co/So4PKu7LmR', '• A higher patient relapse rate (e.g., 10%) than currently estimated for COVID-19 (&lt;1%) would not alter our conclusions on the effectiveness of the isolation strategies significantly.\nFind all the details in our #NetSci #COVID19  paper on arXiv: https://t.co/eoHXp0nxN0 https://t.co/eM3kwDu9Vi']",20,04,1865
208,163,1400864196305297408,2485053080,Swarnadeep Saha,"New #NAACL2021 paper (next Tues) on explaining compositional reasoning w/ multiple proof graphs ""multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning""  Paper: <LINK> Code: <LINK>  @prateeky2806 @mohitban47 1/n <LINK> Compositional reasoning is not always unique -- there can be multiple ways of reaching the correct answer. With multiPRover, we extend our #EMNLP2020 work on PRover (which generates a single proof) by now tackling a more challenging task of generating a *set* of proof graphs. 2/n We find that PRover, when asked to generate top-k proofs, does not perform well & that multiple proofs for a ques. often have common subgraphs between them. So, to jointly learn from all proofs & exploit their correlations better, we pose it as a (graph) set-generation task. 3/n <LINK> We propose 2 multiPRover models: (1) Multilabel-multiPRover generating a set of proofs via multi-label classification & implicit conditioning between proofs, (2) Iterative-multiPRover generating proofs iteratively by explicitly conditioning on the previously generated proofs 4/n Both multiPRover models show signif. improvements on all synthetic, zero-shot, & human-paraphrased datasets (from RuleTakers). Iterative-multiPRover also outperforms PRover on a zero-shot dataset with all single-proof examples & has better generalization to higher depth ques. 5/n <LINK>",https://arxiv.org/abs/2106.01354,"We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent. Our code and models are publicly available at this https URL ","multiPRover: Generating Multiple Proofs for Improved Interpretability in
  Rule Reasoning",5,"['New #NAACL2021 paper (next Tues) on explaining compositional reasoning w/ multiple proof graphs ""multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning""\n \nPaper: <LINK>\nCode: <LINK>\n \n@prateeky2806 @mohitban47\n1/n <LINK>', 'Compositional reasoning is not always unique -- there can be multiple ways of reaching the correct answer. With multiPRover, we extend our #EMNLP2020 work on PRover (which generates a single proof) by now tackling a more challenging task of generating a *set* of proof graphs.\n2/n', 'We find that PRover, when asked to generate top-k proofs, does not perform well &amp; that multiple proofs for a ques. often have common subgraphs between them. So, to jointly learn from all proofs &amp; exploit their correlations better, we pose it as a (graph) set-generation task.\n3/n https://t.co/odAjr2k0B5', 'We propose 2 multiPRover models: (1) Multilabel-multiPRover generating a set of proofs via multi-label classification &amp; implicit conditioning between proofs, (2) Iterative-multiPRover generating proofs iteratively by explicitly conditioning on the previously generated proofs\n4/n', 'Both multiPRover models show signif. improvements on all synthetic, zero-shot, &amp; human-paraphrased datasets (from RuleTakers). Iterative-multiPRover also outperforms PRover on a zero-shot dataset with all single-proof examples &amp; has better generalization to higher depth ques.\n5/n https://t.co/fcRZ9SPpq8']",21,06,1387
209,11,1290269277140836360,1009799569390096384,Brenden Lake,"We train large-scale neural nets ""through the eyes"" of one baby across 2 years of development. New paper from Emin Orhan shows how high-level visual representations emerge from a subset of one baby's experience, through only self-supervised learning. <LINK> (1/2) This work was possible because of the *amazing* SAYCam dataset of baby headcam videos. Please check out the SAYCam paper here from Jess Sullivan, Michelle Mei, @AmyPerfors @ewojcik @mcxfrank <LINK> (2/2) @Werdnamai Great ideas. Yes for 1) and 2). For 3), that's a lot harder, because the videos are what they are! Action is a major factor missing from this kind of work.",http://arxiv.org/abs/2007.16189,"Within months of birth, children develop meaningful expectations about the world around them. How much of this early knowledge can be explained through generic learning mechanisms applied to sensory data, and how much of it requires more substantive innate inductive biases? Addressing this fundamental question in its full generality is currently infeasible, but we can hope to make real progress in more narrowly defined domains, such as the development of high-level visual categories, thanks to improvements in data collecting technology and recent progress in deep learning. In this paper, our goal is precisely to achieve such progress by utilizing modern self-supervised deep learning methods and a recent longitudinal, egocentric video dataset recorded from the perspective of three young children (Sullivan et al., 2020). Our results demonstrate the emergence of powerful, high-level visual representations from developmentally realistic natural videos using generic self-supervised learning objectives. ",Self-supervised learning through the eyes of a child,3,"['We train large-scale neural nets ""through the eyes"" of one baby across 2 years of development. New paper from Emin Orhan shows how high-level visual representations emerge from a subset of one baby\'s experience, through only self-supervised learning. <LINK> (1/2)', 'This work was possible because of the *amazing* SAYCam dataset of baby headcam videos. Please check out the SAYCam paper here from Jess Sullivan, Michelle Mei, @AmyPerfors @ewojcik @mcxfrank https://t.co/I574XmUfF7 (2/2)', ""@Werdnamai Great ideas. Yes for 1) and 2). For 3), that's a lot harder, because the videos are what they are! Action is a major factor missing from this kind of work.""]",20,07,634
210,142,1224611468517105666,1570476014,Yi-Hsuan Yang,"""Pop Music Transformer: Generating Music with Rhythm and Harmony""  \paper: <LINK> \demo: <LINK> \code: <LINK> ```We propose a new event representation of music that make it easy for models to count the beats``` #TaiwanAILabs @chrisdonahuey Thanks! The tracker works fairly well for pop music. We haven't tried other genres yet.",https://arxiv.org/abs/2002.00212,"A great number of deep learning based models have been recently proposed for automatic music composition. Among these models, the Transformer stands out as a prominent approach for generating expressive classical piano performance with a coherent structure of up to one minute. The model is powerful in that it learns abstractions of data on its own, without much human-imposed domain knowledge or constraints. In contrast with this general approach, this paper shows that Transformers can do even better for music modeling, when we improve the way a musical score is converted into the data fed to a Transformer model. In particular, we seek to impose a metrical structure in the input data, so that Transformers can be more easily aware of the beat-bar-phrase hierarchical structure in music. The new data representation maintains the flexibility of local tempo changes, and provides hurdles to control the rhythmic and harmonic structure of music. With this approach, we build a Pop Music Transformer that composes Pop piano music with better rhythmic structure than existing Transformer models. ","Pop Music Transformer: Beat-based Modeling and Generation of Expressive
  Pop Piano Compositions",2,"['""Pop Music Transformer: Generating Music with Rhythm and Harmony""  \n\\paper: <LINK>\n\\demo: <LINK>\n\\code: <LINK>\n\n```We propose a new event representation of music that make it easy for models to count the beats```\n#TaiwanAILabs', ""@chrisdonahuey Thanks! The tracker works fairly well for pop music.  We haven't tried other genres yet.""]",20,02,327
211,137,1511764384439316485,95734122,Clément Livache,"Our paper on optically excited ASE in a fully-working, high current quantum dot LED is available on arXiv! We show that we can make a solution-processed LED that supports light amplification thanks to new high-gain dots and reduced optical losses. <LINK>",https://arxiv.org/abs/2204.01929,"Laser diodes based on solution-processable materials could benefit numerous technologies including integrated electronics and photonics, telecommunication, and medical diagnostics. An attractive system for implementing these devices is colloidal semiconductor quantum dots (QDs). The primary challenge that hampered progress towards a QD laser diode (QLD) has been fast nonradiative Auger decay of optical-gain-active multicarrier states. Recently, this problem has been resolved by employing continuously graded QDs (cg-QDs) wherein Auger recombination is strongly suppressed. The use of these structures allowed for demonstrations of optical gain with electrical pumping and optically-excited lasing in multilayered LED-like devices. Here we report on achieving the next critical milestone towards a QLD, which is the demonstration of optically excited amplified spontaneous emission from a fully functional high-current density electroluminescent device. This advance has become possible due to excellent optical gain properties of novel 'compact' cg-QDs and a new LED architecture, which allows for concerted optimization of its optical and electrical properties. The results of this work strongly suggest the feasibility of the final step towards a functional QLD, which is the demonstration of lasing with electrical pumping. ","Optically Excited Two-Band Amplified Spontaneous Emission from a
  High-Current-Density Quantum-Dot LED",1,"['Our paper on optically excited ASE in a fully-working, high current quantum dot LED is available on arXiv! We show that we can make a solution-processed LED that supports light amplification thanks to new high-gain dots and reduced optical losses. <LINK>']",22,04,254
212,96,1057558437159206912,869154646694264832,Hugh Salimbeni,"Delighted to share our new paper, Gaussian Process Conditional Density Estimation <LINK>, appearing at NIPS. With @vdutor, @mpd37 and @jameshensman. If you’re a VAE person, you can see this as a conditional VAE with a Gaussian process for the decoder. If you’re a GP person, you can see this as a hybrid between (mutioutput) regression and the Bayesian-GPLVM. We combine observed inputs with latent variables via concatenation. On the GP side, we develop the GPLVM model in a number of ways: natural gradients (which are essential in the minibatch setting), correlated outputs (e.g. for a priori pixel correlations) and probabilistic linear projections of the inputs On the VAE side, we show being Bayesian about the mapping mitigates overfitting and improves test performance, especially in the low data regimes. It was great fun working on this project with the excellent people at @PROWLER_IO ! Particular shout out to @vdutor 👏👏👏",https://arxiv.org/abs/1810.12750,"Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GP) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images. ",Gaussian Process Conditional Density Estimation,6,"['Delighted to share our new paper, Gaussian Process Conditional Density Estimation <LINK>, appearing at NIPS. With @vdutor, @mpd37 and @jameshensman.', 'If you’re a VAE person, you can see this as a conditional VAE with a Gaussian process for the decoder.', 'If you’re a GP person, you can see this as a hybrid between (mutioutput) regression and the Bayesian-GPLVM. We combine observed inputs with latent variables via concatenation.', 'On the GP side, we develop the GPLVM model in a number of ways: natural gradients (which are essential in the minibatch setting), correlated outputs (e.g. for a priori pixel correlations) and probabilistic linear projections of the inputs', 'On the VAE side, we show being Bayesian about the mapping mitigates overfitting and improves test performance, especially in the low data regimes.', 'It was great fun working on this project with the excellent people at @PROWLER_IO ! Particular shout out to @vdutor 👏👏👏']",18,10,933
213,152,1410843177058127875,96253726,Navin Sridhar,"New paper! <LINK> The coronal plasmas of black holes are subject to inverse-Compton cooling by the soft ~blackbody photons from the accretion disk. What powers the hard, non-thermal X-rays from BH coronae despite this radiative cooling is an unsolved mystery... In this work, we show that the bulk motion of plasmoid chains—resulting from the reconnection of magnetic field lines anchored onto the accretion disk—can Compton up-scatter the soft disk photons into a hard, non-thermal spectrum. *This works despite a cooled-down coronal plasma* <LINK> We demonstrate this using first-principle PIC simulations of relativistic reconnection in pair plasmas for different levels of magnetization (σ) and seed photon density. The spectrum of bulk motions resembles a ~100 keV Maxwellian, and barely depends on the above two parameters. We perform Monte-Carlo calculations of the radiative transfer of seed photons through the reconnection layer laden with particles, whose momenta are obtained from PIC simulations: the σ=10 model describes the BH ""hard state"" spectrum remarkably well, across 3 decades in energy! <LINK> We welcome you to our paper—available on arXiv (<LINK>)—for a detailed dive into our calculations, assumptions, and results. Stay tuned to the second part of this series, where we will be modifying the composition of the corona! All comments appreciated! <LINK>",https://arxiv.org/abs/2107.00263,"We perform two-dimensional particle-in-cell simulations of reconnection in magnetically dominated electron-positron plasmas subject to strong Compton cooling. We vary the magnetization $\sigma\gg1$, defined as the ratio of magnetic tension to plasma inertia, and the strength of cooling losses. Magnetic reconnection under such conditions can operate in magnetically dominated coronae around accreting black holes, which produce hard X-rays through Comptonization of seed soft photons. We find that the particle energy spectrum is dominated by a peak at mildly relativistic energies, which results from bulk motions of cooled plasmoids. The peak has a quasi-Maxwellian shape with an effective temperature of $\sim 100$ keV, which depends only weakly on the flow magnetization and the strength of radiative cooling. The mean bulk energy of the reconnected plasma is roughly independent of $\sigma$, whereas the variance is larger for higher magnetizations. The spectra also display a high-energy tail, which receives $\sim 25$% of the dissipated reconnection power for $\sigma=10$ and $\sim 40$% for $\sigma=40$. We complement our particle-in-cell studies with a Monte-Carlo simulation of the transfer of seed soft photons through the reconnection layer, and find the escaping X-ray spectrum. The simulation demonstrates that Comptonization is dominated by the bulk motions in the chain of Compton-cooled plasmoids and, for $\sigma\sim 10$, yields a spectrum consistent with the typical hard state of accreting black holes. ","Comptonization by Reconnection Plasmoids in Black Hole Coronae I:
  Magnetically Dominated Pair Plasma",5,"['New paper!\n<LINK>\nThe coronal plasmas of black holes are subject to inverse-Compton cooling by the soft ~blackbody photons from the accretion disk. What powers the hard, non-thermal X-rays from BH coronae despite this radiative cooling is an unsolved mystery...', 'In this work, we show that the bulk motion of plasmoid chains—resulting from the reconnection of magnetic field lines anchored onto the accretion disk—can Compton up-scatter the soft disk photons into a hard, non-thermal spectrum. *This works despite a cooled-down coronal plasma* https://t.co/jQy5UDZsT1', 'We demonstrate this using first-principle PIC simulations of relativistic reconnection in pair plasmas for different levels of magnetization (σ) and seed photon density. The spectrum of bulk motions resembles a ~100 keV Maxwellian, and barely depends on the above two parameters.', 'We perform Monte-Carlo calculations of the radiative transfer of seed photons through the reconnection layer laden with particles, whose momenta are obtained from PIC simulations: the σ=10 model describes the BH ""hard state"" spectrum remarkably well, across 3 decades in energy! https://t.co/T1OGjAq48Q', 'We welcome you to our paper—available on arXiv (https://t.co/8Z3diNKhOF)—for a detailed dive into our calculations, assumptions, and results. Stay tuned to the second part of this series, where we will be modifying the composition of the corona! All comments appreciated! https://t.co/6qkp5SaFcp']",21,07,1377
214,17,1012126279871627265,2902658140,Sander Dieleman,Stacking WaveNet autoencoders on top of each other leads to raw audio models that can capture long-range structure in music. Check out our new paper: <LINK> Listen to some minute-long piano music samples: <LINK> <LINK> more unconditional samples and reconstructions are available here: <LINK>,https://arxiv.org/abs/1806.10474,"Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds. ","The challenge of realistic music generation: modelling raw audio at
  scale",2,"['Stacking WaveNet autoencoders on top of each other leads to raw audio models that can capture long-range structure in music. Check out our new paper: <LINK>\n\nListen to some minute-long piano music samples: <LINK> <LINK>', 'more unconditional samples and reconstructions are available here: https://t.co/Mu6Cp11LKw']",18,06,292
215,117,1106840083217477632,1095087149932822534,Will Smith,New paper: <LINK> appearing as an oral @cvpr2019. We release CFHM (Combined Face and Head Model) a new publicly available 3D morphable model built by merging LSFM trained on 10k face scans and LYHM trained on 1.2k full heard scans. <LINK> Favourite result in the paper: reconstructing bald Agassi from Agassi with a mullet! <LINK>,https://arxiv.org/abs/1903.03785,"Three-dimensional Morphable Models (3DMMs) are powerful statistical tools for representing the 3D surfaces of an object class. In this context, we identify an interesting question that has previously not received research attention: is it possible to combine two or more 3DMMs that (a) are built using different templates that perhaps only partly overlap, (b) have different representation capabilities and (c) are built from different datasets that may not be publicly-available? In answering this question, we make two contributions. First, we propose two methods for solving this problem: i. use a regressor to complete missing parts of one model using the other, ii. use the Gaussian Process framework to blend covariance matrices from multiple models. Second, as an example application of our approach, we build a new face-and-head shape model that combines the variability and facial detail of the LSFM with the full head modelling of the LYHM. The resulting combined shape model achieves state-of-the-art performance and outperforms existing head models by a large margin. Finally, as an application experiment, we reconstruct full head representations from single, unconstrained images by utilizing our proposed large-scale model in conjunction with the FaceWarehouse blendshapes for handling expressions. ",Combining 3D Morphable Models: A Large scale Face-and-Head Model,2,"['New paper: <LINK> appearing as an oral @cvpr2019. We release CFHM (Combined Face and Head Model) a new publicly available 3D morphable model built by merging LSFM trained on 10k face scans and LYHM trained on 1.2k full heard scans. <LINK>', 'Favourite result in the paper: reconstructing bald Agassi from Agassi with a mullet! https://t.co/QOSgcLbNDq']",19,03,330
216,74,1417135351592800256,724917570705461248,Jan van Roestel,"On ArXiv today; our paper on 5 new eclipsing AM CVn systems discovered with @ZTFsurvey <LINK> AM CVn systems are a rare type of accreting white dwarf with a degenerate donor. @ztfsurvey We found these 5 systems by analysing the ZTF lightcurves of all white dwarfs (identified using Gaia). We specifically searched for dimming events that occurred with a period of &lt;70 minutes. We estimate that, as ZTF continues to observe, 1-4 more can be found. <LINK> Eclipsing systems are very valuable because they allow the mass and radius of both components to be measured (mostly) independent of models. This shows that the white dwarf are more massive than typical single white dwarfs (~0.8 solar mass instead of ~0.5). Even more interesting, the donor stars with a mass of just 1-4% of a solar mass seem to be inflated and large. Comparison with models suggests that they had Helium stars as a progenitor, but the long period systems are still large compared to these models. <LINK> They are spectroscopically also very interesting because we are seeing the elements from the core-remnant of a star! Besides helium, a lot of metal lines including potassium and zinc which have not been seen before in AM CVn systems <LINK>",https://arxiv.org/abs/2107.07573,"AM CVn systems are ultra-compact, helium-rich, accreting binaries with degenerate or semi-degenerate donors. We report the discovery of five new eclipsing AM CVn systems with orbital periods of 61.5, 55.5, 53.3, 37.4, and 35.4 minutes. These systems were discovered by searching for deep eclipses in the Zwicky Transient Facility (ZTF) lightcurves of white dwarfs selected using Gaia parallaxes. We obtained phase-resolved spectroscopy to confirm that all systems are AM CVn binaries, and we obtained high-speed photometry to confirm the eclipse and characterize the systems. The spectra of two long-period systems (61.5 and 53.3 minutes) show many emission and absorption lines, indicating the presence of N, O, Na, Mg, Si, and Ca, and also the K and Zn, elements which have never been detected in AM CVn systems before. By modelling the high-speed photometry, we measured the mass and radius of the donor star, potentially constraining the evolutionary channel that formed these AM CVn systems. We determined that the average mass of the accreting white dwarf is $\approx0.8$$\mathrm{M_{\odot}}$, and that the white dwarfs in long-period systems are hotter than predicted by recently updated theoretical models. The donors have a high entropy and are a factor of $\approx$ 2 more massive compared to zero-entropy donors at the same orbital period. The large donor radius is most consistent with He-star progenitors, although the observed spectral features seem to contradict this. The discovery of 5 new eclipsing AM~CVn systems is consistent with the known observed AM CVn space density and estimated ZTF recovery efficiency. Based on this estimate, we expect to find another 1--4 eclipsing AM CVn systems as ZTF continues to obtain data. This will further increase our understanding of the population, but will require high precision data to better characterize these 5 systems and any new discoveries. ",Discovery and characterization of five new eclipsing AM CVn systems,5,"['On ArXiv today; our paper on 5 new eclipsing AM CVn systems discovered with @ZTFsurvey <LINK> \nAM CVn systems are a rare type of accreting white dwarf with a degenerate donor.', '@ztfsurvey We found these 5 systems by analysing the ZTF lightcurves of all white dwarfs (identified using Gaia). We specifically searched for dimming events that occurred with a period of &lt;70 minutes. We estimate that, as ZTF continues to observe, 1-4 more can be found. https://t.co/H41J3vNAd0', 'Eclipsing systems are very valuable because they allow the mass and radius of both components to be measured (mostly) independent of models. This shows that the white dwarf are more massive than typical single white dwarfs (~0.8 solar mass instead of ~0.5).', 'Even more interesting, the donor stars with a mass of just 1-4% of a solar mass seem to be inflated and large. Comparison with models suggests that they had Helium stars as a progenitor, but the long period systems are still large compared to these models. https://t.co/fwAP8m2NMp', 'They are spectroscopically also very interesting because we are seeing the elements from the core-remnant of a star! Besides helium, a lot of metal lines including potassium and zinc which have not been seen before in AM CVn systems https://t.co/gCV1J1j0mT']",21,07,1218
217,186,1336515249063800832,2783180568,Max Radin,"Excited to share our study comparing state-of-the-art VQE to classical quantum-chemistry methods! We found that for small organic molecules, classical methods are much faster. Very grateful to have worked with @jfgonthier_qc, @RomeroFontalvoJ, and @bp_plc <LINK> <LINK>",https://arxiv.org/abs/2012.04001,"Recent advances in Noisy Intermediate-Scale Quantum (NISQ) devices have brought much attention to the potential of the Variational Quantum Eigensolver (VQE) and related techniques to provide practical quantum advantage in computational chemistry. However, it is not yet clear whether such algorithms, even in the absence of device error, could achieve quantum advantage for systems of practical interest and how large such an advantage might be. To address these questions, we have performed an exhaustive set of benchmarks to estimate number of qubits and number of measurements required to compute the combustion energies of small organic molecules to within chemical accuracy using VQE as well as state-of-the-art classical algorithms. We consider several key modifications to VQE, including the use of Frozen Natural Orbitals, various Hamiltonian decomposition techniques, and the application of fermionic marginal constraints. Our results indicate that although Frozen Natural Orbitals and low-rank factorizations of the Hamiltonian significantly reduce the qubit and measurement requirements, these techniques are not sufficient to achieve practical quantum computational advantage in the calculation of organic molecule combustion energies. This suggests that new approaches to estimation leveraging quantum coherence, such as Bayesian amplitude estimation [arxiv:2006.09350, arxiv:2006.09349], may be required in order to achieve practical quantum advantage with near-term devices. Our work also highlights the crucial role that resource and performance assessments of quantum algorithms play in identifying quantum advantage and guiding quantum algorithm design. ","Identifying challenges towards practical quantum advantage through
  resource estimation: the measurement roadblock in the variational quantum
  eigensolver",1,"['Excited to share our study comparing state-of-the-art VQE to classical quantum-chemistry methods! We found that for small organic molecules, classical methods are much faster. Very grateful to have worked with @jfgonthier_qc, @RomeroFontalvoJ, and @bp_plc <LINK> <LINK>']",20,12,269
218,46,1276250079356170240,1133088397273313282,francesco croce,"Check out our new paper on sparse black-box perturbations! New SOTA in query efficiency and success rate. For L0 attacks, changing *only 0.1% pixels* is sufficient to break the models. @maksym_andr  Paper: <LINK> Code: <LINK> (1/n) <LINK> Sparse perturbations are challenging for gradient-based methods because of the combinatorial constraints. To tackle this problem, we propose a flexible framework based on random search that naturally handles complicated constraints and leads to query-efficient attacks. (2/n) <LINK> Our framework leads to the first black-box patch and frame attacks that don’t rely on extra knowledge such as a surrogate model. Having access to a surrogate model is a very strong assumption and it can be very expensive to obtain. (3/n) <LINK> Moreover, although transfer attacks rely on surrogate models, they in general *do not perform well* and patches/frames are no exceptions. Sparse-RS outperforms transfer attacks (Tr-PGD) by a large margin in terms of the success rate. (4/n) <LINK> The resulting adversarial patches are very different from the patches found with white-box PGD and some of them are quite interpretable (see the patch for class Peacock). (5/n) <LINK> We show the versatility of Sparse-RS framework by generating both image-specific and universal patches/frames + L0 perturbations for images and malware. Sparse-RS outperforms other methods on all these threat models including **white-box** L0-PGD attack, see PGD_0 (wb). (6/n) <LINK> Takeaway message: don’t spend queries on estimating the gradient, do random search with a properly chosen sampling distribution! More details are in the paper: <LINK> (7/n)",https://arxiv.org/abs/2006.12834,"We propose a versatile framework based on random search, Sparse-RS, for score-based sparse targeted and untargeted attacks in the black-box setting. Sparse-RS does not rely on substitute models and achieves state-of-the-art success rate and query efficiency for multiple sparse attack models: $l_0$-bounded perturbations, adversarial patches, and adversarial frames. The $l_0$-version of untargeted Sparse-RS outperforms all black-box and even all white-box attacks for different models on MNIST, CIFAR-10, and ImageNet. Moreover, our untargeted Sparse-RS achieves very high success rates even for the challenging settings of $20\times20$ adversarial patches and $2$-pixel wide adversarial frames for $224\times224$ images. Finally, we show that Sparse-RS can be applied to generate targeted universal adversarial patches where it significantly outperforms the existing approaches. The code of our framework is available at this https URL ","Sparse-RS: a versatile framework for query-efficient sparse black-box
  adversarial attacks",7,"['Check out our new paper on sparse black-box perturbations! New SOTA in query efficiency and success rate. For L0 attacks, changing *only 0.1% pixels* is sufficient to break the models. @maksym_andr \n\nPaper: <LINK>\nCode: <LINK>\n(1/n) <LINK>', 'Sparse perturbations are challenging for gradient-based methods because of the combinatorial constraints. To tackle this problem, we propose a flexible framework based on random search that naturally handles complicated constraints and leads to query-efficient attacks.\n(2/n) https://t.co/1bjL34J7uJ', 'Our framework leads to the first black-box patch and frame attacks that don’t rely on extra knowledge such as a surrogate model. Having access to a surrogate model is a very strong assumption and it can be very expensive to obtain.\n(3/n) https://t.co/xUDwvkXzpM', 'Moreover, although transfer attacks rely on surrogate models, they in general *do not perform well* and patches/frames are no exceptions. Sparse-RS outperforms transfer attacks (Tr-PGD) by a large margin in terms of the success rate.\n(4/n) https://t.co/z4Ijtk6mcM', 'The resulting adversarial patches are very different from the patches found with white-box PGD and some of them are quite interpretable (see the patch for class Peacock).\n(5/n) https://t.co/mWatfd4Ybc', 'We show the versatility of Sparse-RS framework by generating both image-specific and universal patches/frames + L0 perturbations for images and malware.  Sparse-RS outperforms other methods on all these threat models including **white-box** L0-PGD attack, see PGD_0 (wb).\n(6/n) https://t.co/X5bFdrWbT6', 'Takeaway message: don’t spend queries on estimating the gradient, do random search with a properly chosen sampling distribution! \nMore details are in the paper: https://t.co/8MvRlRrdr7\n(7/n)']",20,06,1654
219,161,1476575324196511746,937194665991987200,Enrique Solano,"Last masterpiece 2021 <LINK> We propose encoding non-Markovian quantum dynamics of one and two coupled quantum memristors onto digital quantum computers, paving the way to Neuromorphic Quantum Computing in NISQ era @QuantumFlagship @QuantenTech @QuantumDaily <LINK>",https://arxiv.org/abs/2112.14660,"We propose the encoding of memristive quantum dynamics on a digital quantum computer. Using a set of auxiliary qubits, we simulate an effective non-Markovian environment inspired by a collisional model, reproducing memristive features between expectation values of different operators in a single qubit. We numerically test our proposal in an IBM quantum simulator with 32 qubits, obtaining the pinched hysteresis curve that is characteristic of a quantum memristor. Furthermore, we extend our method to the case of two coupled quantum memristors, opening the door to the study of neuromorphic quantum computing in the NISQ era. ",Quantum Memristors with Quantum Computers,1,"['Last masterpiece 2021 <LINK> We propose encoding non-Markovian quantum dynamics of one and two coupled quantum memristors onto digital quantum computers, paving the way to Neuromorphic Quantum Computing in NISQ era @QuantumFlagship @QuantenTech @QuantumDaily <LINK>']",21,12,265
220,341,1313109648023719936,182730982,Andreas Rücklé,"I'm excited to share “MultiCQA”, accepted at @EMNLP2020 We train 140 models on different domains and surprisingly find that neither domain similarity nor data size are critical factors for the best zero-shot transferability. <LINK> \w @PfeiffJo IGurevych <LINK> We train text matching models on all English StackExchange forums with self-supervision. The majority of our 140 models outperforms common IR baselines on non-factoid answer selection and question similarity tasks. <LINK> Our zero-shot MultiCQA model incorporates self-supervised and supervised multi-task learning on all source domains, and outperforms the in-domain SoTA on six evaluation benchmarks. @lintool @emnlp2020 @PfeiffJo That's great work, thanks for pointing it out! We must have missed it and will add it to the camera ready version!",https://arxiv.org/abs/2010.00980,"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks. ","MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on
  a Massive Scale",4,"[""I'm excited to share “MultiCQA”, accepted at @EMNLP2020\n\nWe train 140 models on different domains and surprisingly find that neither domain similarity nor data size are critical factors for the best zero-shot transferability.\n\n<LINK>\n\\w  @PfeiffJo IGurevych <LINK>"", 'We train text matching models on all English StackExchange forums with self-supervision. The majority of our 140 models outperforms common IR baselines on non-factoid answer selection and question similarity tasks. https://t.co/7bWWYOoyh1', 'Our zero-shot MultiCQA model incorporates self-supervised and supervised multi-task learning on all source domains, and outperforms the in-domain SoTA on six evaluation benchmarks.', ""@lintool @emnlp2020 @PfeiffJo That's great work, thanks for pointing it out! We must have missed it and will add it to the camera ready version!""]",20,10,809
221,63,1339421633572827139,864023169212055552,Mojtaba Raouf,Our new paper using the @SAMI_survey has been accepted for publication in ApJ. This study offers the first evidence that the dynamical state of galaxy groups may influence the BGG’s stellar and gas kinematics. The version of arxiv can be found here: <LINK> <LINK>,http://arxiv.org/abs/2012.08634,"We study the stellar and gas kinematics of the brightest group galaxies (BGGs) in dynamically relaxed and unrelaxed galaxy groups for a sample of 154 galaxies in the SAMI galaxy survey. We characterize the dynamical state of the groups using the luminosity gap between the two most luminous galaxies and the BGG offset from the luminosity centroid of the group. We find that the misalignment between the rotation axis of gas and stellar components is more frequent in the BGGs in unrelaxed groups, although with quite low statistical significance. Meanwhile galaxies whose stellar dynamics would be classified as `regular rotators' based on their kinemetry are more common in relaxed groups. We confirm that this dependency on group dynamical state remains valid at fixed stellar mass and Sersic index. The observed trend could potentially originate from a differing BGG accretion history in virialised and evolving groups. Amongst the halo relaxation probes, the group BGG offset appears to play a stronger role than the luminosity gap on the stellar kinematic differences of the BGGs. However, both the group BGG offset and luminosity gap appear to roughly equally drive the misalignment between the gas and stellar component of the BGGs in one direction. This study offers the first evidence that the dynamical state of galaxy groups may influence the BGG's stellar and gas kinematics and calls for further studies using a larger sample with higher signal-to-noise. ","The SAMI Galaxy Survey: Kinematics of stars and gas in brightest group
  galaxies; the role of group dynamics",1,['Our new paper using the @SAMI_survey has been accepted for publication in ApJ.  This study offers the first evidence that the dynamical state of galaxy groups may influence the BGG’s stellar and gas kinematics. The version of arxiv can be found here: <LINK> <LINK>'],20,12,263
222,106,1225785674487517185,4249537197,Christian Wolf,"New paper: we imbue Deep-RL agents for 3D environments with inductive bias using projective geometry, and we show that the algorithm automatically discovers objet affordances and places objects on a map. Work by @edwardbeeching + J. Dibangoye, O. Simonin. <LINK> <LINK>",https://arxiv.org/abs/2002.02286,"Tasks involving localization, memorization and planning in partially observable 3D environments are an ongoing challenge in Deep Reinforcement Learning. We present EgoMap, a spatially structured neural memory architecture. EgoMap augments a deep reinforcement learning agent's performance in 3D environments on challenging tasks with multi-step objectives. The EgoMap architecture incorporates several inductive biases including a differentiable inverse projection of CNN feature vectors onto a top-down spatially structured map. The map is updated with ego-motion measurements through a differentiable affine transform. We show this architecture outperforms both standard recurrent agents and state of the art agents with structured memory. We demonstrate that incorporating these inductive biases into an agent's architecture allows for stable training with reward alone, circumventing the expense of acquiring and labelling expert trajectories. A detailed ablation study demonstrates the impact of key aspects of the architecture and through extensive qualitative analysis, we show how the agent exploits its structured internal memory to achieve higher performance. ",EgoMap: Projective mapping and structured egocentric memory for Deep RL,1,"['New paper: we imbue Deep-RL agents for 3D environments with inductive bias using projective geometry, and we show that the algorithm automatically discovers objet affordances and places objects on a map. Work by @edwardbeeching + J. Dibangoye, O. Simonin. <LINK> <LINK>']",20,02,269
223,75,1459114254863265793,54789976,Sharath Adavanne,"📢 New paper: A multi-source localization and tracking approach for dynamic sound scenes with varying numbers of sources.  'Differentiable Tracking-Based Training of Deep Learning Sound Source Localizers' with Archontis Politis and @TuomasVirt at WASPAA <LINK> DNN-based sound source localization models are commonly set as classification or regression tasks. Although the classification approach has shown good results, it is challenging to scale it to higher resolution, multi-source localization, and tracking. <LINK> The multi-output regression-based approach is an alternative solution, however, training it is challenging. Especially, when the number of sources is varying. We solve this multi-output regression problem by employing our novel deep-learning-based Hungarian network that solves the assignment problem between the varying numbers of predicted and reference locations. Code available here: <LINK> We further formulate the popular tracking-based metrics of CLEAR-MOT as a differential objective function and optimise the multi source localizer directly on it. The proposed training procedure results in large improvements in localization error, detection metrics, and tracking capabilities. Code available here: <LINK>",https://arxiv.org/abs/2111.00030,"Data-based and learning-based sound source localization (SSL) has shown promising results in challenging conditions, and is commonly set as a classification or a regression problem. Regression-based approaches have certain advantages over classification-based, such as continuous direction-of-arrival estimation of static and moving sources. However, multi-source scenarios require multiple regressors without a clear training strategy up-to-date, that does not rely on auxiliary information such as simultaneous sound classification. We investigate end-to-end training of such methods with a technique recently proposed for video object detectors, adapted to the SSL setting. A differentiable network is constructed that can be plugged to the output of the localizer to solve the optimal assignment between predictions and references, optimizing directly the popular CLEAR-MOT tracking metrics. Results indicate large improvements over directly optimizing mean squared errors, in terms of localization error, detection metrics, and tracking capabilities. ","Differentiable Tracking-Based Training of Deep Learning Sound Source
  Localizers",6,"[""📢 New paper: A multi-source localization and tracking approach for dynamic sound scenes with varying numbers of sources. \n\n'Differentiable Tracking-Based Training of Deep Learning Sound Source Localizers' with Archontis Politis and @TuomasVirt at WASPAA\n\n<LINK>"", 'DNN-based sound source localization models are commonly set as classification or regression tasks. Although the classification approach has shown good results, it is challenging to scale it to higher resolution, multi-source localization, and tracking. https://t.co/d4o7tNguhb', 'The multi-output regression-based approach is an alternative solution, however, training it is challenging. Especially, when the number of sources is varying.', 'We solve this multi-output regression problem by employing our novel deep-learning-based Hungarian network that solves the assignment problem between the varying numbers of predicted and reference locations. Code available here: https://t.co/Z4XrP3s6ge', 'We further formulate the popular tracking-based metrics of CLEAR-MOT as a differential objective function and optimise the multi source localizer directly on it.', 'The proposed training procedure results in large improvements in localization error, detection metrics, and tracking capabilities. Code available here:  https://t.co/hYAoUY1J3y']",21,11,1235
224,248,1271027698157182978,1259188770,Olivier Bachem,"We spent the last months at @GoogleAI Zurich & Paris trying to understand on-policy RL for locomotion using a large scale study (&gt;250'000 models). Check out <LINK> for insights and practical recommendations on &gt;50 high- and low-level implementation decisions! 1/2 <LINK> This is joint work w/ Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, @RaphaelMarinier, @leonardhussenot, Matthieu Geist, Olivier Pietquin, @MMMichalski, @sylvain_gelly. 2/2",https://arxiv.org/abs/2006.05990,"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents. ","What Matters In On-Policy Reinforcement Learning? A Large-Scale
  Empirical Study",2,"[""We spent the last months at @GoogleAI Zurich &amp; Paris trying to understand on-policy RL for locomotion using a large scale study (&gt;250'000 models). Check out <LINK> for insights and practical recommendations on &gt;50 high- and low-level implementation decisions! 1/2 <LINK>"", 'This is joint work w/ Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, @RaphaelMarinier, @leonardhussenot, Matthieu Geist, Olivier Pietquin, @MMMichalski, @sylvain_gelly. 2/2']",20,06,482
225,261,1275399447019696129,1007655562908139520,Fabian Fuchs,"How should we leverage symmetries in point cloud tasks while maintaining detail? We propose combining the powerful concepts of self-attention and equivariance. Delighted to share the 🔥SE3-Transformer🔥 with @deworrall92, Volker Fischer & @wellingmax  <LINK> <LINK>",https://arxiv.org/abs/2006.10503,"We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model. The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds and graphs with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy N-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention. ",SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,1,"['How should we leverage symmetries in point cloud tasks while maintaining detail? We propose combining the powerful concepts of self-attention and equivariance.\n\nDelighted to share the 🔥SE3-Transformer🔥 with @deworrall92, Volker Fischer &amp; @wellingmax \n\n<LINK> <LINK>']",20,06,263
226,293,1320820554321055746,3524520857,Piotr Żelasko,"In our new study, we aim to gain some insights into the limitations of zero-shot ASR transfer to an unknown language. This work was done together with @SFeng9, Laureano Moro, Ali Abavisani, @OScharenborg, @hasegawajohnson, and Najim Dehak. 🔗<LINK> <LINK>",https://arxiv.org/abs/2010.12104,"The idea of combining multiple languages' recordings to train a single automatic speech recognition (ASR) model brings the promise of the emergence of universal speech representation. Recently, a Transformer encoder-decoder model has been shown to leverage multilingual data well in IPA transcriptions of languages presented during training. However, the representations it learned were not successful in zero-shot transfer to unseen languages. Because that model lacks an explicit factorization of the acoustic model (AM) and language model (LM), it is unclear to what degree the performance suffered from differences in pronunciation or the mismatch in phonotactics. To gain more insight into the factors limiting zero-shot ASR transfer, we replace the encoder-decoder with a hybrid ASR system consisting of a separate AM and LM. Then, we perform an extensive evaluation of monolingual, multilingual, and crosslingual (zero-shot) acoustic and language models on a set of 13 phonetically diverse languages. We show that the gain from modeling crosslingual phonotactics is limited, and imposing a too strong model can hurt the zero-shot transfer. Furthermore, we find that a multilingual LM hurts a multilingual ASR system's performance, and retaining only the target language's phonotactic data in LM training is preferable. ",How Phonotactics Affect Multilingual and Zero-shot ASR Performance,1,"['In our new study, we aim to gain some insights into the limitations of zero-shot ASR transfer to an unknown language. This work was done together with @SFeng9, Laureano Moro, Ali Abavisani, @OScharenborg, @hasegawajohnson, and Najim Dehak.\n\n🔗<LINK> <LINK>']",20,10,254
227,103,1199715545068650496,838292815,Ofir Nachum,"<LINK> Offline RL -what do you need to know about this notoriously difficult regime? Although recent papers propose a variety of algorithmic novelties, we find many of these unnecessary in practice. Extensive studies will hopefully guide future research &practice <LINK>",https://arxiv.org/abs/1911.11361,"In reinforcement learning (RL) research, it is common to assume access to direct online interactions with the environment. However in many real-world applications, access to the environment is limited to a fixed offline dataset of logged experience. In such settings, standard RL algorithms have been shown to diverge or otherwise yield poor performance. Accordingly, recent work has suggested a number of remedies to these issues. In this work, we introduce a general framework, behavior regularized actor critic (BRAC), to empirically evaluate recently proposed methods as well as a number of simple baselines across a variety of offline continuous control tasks. Surprisingly, we find that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Additional ablations provide insights into which design choices matter most in the offline RL setting. ",Behavior Regularized Offline Reinforcement Learning,1,"['<LINK> Offline RL -what do you need to know about this notoriously difficult regime? Although recent papers propose a variety of algorithmic novelties, we find many of these unnecessary in practice. Extensive studies will hopefully guide future research &amp;practice <LINK>']",19,11,270
228,156,1395721790064926720,1238126323714801665,Dr. Shweta Dalal,"New paper on arXiv today -<LINK> We detect 6 cool Jupiters, 3 brown dwarfs, and 16 low-mass stars with the radial velocity method using the SOPHIE spectrograph at OHP. We also perform astrometry analyses using Hipparcos and Gaia data! @FlavienKiefer <LINK> One of the new cool Jupiters, BD+631405 b (msini = 3.96 Mj and e= 0.88), adds to the population of highly eccentric cool Jupiters, and it is presently the most massive member. <LINK> Another interesting star is HD 205521, which has a companion with msini ~ 26 Mj but our analyses of the Hipparcos and Gaia astrometric data both find that the orbit is close to face-on and that the companion is actually a star. <LINK>",https://arxiv.org/abs/2105.09741,"Distinguishing classes within substellar objects and understanding their formation and evolution need larger samples of substellar companions such as exoplanets, brown dwarfs, and low-mass stars. In this paper, we look for substellar companions using radial velocity surveys of FGK stars with the SOPHIE spectrograph at the Observatoire de Haute-Provence. We assign here the radial velocity variations of 27 stars to their orbital motion induced by low-mass companions. We also constrained their plane-of-the-sky motion using HIPPARCOS and Gaia Data Release 1 measurements, which constrain the true masses of some of these companions. We report the detection and characterization of six cool Jupiters, three brown dwarf candidates, and 16 low-mass stellar companions. We additionally update the orbital parameters of the low-mass star HD 8291 B, and we conclude that the radial velocity variations of HD 204277 are likely due to stellar activity despite resembling the signal of a giant planet. One of the new giant planets, BD+631405 b, adds to the population of highly eccentric cool Jupiters, and it is presently the most massive member. Two of the cool Jupiter systems also exhibit signatures of an additional outer companion. The orbital periods of the new companions span 30 days to 11.5 years, their masses 0.72 Jupiter mass to 0.61 Solar mass, and their eccentricities 0.04 to 0.88. These discoveries probe the diversity of substellar objects and low-mass stars, which will help constrain the models of their formation and evolution. ","The SOPHIE search for northern extrasolar planets -- XVII. A wealth of
  new objects: Six cool Jupiters, three brown dwarfs, and 16 low-mass binary
  stars",3,"['New paper on arXiv today -<LINK>\nWe detect 6 cool Jupiters, 3 brown dwarfs, and 16 low-mass stars with the radial velocity method using the SOPHIE spectrograph at OHP. We also perform astrometry analyses using Hipparcos and Gaia data! @FlavienKiefer <LINK>', 'One of the new cool Jupiters, BD+631405 b (msini = 3.96 Mj and e= 0.88), adds to the population of highly eccentric cool Jupiters, and it is presently the most massive member. https://t.co/nl2aSPruuE', 'Another interesting star is HD 205521, which has a companion with msini ~ 26 Mj but our analyses of the Hipparcos and Gaia astrometric data both find that the orbit is close to face-on and that the companion is actually a star. https://t.co/lhcLplsUEP']",21,05,674
229,27,1354253759816343553,4902145390,Gordan Krnjaic,"Cheers to awesome collaborators Rodolfo Capdevilla, @drc83 , and Yoni Kahn for our new paper tonight. We determine the highest mass particles that could be responsible for the g-2 anomaly and argue that a suitable muon collider could cover all of them <LINK> Of course, this assumes  1) The g-2 anomaly is ultimately verified and 2) Light, weakly coupled solutions are excluded by low energy measurements @dangaristo @drc83 Indeed, muon colliders are much smaller than other machines <LINK> @ducciolvp @drc83 Theorem? I hardly knew 'em! @nausheenrshah I wish I had a better answer! Some months ago I heard they'd unblind in January, but here we are...",https://arxiv.org/abs/2101.10334,"We perform a model-exhaustive analysis of all possible beyond Standard Model (BSM) solutions to the $(g-2)_\mu$ anomaly to study production of the associated new states at future muon colliders, and formulate a no-lose theorem for the discovery of new physics if the anomaly is confirmed and weakly coupled solutions below the GeV scale are excluded. Our goal is to find the highest possible mass scale of new physics subject only to perturbative unitarity, and optionally the requirements of minimum flavour violation (MFV) and/or naturalness. We prove that a 3 TeV muon collider is guaranteed to discover all BSM scenarios in which $\Delta a_\mu$ is generated by SM singlets with masses above $\sim $ GeV; lighter singlets will be discovered by upcoming low-energy experiments. If new states with electroweak quantum numbers contribute to $(g-2)_\mu$, the minimal requirements of perturbative unitarity guarantee new charged states below $\mathcal{O}(100 {\rm TeV})$, but this is strongly disfavoured by stringent constraints on charged lepton flavour violating (CLFV) decays. Reasonable BSM theories that satisfy CLFV bounds by obeying Minimal Flavour Violation (MFV) and avoid generating two new hierarchy problems require the existence of at least one new charged state below $\sim 10$ TeV. This strongly motivates the construction of high-energy muon colliders, which are guaranteed to discover new physics: either by producing these new charged states directly, or by setting a strong lower bound on their mass, which would empirically prove that the universe is fine-tuned and violates the assumptions of MFV while somehow not generating large CLFVs. The former case is obviously the desired outcome, but the latter scenario would perhaps teach us even more about the universe by profoundly revising our understanding of naturalness, cosmological vacuum selection, and the SM flavour puzzle. ","A No-Lose Theorem for Discovering the New Physics of $(g-2)_\mu$ at Muon
  Colliders",5,"['Cheers to awesome collaborators Rodolfo Capdevilla, @drc83 , and Yoni Kahn for our new paper tonight. We determine the highest mass particles that could be responsible for the g-2 anomaly and argue that a suitable muon collider could cover all of them <LINK>', 'Of course, this assumes \n\n1) The g-2 anomaly is ultimately verified and\n2) Light, weakly coupled solutions are excluded by low energy measurements', '@dangaristo @drc83 Indeed, muon colliders are much smaller than other machines https://t.co/OiwB5esJe4', ""@ducciolvp @drc83 Theorem? I hardly knew 'em!"", ""@nausheenrshah I wish I had a better answer! Some months ago I heard they'd unblind in January, but here we are...""]",21,01,651
230,55,1286652601925341191,19333650,Vedant Chandra,"Our new (and my first) paper was accepted to @RAS_Journals and just went up on @arxiv: ""Computational Tools for the Spectroscopic Analysis of White Dwarfs"", written with Hsiang-Chih Hwang, Nadia L. Zakamska, and Tamás Budavári.  <LINK> <LINK> A year ago, our research group wanted to analyze white dwarf spectra to characterize their temperature and surface gravity - but it turns out, most of the relevant code and theoretical models are proprietary and kept under restricted access. So, we built the tool we needed. We developed two methods to infer stellar parameters ('labels') from white dwarf spectra. The first uses high-speed neural networks to interpolate a grid of synthetic model spectra, enabling the rapid fitting of models to observations. <LINK> The second method maps line summaries of the prominent Balmer absorption lines to stellar labels with a random forest regression model. We found that these simple line summaries (width and amplitude) carry a lot of information about the stellar parameters. <LINK> We tested our methods on over five thousand white dwarf spectra from @sdssurveys, and recovered stellar labels derived by a previous group (Tremblay et. al 2019). The random forest method is particularly fast and could be useful with upcoming large-scale spectroscopic surveys. <LINK> Finally, we discuss an exciting application of our tool beyond label inference - finding interesting, exotic systems like magnetic white dwarfs (shown here) and double-degenerate binaries. Combining spectroscopic and photometric information is especially useful. <LINK> This work is built on the shoulders of giants in the white dwarf community who have developed theoretical models that form the basis of our techniques. We hope to continuously upgrade and improve our methods as new models and codes are made publicly available. Personally, I'm especially grateful to my advisors, Professor Nadia Zakamska and graduate student Hsiang-Chih Hwang, as well as @SihaoCheng and @jotajotahermes for illuminating conversations. This was fun. Our tool is under active development, and the latest version is available on GitHub: <LINK> and has documentation hosted here: <LINK>.",https://arxiv.org/abs/2007.11598,"The spectroscopic features of white dwarfs are formed in the thin upper layer of their stellar photosphere. These features carry information about the white dwarf's surface temperature, surface gravity, and chemical composition (hereafter 'labels'). Existing methods to determine these labels rely on complex ab-initio theoretical models which are not always publicly available. Here we present two techniques to determine atmospheric labels from white dwarf spectra: a generative fitting pipeline that interpolates theoretical spectra with artificial neural networks, and a random forest regression model using parameters derived from absorption line features. We test and compare our methods using a large catalog of white dwarfs from the Sloan Digital Sky Survey (SDSS), achieving the same accuracy and negligible bias compared to previous studies. We package our techniques into an open-source Python module 'wdtools' that provides a computationally inexpensive way to determine stellar labels from white dwarf spectra observed from any facility. We will actively develop and update our tool as more theoretical models become publicly available. We discuss applications of our tool in its present form to identify interesting outlier white dwarf systems including those with magnetic fields, helium-rich atmospheres, and double-degenerate binaries. ",Computational Tools for the Spectroscopic Analysis of White Dwarfs,9,"['Our new (and my first) paper was accepted to @RAS_Journals and just went up on @arxiv: ""Computational Tools for the Spectroscopic Analysis of White Dwarfs"", written with Hsiang-Chih Hwang, Nadia L. Zakamska, and Tamás Budavári. \n\n<LINK> <LINK>', 'A year ago, our research group wanted to analyze white dwarf spectra to characterize their temperature and surface gravity - but it turns out, most of the relevant code and theoretical models are proprietary and kept under restricted access.', ""So, we built the tool we needed. We developed two methods to infer stellar parameters ('labels') from white dwarf spectra. The first uses high-speed neural networks to interpolate a grid of synthetic model spectra, enabling the rapid fitting of models to observations. https://t.co/8SSGDzfcxN"", 'The second method maps line summaries of the prominent Balmer absorption lines to stellar labels with a random forest regression model. We found that these simple line summaries (width and amplitude) carry a lot of information about the stellar parameters. https://t.co/9cUac88Dlx', 'We tested our methods on over five thousand white dwarf spectra from @sdssurveys, and recovered stellar labels derived by a previous group (Tremblay et. al 2019). The random forest method is particularly fast and could be useful with upcoming large-scale spectroscopic surveys. https://t.co/IeZ7CfwaOr', 'Finally, we discuss an exciting application of our tool beyond label inference - finding interesting, exotic systems like magnetic white dwarfs (shown here) and double-degenerate binaries. Combining spectroscopic and photometric information is especially useful. https://t.co/pPDgWEFccq', 'This work is built on the shoulders of giants in the white dwarf community who have developed theoretical models that form the basis of our techniques. We hope to continuously upgrade and improve our methods as new models and codes are made publicly available.', ""Personally, I'm especially grateful to my advisors, Professor Nadia Zakamska and graduate student Hsiang-Chih Hwang, as well as @SihaoCheng and @jotajotahermes for illuminating conversations. This was fun."", 'Our tool is under active development, and the latest version is available on GitHub: https://t.co/2haXP1pp6z and has documentation hosted here: https://t.co/Mcu1XAVQ7Z.']",20,07,2181
231,116,1208053979881467905,125494187,Faisal Mahmood,"Our work on Pathomic Fusion is now on arXiv, <LINK> TL;DR: We propose a scalable method for integrating histology and -omic data using attention gating and tensor fusion for prognosis, treatment response.. prediction #pathology #pathomics #ComputationalPathology <LINK>",https://arxiv.org/abs/1912.08937,"Cancer diagnosis, prognosis, and therapeutic response predictions are based on morphological information from histology slides and molecular profiles from genomic data. However, most deep learning-based objective outcome prediction and grading paradigms are based on histology or genomics alone and do not make use of the complementary information in an intuitive manner. In this work, we propose Pathomic Fusion, an interpretable strategy for end-to-end multimodal fusion of histology image and genomic (mutations, CNV, RNA-Seq) features for survival outcome prediction. Our approach models pairwise feature interactions across modalities by taking the Kronecker product of unimodal feature representations and controls the expressiveness of each representation via a gating-based attention mechanism. Following supervised learning, we are able to interpret and saliently localize features across each modality, and understand how feature importance shifts when conditioning on multimodal input. We validate our approach using glioma and clear cell renal cell carcinoma datasets from the Cancer Genome Atlas (TCGA), which contains paired whole-slide image, genotype, and transcriptome data with ground truth survival and histologic grade labels. In a 15-fold cross-validation, our results demonstrate that the proposed multimodal fusion paradigm improves prognostic determinations from ground truth grading and molecular subtyping, as well as unimodal deep networks trained on histology and genomic data alone. The proposed method establishes insight and theory on how to train deep networks on multimodal biomedical data in an intuitive manner, which will be useful for other problems in medicine that seek to combine heterogeneous data streams for understanding diseases and predicting response and resistance to treatment. ","Pathomic Fusion: An Integrated Framework for Fusing Histopathology and
  Genomic Features for Cancer Diagnosis and Prognosis",1,"['Our work on Pathomic Fusion is now on arXiv, <LINK> \nTL;DR: We propose a scalable method for integrating histology and -omic data using attention gating and tensor fusion for prognosis, treatment response.. prediction #pathology #pathomics #ComputationalPathology <LINK>']",19,12,269
232,57,982956416410505218,131879500,John Ilee,"CO overtone emission is a really useful tracer of small-scale discs around massive YSOs, but why do we only see it in ~25% of spectra?  Manfred Mann said it best - we’re just blinded by the light.  New paper out now: <LINK> #Astronomy #Arxiv #Astroph 😎 <LINK>",https://arxiv.org/abs/1804.01934,"To date, there is no explanation as to why disc-tracing CO first overtone (or `bandhead') emission is not a ubiquitous feature in low- to medium-resolution spectra of massive young stellar objects, but instead is only detected toward approximately 25 per cent of their spectra. In this paper, we investigate the hypothesis that only certain mass accretion rates result in detectable bandhead emission in the near infrared spectra of MYSOs. Using an analytic disc model combined with an LTE model of the CO emission, we find that high accretion rates ($\gtrsim 10^{-4}\,{\rm M}_{\odot}{\mathrm{yr}}^{-1}$) result in large dust sublimation radii, a larger contribution to the $K$-band continuum from hot dust at the dust sublimation radius, and therefore correspondingly lower CO emission with respect to the continuum. On the other hand, low accretion rates ($\lesssim10^{-6}\,{\rm M}_{\odot}{\mathrm{yr}}^{-1}$) result in smaller dust sublimation radii, a correspondingly smaller emitting area of CO, and thus also lower CO emission with respect to the continuum. In general, moderate accretion rates produce the most prominent, and therefore detectable, CO first overtone emission. We compare our findings to a recent near-infrared spectroscopic survey of MYSOs, finding results consistent with our hypothesis. We conclude that the detection rate of CO bandhead emission in the spectra of MYSOs could be the result of MYSOs exhibiting a range of mass accretion rates, perhaps due to the variable accretion suggested by recent multi-epoch observations of these objects. ","Blinded by the light: on the relationship between CO first overtone
  emission and mass accretion rate in massive young stellar objects",1,"['CO overtone emission is a really useful tracer of small-scale discs around massive YSOs, but why do we only see it in ~25% of spectra? \n\nManfred Mann said it best - we’re just blinded by the light. \n\nNew paper out now: <LINK>\n\n#Astronomy #Arxiv #Astroph 😎 <LINK>']",18,04,259
233,115,1356247413665783811,904820264021749761,Samarth Mishra,"New preprint out (<LINK>) for work done with @kate_saenko_ and Venkatesh Saligrama.  We find how effective self-supervised pretraining and image augmentation based consistency are on semi-supervised domain adaptation. (spoiler : very effective) 1/ <LINK> In visual domain adaptation, a learner is tasked with identifying classes in a target visual domain given labelled data in a different source domain (e.g. classifying real images using only labelled hand-sketches of objects).  2/ A central element to a range of prior art is (adversarial) domain alignment --- trying to find a feature space where source and target domain data are indistinguishable.  3/ We find that with few target labels (available in semi-supervised domain adaptation) and our pretraining and consistency approach, the need for adversarial domain alignment is mitigated.  4/ Pretraining and Consistency (PAC) outperforms prior art based on adversarial domain alignment on multiple benchmarks including the large and challenging DomainNet.  5/5 <LINK>",http://arxiv.org/abs/2101.12727,"Most modern unsupervised domain adaptation (UDA) approaches are rooted in domain alignment, i.e., learning to align source and target features to learn a target domain classifier using source labels. In semi-supervised domain adaptation (SSDA), when the learner can access few target domain labels, prior approaches have followed UDA theory to use domain alignment for learning. We show that the case of SSDA is different and a good target classifier can be learned without needing alignment. We use self-supervised pretraining (via rotation prediction) and consistency regularization to achieve well separated target clusters, aiding in learning a low error target classifier. With our Pretraining and Consistency (PAC) approach, we achieve state of the art target accuracy on this semi-supervised domain adaptation task, surpassing multiple adversarial domain alignment methods, across multiple datasets. PAC, while using simple techniques, performs remarkably well on large and challenging SSDA benchmarks like DomainNet and Visda-17, often outperforming recent state of the art by sizeable margins. Code for our experiments can be found at this https URL ","Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining
  and Consistency",5,"['New preprint out (<LINK>) for work done with @kate_saenko_ and Venkatesh Saligrama. \n\nWe find how effective self-supervised pretraining and image augmentation based consistency are on semi-supervised domain adaptation. (spoiler : very effective)\n\n1/ <LINK>', 'In visual domain adaptation, a learner is tasked with identifying classes in a target visual domain given labelled data in a different source domain (e.g. classifying real images using only labelled hand-sketches of objects). \n\n2/', 'A central element to a range of prior art is (adversarial) domain alignment --- trying to find a feature space where source and target domain data are indistinguishable.  \n\n3/', 'We find that with few target labels (available in semi-supervised domain adaptation) and our pretraining and consistency approach, the need for adversarial domain alignment is mitigated. \n\n4/', 'Pretraining and Consistency (PAC) outperforms prior art based on adversarial domain alignment on multiple benchmarks including the large and challenging DomainNet. \n\n5/5 https://t.co/ZM3EMJCXMl']",21,01,1025
234,17,1290081013876420608,2958183660,Dr. Kathryn Neugent,"New Paper Alert! Let's talk about the binary fraction of red supergiants ... a thread! <LINK> <LINK> For my PhD research at @uwastronomy with @emsque, I found that red supergiants (RSGs) primarily have B-type companions. I went searching for these systems in the Andromeda Galaxy, the Triangulum Galaxy and the Large and Small Magellanic Clouds. So far, we've found over 250! Next I focused on determining the binary *fraction* of RSGs in the Large Magellanic Cloud. Un-evolved OB stars *love* to be in binary systems, but do RSGs? <LINK> Using a sample set of spectroscopically confirmed binary and single RSGs and their photometric colors, I used a K-Nearest Neighbor algorithm to classify RSGs in the Large Magellanic Cloud (thanks @trevorzaylen for the suggestion!). <LINK> Here's the resulting color-color plot! The redder the point, the more likely the star is to be a single RSG. The bluer the point, the more likely it is to be a binary RSG! <LINK> Accounting for our observational biases (such as RSGs *not* in systems with B-type stars), we find that the binary fraction of RSGs in the Large Magellanic Cloud is only around 20%! <LINK> This percentage is much lower than for the un-evolved OB stars ... but we think this is because some of the RSGs will actually *merge* with their companion and appear single! (see paper for more discussion ... and more research to come!) Next up I'm going to look at the binary fraction in M31 and M33 and see if there's any change with metallicity! Stay tuned! And many thanks to @emsque, @MassiveStarGuy, @mrdrout and Nidia Morrell for being great collaborators! <LINK> Remember, as @dalcantonJD always says, #StarsAreStillInteresting!!!  <LINK> &lt;/thread&gt;",https://arxiv.org/abs/2007.15852,"The binary fraction of unevolved massive stars is thought to be 70-100% but there are few observational constraints on the binary fraction of the evolved version of a subset of these stars, the red supergiants (RSGs). Here we identify a complete sample of RSGs in the Large Magellanic Cloud (LMC) using new spectroscopic observations and archival UV, IR and broadband optical photometry. We find 4090 RSGs with log L/Lo > 3.5 with 1820 of them having log L/Lo > 4, which we believe is our completeness limit. We additionally spectroscopically confirmed 38 new RSG+B star binaries in the LMC, bringing the total known up to 55. We then estimated the binary fraction using a k-nearest neighbors algorithm that classifies stars as single or binary based on photometry with a spectroscopic sample as a training set. We take into account observational biases such as line-of-sight stars and binaries in eclipse while also calculating model-dependent corrections for RSGs with companions that our observations were not designed to detect. Based on our data, we find an initial result of 13.5 +7.56/-6.67% for RSGs with O or B-type companions. Using the Binary Population and Spectral Synthesis (BPASS) models to correct for unobserved systems, this corresponds to a total RSG binary fraction of 19.5 +7.6/-6.7%. This number is in broad agreement with what we would expect given an initial OB binary distribution of 70%, a predicted merger fraction of 20-30% and a binary interaction fraction of 40-50%. ",The Red Supergiant Binary Fraction of the Large Magellanic Cloud,9,"[""New Paper Alert! Let's talk about the binary fraction of red supergiants ... a thread!\n\n<LINK> <LINK>"", ""For my PhD research at @uwastronomy with @emsque, I found that red supergiants (RSGs) primarily have B-type companions. I went searching for these systems in the Andromeda Galaxy, the Triangulum Galaxy and the Large and Small Magellanic Clouds. So far, we've found over 250!"", 'Next I focused on determining the binary *fraction* of RSGs in the Large Magellanic Cloud. Un-evolved OB stars  *love* to be in binary systems, but do RSGs? https://t.co/MIfdZhRIPB', 'Using a sample set of spectroscopically confirmed binary and single RSGs and their photometric colors, I used a K-Nearest Neighbor algorithm to classify RSGs in the Large Magellanic Cloud (thanks @trevorzaylen for the suggestion!). https://t.co/3Ym8O5ERDs', ""Here's the resulting color-color plot! The redder the point, the more likely the star is to be a single RSG. The bluer the point, the more likely it is to be a binary RSG! https://t.co/knQDW5Vo2y"", 'Accounting for our observational biases (such as RSGs *not* in systems with B-type stars), we find that the binary fraction of RSGs in the Large Magellanic Cloud is only around 20%! https://t.co/OhWI0j6Cvn', 'This percentage is much lower than for the un-evolved OB stars ... but we think this is because some of the RSGs will actually *merge* with their companion and appear single! (see paper for more discussion ... and more research to come!)', ""Next up I'm going to look at the binary fraction in M31 and M33 and see if there's any change with metallicity! Stay tuned! And many thanks to @emsque, @MassiveStarGuy, @mrdrout and Nidia Morrell for being great collaborators! https://t.co/uNqaVhasns"", 'Remember, as @dalcantonJD always says, #StarsAreStillInteresting!!! \n\nhttps://t.co/eOIyqs1W7G\n\n&lt;/thread&gt;']",20,07,1709
235,141,1281258091804340234,809477971518095361,Hans,"new paper out with @robertghrist: ever wondered if lattice theory and sheaf theory had a baby (and no i'm not talking about topoi...) that was raised by GSP??? look no further! applications *in progress* including consensus, distributed opt., GNNs & more <LINK> <LINK>",https://arxiv.org/abs/2007.04099,"This paper initiates a discrete Hodge theory for cellular sheaves taking values in a category of lattices and Galois connections. The key development is the Tarski Laplacian, an endomorphism on the cochain complex whose fixed points yield a cohomology that agrees with the global section functor in degree zero. This has immediate applications in consensus and distributed optimization problems over networks and broader potential applications. ",Cellular Sheaves of Lattices and the Tarski Laplacian,1,"[""new paper out with @robertghrist: ever wondered if lattice theory and sheaf theory had a baby (and no i'm not talking about topoi...) that was raised by GSP??? look no further! applications *in progress* including consensus, distributed opt., GNNs &amp; more\n<LINK> <LINK>""]",20,07,268
236,142,1317068719173009408,11892372,Fabrizio Silvestri,"We just released on arXiv (<LINK>) our new paper on ""Neural Databases"". We presents NeuralDB, a DB system with no pre-defined schema, in which updates & queries are given in natural language. @j6mes, @myazdani999, @marzieh_saeidi, @riedelcastro, and Alon Halevi @srchvrs @Mniepert @j6mes @myazdani999 @marzieh_saeidi @riedelcastro Roots are roots! :) You can't forget about them :)",https://arxiv.org/abs/2010.06973,"In recent years, neural networks have shown impressive performance gains on long-standing AI problems, and in particular, answering queries from natural language text. These advances raise the question of whether they can be extended to a point where we can relax the fundamental assumption of database management, namely, that our data is represented as fields of a pre-defined schema. This paper presents a first step in answering that question. We describe NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language. We develop query processing techniques that build on the primitives offered by the state of the art Natural Language Processing methods. We begin by demonstrating that at the core, recent NLP transformers, powered by pre-trained language models, can answer select-project-join queries if they are given the exact set of relevant facts. However, they cannot scale to non-trivial databases and cannot perform aggregation queries. Based on these findings, we describe a NeuralDB architecture that runs multiple Neural SPJ operators in parallel, each with a set of database sentences that can produce one of the answers to the query. The result of these operators is fed to an aggregation operator if needed. We describe an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators. Importantly, this algorithm can be trained by the Neural SPJ operator itself. We experimentally validate the accuracy of NeuralDB and its components, showing that we can answer queries over thousands of sentences with very high accuracy. ",Neural Databases,2,"['We just released on arXiv (<LINK>) our new paper on ""Neural Databases"".\n\nWe presents NeuralDB, a DB system with no pre-defined schema, in which updates &amp; queries are given in natural language.\n@j6mes, @myazdani999, @marzieh_saeidi, @riedelcastro, and Alon Halevi', ""@srchvrs @Mniepert @j6mes @myazdani999 @marzieh_saeidi @riedelcastro Roots are roots! :) You can't forget about them :)""]",20,10,381
237,13,1509878142034493444,901266828655284225,Brian Metzger,"New paper from my student Dhruv on nu-driven winds from rapidly spinning (~ms) just-born neutron stars. Nucleosynthesis will differ from typical slow-spinning NS birth, but the rotation-boosted mass-loss rate allows these beasts to out-punch their weight. <LINK> <LINK>",https://arxiv.org/abs/2203.16560,"We explore the effects of rapid rotation on the properties of neutrino-heated winds from proto-neutron stars (PNS) formed in core-collapse supernovae or neutron-star mergers by means of three-dimensional general-relativistic hydrodynamical simulations with M0 neutrino transport. We focus on conditions characteristic of a few seconds into the PNS cooling evolution when the neutrino luminosities obey $L_{\nu_e} + L_{\bar{\nu}_e} \approx 7\times 10^{51}$ erg s$^{-1}$, and over which most of the wind mass-loss will occur. After an initial transient phase, all of our models reach approximately steady-state outflow solutions with positive energies and sonic surfaces captured on the computational grid. Our non-rotating and slower-rotating models (angular velocity relative to Keplerian $\Omega/\Omega_{\rm K} \lesssim 0.4$; spin period $P \gtrsim 2$ ms) generate approximately spherically symmetric outflows with properties in good agreement with previous PNS wind studies. By contrast, our most rapidly spinning PNS solutions ($\Omega/\Omega_{\rm K} \gtrsim 0.75$; $P \approx 1$ ms) generate outflows focused in the rotational equatorial plane with much higher mass-loss rates (by over an order of magnitude), lower velocities, lower entropy, and lower asymptotic electron fractions, than otherwise similar non-rotating wind solutions. Although such rapidly spinning PNS are likely rare in nature, their atypical nucleosynthetic composition and outsized mass yields could render them important contributors of light neutron-rich nuclei compared to more common slowly rotating PNS birth. Our calculations pave the way to including the combined effects of rotation and a dynamically-important large-scale magnetic field on the wind properties within a 3D GRMHD framework. ","Three-Dimensional General-Relativistic Simulations of Neutrino-Driven
  Winds from Rotating Proto-Neutron Stars",1,"['New paper from my student Dhruv on nu-driven winds from rapidly spinning (~ms) just-born neutron stars.  Nucleosynthesis will differ from typical slow-spinning NS birth, but the rotation-boosted mass-loss rate allows these beasts to out-punch their weight. <LINK> <LINK>']",22,03,269
238,134,1257287512067883008,2613619922,byron wallace,"new work on learning to provide *faithful* rationales led by @successar_nlp w/@sarahwiegreffe, & @yuvalpi to appear #acl2020nlp.  paper: <LINK> 👇quick thread 1/5 we'd often like models to tell us why they made particular predictions, but it can be tricky to know which tokens influenced outputs in the era of contextualized embeddings (which allow arbitrary interactions). 2/5 performing *hard* selection over inputs to extract rationales fixes this, but training discrete selection models is finicky 3/5 so, we propose a very simple method (FRESH) for discrete rationale selection: use arbitrary saliency scores + heuristics to derive pseudo-labels on tokens and then train an extractor on these -- an independent classifier module then *only* uses extracted rationales. 4/5 <LINK> it turns out that despite its simplicity, this strategy fares as well or better than, e.g., using REINFORCE to train the discrete extraction module in our experiments 5/5 <LINK>",https://arxiv.org/abs/2005.00115,"In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text `responsible for' corresponding model output; when such a snippet comprises tokens that indeed informed the model's prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to `end-to-end' approaches, while being more general and easier to train. Code is available at this https URL ",Learning to Faithfully Rationalize by Construction,5,"['new work on learning to provide *faithful* rationales led by @successar_nlp w/@sarahwiegreffe, &amp; @yuvalpi to appear #acl2020nlp. \n\npaper: <LINK>\n\n👇quick thread 1/5', ""we'd often like models to tell us why they made particular predictions, but it can be tricky to know which tokens influenced outputs in the era of contextualized embeddings (which allow arbitrary interactions).  2/5"", 'performing *hard* selection over inputs to extract rationales fixes this, but training discrete selection models is finicky 3/5', 'so, we propose a very simple method (FRESH) for discrete rationale selection: use arbitrary saliency scores + heuristics to derive pseudo-labels on tokens and then train an extractor on these -- an independent classifier module then *only* uses extracted rationales. 4/5 https://t.co/JQz5J2vsm1', 'it turns out that despite its simplicity, this strategy fares as well or better than, e.g., using REINFORCE to train the discrete extraction module in our experiments 5/5 https://t.co/iPJiv7ZR0M']",20,05,960
239,68,1417496293602205707,905075448798998530,Nikola Milojević-Dupont,"Check out our new preprint: we gathered and analyzed open geospatial data on buildings worldwide trying to understand current practices and how useful is current data for urban sustainability research. Paper: <LINK> Website: <LINK> <LINK> Why? Maps of buildings footprint, age, type, height.. are primordial to transform cities to improve social and environmental sustainability, resilience to climate change, etc. But current data are fragmented, making it difficult to identify what is available & where. We have identified and benchmarked more than 140 government releases from 28 countries containing above 100 million buildings, based on five dimensions: accessibility, richness, data quality, harmonization, and relationships with other actors Summary table - end of manuscript👇 <LINK> TL;DR: 1. data on buildings are increasingly available but mostly limited to the global north 2. many datasets are useful for sustainability research but broader adoption of best practices is needed for higher relevance to most timely issues (urbanization, decarbonization,...) 3. the heterogeneity in current practices generates difficulties to access and use the data even for advanced users   4. OSM and government data complement each other in many regions: there is a large potential from further integration <LINK>",https://arxiv.org/abs/2107.04023,"As buildings are central to the social and environmental sustainability of human settlements, high-quality geospatial data are necessary to support their management and planning. Authorities around the world are increasingly collecting and releasing such data openly, but these are mostly disconnected initiatives, making it challenging for users to fully leverage their potential for urban sustainability. We conduct a global study of 2D geospatial data on buildings that are released by governments for free access, ranging from individual cities to whole countries. We identify and benchmark more than 140 releases from 28 countries containing above 100 million buildings, based on five dimensions: accessibility, richness, data quality, harmonisation, and relationships with other actors. We find that much building data released by governments is valuable for spatial analyses, but there are large disparities among them and not all instances are of high quality, harmonised, and rich in descriptive information. Our study also compares authoritative data to OpenStreetMap, a crowdsourced counterpart, suggesting a mutually beneficial and complementary relationship. ","Open government geospatial data on buildings for planning sustainable
  and resilient cities",5,"['Check out our new preprint:\n\nwe gathered and analyzed open geospatial data on buildings worldwide trying to understand current practices and how useful is current data for urban sustainability research.\n\nPaper: <LINK>\n\nWebsite: <LINK> <LINK>', 'Why? Maps of buildings footprint, age, type, height.. are primordial to transform cities to improve social and environmental sustainability, resilience to climate change, etc. But current data are fragmented, making it difficult to identify what is available &amp; where.', 'We have identified and benchmarked more than 140 government releases from 28 countries containing above 100 million buildings, based on five dimensions:\n\naccessibility, richness, data quality, harmonization, and relationships with other actors\n\nSummary table - end of manuscript👇 https://t.co/lUpKuYhfKT', 'TL;DR:\n\n1. data on buildings are increasingly available but mostly limited to the global north\n\n2. many datasets are useful for sustainability research but broader adoption of best practices is needed for higher relevance to most timely issues (urbanization, decarbonization,...)', '3. the heterogeneity in current practices generates difficulties to access and use the data even for advanced users\n    \n4. OSM and government data complement each other in many regions: there is a large potential from further integration https://t.co/m50sO0TUxq']",21,07,1312
240,160,1367492022555279367,24859650,Jan-Willem van de Meent,"New working paper: Learning Proposals for Probabilistic Programs with Inference Combinators ArXiv: <LINK>  Joint work, several years in the making, by Sam Stites* (@SamStites), Heiko Zimmermann* (@zmheiko), Hao Wu (@Hao_Wu_), and Eli Sennesh (@EliSennesh) [1/] <LINK> Amortized inference methods train a neural proposal by minimizing a variational objective. These methods are straightforward to use in unstructured models, such as standard VAEs, but can be difficult to apply to more structured models, such as (deep) probabilistic programs. [2/] There has been lots of great work that improves variational methods using techniques from importance sampling and MCMC, but these more sophisticated methods are difficult to apply to probabilistic programs, in part because they are rarely fully model agnostic. [3/] In this paper, we propose a language for inference algorithms. This language defines inference combinators; functions that can be composed to define user-programmable importance samplers for probabilistic programs. [4/] <LINK> To reason about validity of inference, we adapt Christian Naesseth's (@chris_naesseth) work on nested importance samplers and proper weighting. This allows us to design a language in which any composition of combinators defines a valid importance sampler. [5/] We combine this language with a backend for variational inference that implements gradient estimates for the forward and reverse KL divergence, as well as for our recently-proposed nested variational objectives (<LINK>). [6/] The result is a language that allows users to concisely define advanced methods, such as Hao's recent work on amortized Gibbs methods (<LINK>), and define objectives to train both generative models and proposals. [7/] <LINK> We implement combinators and gradient estimators in Probabilistic Torch (release forthcoming). However, the language in this paper can be implemented on top of many (deep) probabilistic programming languages. [8/] This project has been several years in the making. We started thinking about this language design in 2018 (<LINK>), but eventually realized that we did not sufficiently understand how to precisely connect importance sampling to variational inference [9/] We then went back and did work on amortized population Gibbs methods, which lead to work on nested variational inference, and this work allowed us to put the pieces together in this paper. I'm super proud of Sam, Heiko, Hao, and Eli, who each made major contributions!",https://arxiv.org/abs/2103.00668,"We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing. ",Learning Proposals for Probabilistic Programs with Inference Combinators,10,"['New working paper: Learning Proposals for Probabilistic Programs with Inference Combinators\n\nArXiv: <LINK> \n\nJoint work, several years in the making, by Sam Stites* (@SamStites), Heiko Zimmermann* (@zmheiko), Hao Wu (@Hao_Wu_), and Eli Sennesh (@EliSennesh) [1/] <LINK>', 'Amortized inference methods train a neural proposal by minimizing a variational objective. These methods are straightforward to use in unstructured models, such as standard VAEs, but can be difficult to apply to more structured models, such as (deep) probabilistic programs. [2/]', 'There has been lots of great work that improves variational methods using techniques from importance sampling and MCMC, but these more sophisticated methods are difficult to apply to probabilistic programs, in part because they are rarely fully model agnostic. [3/]', 'In this paper, we propose a language for inference algorithms. This language defines inference combinators; functions that can be composed to define user-programmable importance samplers for probabilistic programs. [4/] https://t.co/bQSHUGlgPr', ""To reason about validity of inference, we adapt Christian Naesseth's (@chris_naesseth) work on nested importance samplers and proper weighting. This allows us to design a language in which any composition of combinators defines a valid importance sampler. [5/]"", 'We combine this language with a backend for variational inference that implements gradient estimates for the forward and reverse KL divergence, as well as for our recently-proposed nested variational objectives (https://t.co/hzLJ2IpIWm). [6/]', ""The result is a language that allows users to concisely define advanced methods, such as Hao's recent work on amortized Gibbs methods (https://t.co/bqQ07ixS6E), and define objectives to train both generative models and proposals. [7/] https://t.co/bjuDT4WywY"", 'We implement combinators and gradient estimators in Probabilistic Torch (release forthcoming). However, the language in this paper can be implemented on top of many (deep) probabilistic programming languages. [8/]', 'This project has been several years in the making. We started thinking about this language design in 2018 (https://t.co/IZXCdqjfyI), but eventually realized that we did not sufficiently understand how to precisely connect importance sampling to variational inference [9/]', ""We then went back and did work on amortized population Gibbs methods, which lead to work on nested variational inference, and this work allowed us to put the pieces together in this paper. I'm super proud of Sam, Heiko, Hao, and Eli, who each made major contributions!""]",21,03,2490
241,165,1316321913132679168,1140222123006472194,Kasper Elm Heintz,"Happy to announce that our new paper: <LINK> was posted today on ArXiv! In this, we basically ask: can you efficiently identify quasars purely as point-like sources that doesn’t move on the sky? Turns out: Yes! As long as its far away from the Galactic plane This has been a rollercoaster of a project, starting with the release of @ESAGaia-DR2 more than 2 years ago, and with several non-succesful observing runs due to clouds, ice, etc. <LINK> When we *finally* got spectra of our targets, we have had several first-year students at @DAWNCopenhagen/@uni_copenhagen help us classify them as either stars ⭐️ or quasars 💥 (some many billion of lightyears away) — and for many of them, this was their first academic paper! Briefly, what we found was: it is possible to efficiently select quasars (~70% of the total number of targets) purely as sources with zero proper motions and parallaxes as measured by @ESAGaia-DR2. <LINK> This is, however, only feasible to reach at high Galactic latitudes, i.e. far away from the Galactic plane. Moving closer, we would be completely overwhelmed by stars that were apparently non-moving on the sky (at least not in the transverse direction). <LINK> This *complete* sample of quasars allowed us to examine the intrinsic properties of the quasar population, and specifically how large the fraction of “red” quasars is (anyway between 10% and 40% depending on the brightness limit). <LINK> In this survey we also discovered several very interesting quasars — here is one example that shines right through the disk of a nearby (z~0.02) galaxy! This allowed us to study the dust and metals in this disk, that was imposed on the background quasar spectrum. <LINK> Finally, we could also assess how complete other, more commonly used photometric quasar surveys were at similar brightness — turns out, they only manage to find 85-90% of the quasars we do! <LINK> This just cements the fact that this survey is the only way to select quasars, or extragalactic point-sources in general, without assumptions of their intrinsic emission (i.e. radio-loud, blue UV colors, etc) — so basically, quasars that don’t look like typical quasars! @dr_guangtou No apparently not! As far as I understand, extended sources will not trigger a detection on Gaia’s CCD — however, for bright AGN (and this spectacular case) Gaia will be sensitive enough to disentangle and accurately measure the central (or outlying) point-source.",https://arxiv.org/abs/2010.05934,"Here we explore the efficiency and fidelity of a purely astrometric selection of quasars as point sources with zero proper motions in the {\it Gaia} data release 2 (DR2). We have built a complete candidate sample including 104 Gaia-DR2 point sources brighter than $G<20$ mag within one degree of the north Galactic pole (NGP), all with proper motions consistent with zero within 2$\sigma$ uncertainty. In addition to pre-existing spectra, we have secured long-slit spectroscopy of all the remaining candidates and find that all 104 stationary point sources in the field can be classified as either quasars (63) or stars (41). The selection efficiency of the zero-proper-motion criterion at high Galactic latitudes is thus $\approx 60\%$. Based on this complete quasar sample we examine the basic properties of the underlying quasar population within the imposed limiting magnitude. We find that the surface density of quasars is 20 deg$^{-2}$, the redshift distribution peaks at $z\sim1.5$, and that only eight systems ($13^{+5}_{-3}\%$) show significant dust reddening. We then explore the selection efficiency of commonly used optical, near- and mid-infrared quasar identification techniques and find that they are all complete at the $85-90\%$ level compared to the astrometric selection. Finally, we discuss how the astrometric selection can be improved to an efficiency of $\approx70\%$ by including an additional cut requiring parallaxes of the candidates to be consistent with zero within 2$\sigma$. The selection efficiency will further increase with the release of future, more sensitive astrometric measurement from the Gaia mission. This type of selection, purely based on the astrometry of the quasar candidates, is unbiased in terms of colours and emission mechanisms of the quasars and thus provides the most complete census of the quasar population within the limiting magnitude of Gaia. ","Spectroscopic classification of a complete sample of
  astrometrically-selected quasar candidates using Gaia DR2",10,"['Happy to announce that our new paper: <LINK>  was posted today on ArXiv! \nIn this, we basically ask: can you efficiently identify quasars purely as point-like sources that doesn’t move on the sky? \nTurns out: Yes! \nAs long as its far away from the Galactic plane', 'This has been a rollercoaster of a project, starting with the release of @ESAGaia-DR2 more than 2 years ago, and with several non-succesful observing runs due to clouds, ice, etc. https://t.co/6lmzM4HrAQ', 'When we *finally* got spectra of our targets, we have had several first-year students at @DAWNCopenhagen/@uni_copenhagen help us classify them as either stars ⭐️ or quasars 💥 (some many billion of lightyears away) — and for many of them, this was their first academic paper!', 'Briefly, what we found was: it is possible to efficiently select quasars (~70% of the total number of targets) purely as sources with zero proper motions and parallaxes as measured by @ESAGaia-DR2. https://t.co/AbzczXmkCI', 'This is, however, only feasible to reach at high Galactic latitudes, i.e. far away from the Galactic plane. Moving closer, we would be completely overwhelmed by stars that were apparently non-moving on the sky (at least not in the transverse direction). https://t.co/nN77blhLfA', 'This *complete* sample of quasars allowed us to examine the intrinsic properties of the quasar population, and specifically how large the fraction of “red” quasars is (anyway between 10% and 40% depending on the brightness limit). https://t.co/B9huHsfbo5', 'In this survey we also discovered several very interesting quasars — here is one example that shines right through the disk of a nearby (z~0.02) galaxy! This allowed us to study the dust and metals in this disk, that was imposed on the background quasar spectrum. https://t.co/MY7pw0cXH7', 'Finally, we could also assess how complete other, more commonly used photometric quasar surveys were at similar brightness — turns out, they only manage to find 85-90% of the quasars we do! https://t.co/eEtbUjJTcS', 'This just cements the fact that this survey is the only way to select quasars, or extragalactic point-sources in general, without assumptions of their intrinsic emission (i.e. radio-loud, blue UV colors, etc) — so basically, quasars that don’t look like typical quasars!', '@dr_guangtou No apparently not! As far as I understand, extended sources will not trigger a detection on Gaia’s CCD — however, for bright AGN (and this spectacular case) Gaia will be sensitive enough to disentangle and accurately measure the central (or outlying) point-source.']",20,10,2441
242,38,1376553367980285955,705950701986275328,Tzanio Kolev,"🤔 Need to couple or transfer fields between high- and low-order simulations? Check out our new paper with Will Pazner:  📄 ""Conservative and accurate solution transfer between high-order and low-order refined finite element spaces""  👉 <LINK> #PoweredByMFEM <LINK>",https://arxiv.org/abs/2103.05283,"In this paper we introduce general transfer operators between high-order and low-order refined finite element spaces that can be used to couple high-order and low-order simulations. Under natural restrictions on the low-order refined space we prove that both the high-to-low-order and low-to-high-order linear mappings are conservative, constant preserving and high-order accurate. While the proofs apply to affine geometries, numerical experiments indicate that the results hold for more general curved and mixed meshes. These operators also have applications in the context of coarsening solution fields defined on meshes with nonconforming refinement. The transfer operators for $H^1$ finite element spaces require a globally coupled solve, for which robust and efficient preconditioners are developed. We present several numerical results confirming our analysis and demonstrate the utility of the new mappings in the context of adaptive mesh refinement and conservative multi-discretization coupling. ","Conservative and accurate solution transfer between high-order and
  low-order refined finite element spaces",1,"['🤔 Need to couple or transfer fields between high- and low-order simulations? Check out our new paper with Will Pazner: \n\n📄 ""Conservative and accurate solution transfer between high-order and low-order refined finite element spaces""  \n\n👉 <LINK>\n\n#PoweredByMFEM <LINK>']",21,03,262
243,93,1136868293883514881,73928069,Shadab Khan,"New paper accepted in @miccai2019 with @AhmedHassan9922  We show a principled approach to incorporating priors from extreme points to generate reliable training data for segmentation tasks. Preprint: <LINK> Code+blog coming soon! Extreme points, as shown by Dim Papadopoulos et al (ICCV), can be annotated much more quickly (~5x) than a bounding box. We take extreme points, and compute a full confidence map by incorporating intuitive priors. These weight maps have desirable isocontours as shown: <LINK> We train a DeepLab inspired architecture with image and confidence map as input, and use it to predict class-agnostic segmentation of an object. At test time, user provides extreme points and the model generates pixel-level segmentation that can be used for supervised training. <LINK> We experimented to see how well supervised training does with generated data compared to ground truth for segmentation of a previously unseen organ (but we fine tuned on a small fraction of the data). We obtained comparable results. <LINK> We hope with this method, we will be able to generate reliable training data for several supervised learning scenarios. BONUS: A fast algorithm to compute distance of points on a 2D grid from a line segment. How fast? ~88ms for a 512*512 grid using non-multi-threaded python. <LINK> Thanks to reviewers for useful feedback, though we particularly thank reviewer number 1 for useful feedback.",https://arxiv.org/abs/1906.02421,"To automate the process of segmenting an anatomy of interest, we can learn a model from previously annotated data. The learning-based approach uses annotations to train a model that tries to emulate the expert labeling on a new data set. While tremendous progress has been made using such approaches, labeling of medical images remains a time-consuming and expensive task. In this paper, we evaluate the utility of extreme points in learning to segment. Specifically, we propose a novel approach to compute a confidence map from extreme points that quantitatively encodes the priors derived from extreme points. We use the confidence map as a cue to train a deep neural network based on ResNet-101 and PSP module to develop a class-agnostic segmentation model that outperforms state-of-the-art method that employs extreme points as a cue. Further, we evaluate a realistic use-case by using our model to generate training data for supervised learning (U-Net) and observed that U-Net performs comparably when trained with either the generated data or the ground truth data. These findings suggest that models trained using cues can be used to generate reliable training data. ","Extreme Points Derived Confidence Map as a Cue For Class-Agnostic
  Segmentation Using Deep Neural Network",6,"['New paper accepted in @miccai2019 with @AhmedHassan9922 \n\nWe show a principled approach to incorporating priors from extreme points to generate reliable training data for segmentation tasks.\n\nPreprint: <LINK>\n\nCode+blog coming soon!', 'Extreme points, as shown by Dim Papadopoulos et al (ICCV), can be annotated much more quickly (~5x) than a bounding box. We take extreme points, and compute a full confidence map by incorporating intuitive priors. These weight maps have desirable isocontours as shown: https://t.co/0HrptE4aS9', 'We train a DeepLab inspired architecture with image and confidence map as input, and use it to predict class-agnostic segmentation of an object. At test time, user provides extreme points and the model generates pixel-level segmentation that can be used for supervised training. https://t.co/4hAyZeSfoD', 'We experimented to see how well supervised training does with generated data compared to ground truth for segmentation of a previously unseen organ (but we fine tuned on a small fraction of the data). We obtained comparable results. https://t.co/qN3ulyVmFs', 'We hope with this method, we will be able to generate reliable training data for several supervised learning scenarios.\n\nBONUS: A fast algorithm to compute distance of points on a 2D grid from a line segment. How fast? ~88ms for a 512*512 grid using non-multi-threaded python. https://t.co/OG4oC73SJp', 'Thanks to reviewers for useful feedback, though we particularly thank reviewer number 1 for useful feedback.']",19,06,1423
244,83,1171394391879163904,5850692,Aaron Roth,"We came up with a new and improved proof of the ""transfer theorem"", showing that differentially private analyses generalize. The paper is here: <LINK> and I wrote a blog post about it here: <LINK>. Continue on for the TL;DR. 1/ <LINK> It is an old observation (I think originally made by @frankmcsherry) that differentially private analyses generalize in expectation. If you (say) privately learn a classifier, then in expectation its test error will be close to its training error. 2/ This is true of many stability notions, but differential privacy is special because it composes adaptively. So you could actually train a sequence of classifiers on the same dataset, using previous outcomes to guide your search, and still not overfit in expectation. 3/ But these ""in expecation"" bounds aren't enough to derive confidence intervals, and so a line of work has emerged lifting these to high probability bounds. Our original paper (DFHPRR in the plot) got the first such bound via a direct moment calculation, but it wasn't tight. 4/ The BNSSSU paper (see the plot) introduced the ingenious ""monitor technique"" which let them prove the asymptotically optimal bound. This idea has proven useful elsewhere. But it also seems to lead to large constant overhead, even though the asymptotics are tight. We avoid it. 5/ Here's our proof: Pretend that after all analyses are complete, the dataset is resampled from its posterior distribution conditioned on the output of the mechanism. This doesn't change the joint distribution on datasets and outputs. So empirical accuracy guarantees still hold. 6/ But the resampled dataset will take query values that are with high probability that close to their posterior means. So the only way the mechanism can promise empirical accuracy is if it promises accuracy with respect to this posterior distribution with high probability. 7/ But differential privacy implies that the posterior expectation of query values must be close to their prior (true) values. And thats it. We get high probability bounds for free, from the high probability sample accuracy bounds. Thats why we don't lose large constants. 8/ This is joint work with terrific co-authors @chrisjung93, Katrina Ligett, @privatealgos, Saeed Sharifi, and Moshe Shenfeld. (So lets call it JLNRSS). It was really fun to work on. And consistent with prior work, we see that transfer theorems require six authors. 9/9",https://arxiv.org/abs/1909.03577,"We give a new proof of the ""transfer theorem"" underlying adaptive data analysis: that any mechanism for answering adaptively chosen statistical queries that is differentially private and sample-accurate is also accurate out-of-sample. Our new proof is elementary and gives structural insights that we expect will be useful elsewhere. We show: 1) that differential privacy ensures that the expectation of any query on the posterior distribution on datasets induced by the transcript of the interaction is close to its true value on the data distribution, and 2) sample accuracy on its own ensures that any query answer produced by the mechanism is close to its posterior expectation with high probability. This second claim follows from a thought experiment in which we imagine that the dataset is resampled from the posterior distribution after the mechanism has committed to its answers. The transfer theorem then follows by summing these two bounds, and in particular, avoids the ""monitor argument"" used to derive high probability bounds in prior work. An upshot of our new proof technique is that the concrete bounds we obtain are substantially better than the best previously known bounds, even though the improvements are in the constants, rather than the asymptotics (which are known to be tight). As we show, our new bounds outperform the naive ""sample-splitting"" baseline at dramatically smaller dataset sizes compared to the previous state of the art, bringing techniques from this literature closer to practicality. ",A New Analysis of Differential Privacy's Generalization Guarantees,9,"['We came up with a new and improved proof of the ""transfer theorem"", showing that differentially private analyses generalize. The paper is here: <LINK> and I wrote a blog post about it here: <LINK>.  Continue on for the TL;DR. 1/ <LINK>', 'It is an old observation (I think originally made by @frankmcsherry) that differentially private analyses generalize in expectation. If you (say) privately learn a classifier, then in expectation its test error will be close to its training error. 2/', 'This is true of many stability notions, but differential privacy is special because it composes adaptively. So you could actually train a sequence of classifiers on the same dataset, using previous outcomes to guide your search, and still not overfit in expectation. 3/', 'But these ""in expecation"" bounds aren\'t enough to derive confidence intervals, and so a line of work has emerged lifting these to high probability bounds. Our original paper (DFHPRR in the plot) got the first such bound via a direct moment calculation, but it wasn\'t tight. 4/', 'The BNSSSU paper (see the plot) introduced the ingenious ""monitor technique"" which let them prove the asymptotically optimal bound. This idea has proven useful elsewhere. But it also seems to lead to large constant overhead, even though the asymptotics are tight. We avoid it. 5/', ""Here's our proof: Pretend that after all analyses are complete, the dataset is resampled from its posterior distribution conditioned on the output of the mechanism. This doesn't change the joint distribution on datasets and outputs. So empirical accuracy guarantees still hold. 6/"", 'But the resampled dataset will take query values that are with high probability that close to their posterior means. So the only way the mechanism can promise empirical accuracy is if it promises accuracy with respect to this posterior distribution with high probability. 7/', ""But differential privacy implies that the posterior expectation of query values must be close to their prior (true) values. And thats it. We get high probability bounds for free, from the high probability sample accuracy bounds. Thats why we don't lose large constants. 8/"", 'This is joint work with terrific co-authors @chrisjung93, Katrina Ligett, @privatealgos, Saeed Sharifi, and Moshe Shenfeld. (So lets call it JLNRSS). It was really fun to work on. And consistent with prior work, we see that transfer theorems require six authors. 9/9']",19,09,2408
245,82,975920258518315008,3333301606,"Keith Hawkins, Ph.D.","Cool result led by Yuan-Sen Ting using a method we developed allowing astronomers to find clump stars burning helium at the cores. We put the method to use to identify 100,000+ of these special stars which can/will be used build a 3-D map our Galaxy!<LINK>",https://arxiv.org/abs/1803.06650,"Core helium-burning red clump (RC) stars are excellent standard candles in the Milky Way. These stars may have more precise distance estimates from spectrophotometry than from Gaia parallaxes beyond 3 kpc. However, RC stars have $T_{\rm eff}$ and log g very similar to some red giant branch (RGB) stars. Especially for low-resolution spectroscopic studies where $T_{\rm eff}$, log g, and [Fe/H] can only be estimated with limited precision, separating RC stars from RGB through established method can incur ~20% contamination. Recently, Hawkins et al. (2018) demonstrated that the additional information in single-epoch spectra, such as the C/N ratio, can be exploited to cleanly differentiate RC and RGB stars. In this second paper of the series, we establish a data-driven mapping from spectral flux space to independently determined asteroseismic parameters, the frequency and the period spacing. From this, we identify 210,371 RC stars from the publicly available LAMOST DR3 and APOGEE DR14 data, with ~9% of contamination. We provide an RC sample of 92,249 stars with a contamination of only ~3%, by restricting the combined analysis to LAMOST stars with ${\rm S/N}_{\rm pix}$ > 75. This demonstrates that high-S/N, low-resolution spectra covering a broad wavelength range can identify RC samples at least as pristine as their high-resolution counterparts. As coming and ongoing surveys such as TESS, DESI, and LAMOST will continue to improve the overlapping training spectroscopic-asteroseismic sample, the method presented in this study provides an efficient and straightforward way to derive a vast yet pristine RC stars to reveal the 3D structure of the Milky Way. ","A large and pristine sample of standard candles across the Milky Way:
  ~100,000 red clump stars with 3% contamination",1,"['Cool result led by Yuan-Sen Ting using a method we developed allowing astronomers to find clump stars burning helium at the cores. We put the method to use to identify 100,000+ of these special stars which can/will be used build a 3-D map our Galaxy!<LINK>']",18,03,256
246,139,1281397084898222080,1074633382452051969,Kimin,"Can ensemble improve off-policy RL by handling various issues? Yes! We present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. New work with @MishaLaskin @AravSrinivas and @pabbeel  Paper: <LINK> 1/N High-level idea: SUNRISE integrates three key ingredients: (a) bootstrap with random initialization for the stability of the learning process, (b) weighted Bellman backups for mitigating error propagation in Q-learning, and (c) an inference method for efficient exploration.  2/N <LINK> Can SUNRISE be useful for continuous control tasks on low-dimensional environments? Yes, SUNRISE consistently improves the performance of SAC across all environments and outperforms the state-of-the-art MBRL baselines like PETS and POPLIN. 3/N <LINK> Can SUNRISE be useful for continuous control tasks on high-dimensional environments? Yes, SUNRISE improves the performance of RAD on all environments from DM Control Suite and achieves SOTA in almost all environments against existing pixel-based RL methods. 4/N <LINK> Can SUNRISE be useful for discrete control tasks on high-dimensional environments? Yes, SUNRISE improves the performance of Rainbow in Atari environments, and achieves state-of-the-art performance on 12 out of 26 environments 5/N <LINK> Ablation study: Remark that the performance gain from SUNRISE only with bootstrap, which corresponds to a naive extension of Bootstrap DQN (<LINK>), is marginal compared to other techniques, such as weighted Bellman backup and UCB exploration. 6/N <LINK> Special thanks to collaborators: @MishaLaskin @AravSrinivas and @pabbeel ! code is available at <LINK> @tesslerc Thank you for the suggestion! I'm gonna update the draft accordingly 😀 @agarwl_ @AravSrinivas Thank you for the pointer!!! I think finding in REM is very interesting and related to SUNRISE 😀",https://arxiv.org/abs/2007.04938,"Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments. Our training code is available at this https URL ","SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep
  Reinforcement Learning",10,"['Can ensemble improve off-policy RL by handling various issues?\n\nYes! We present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. New work with @MishaLaskin @AravSrinivas and @pabbeel \n\nPaper: <LINK>\n\n1/N', 'High-level idea:\nSUNRISE integrates three key ingredients: (a) bootstrap with random initialization for the stability of the learning process, (b) weighted Bellman backups for mitigating error propagation in Q-learning, and (c) an inference method for efficient exploration. \n\n2/N https://t.co/8vtP1lj3xT', 'Can SUNRISE be useful for continuous control tasks on low-dimensional environments?\n\nYes, SUNRISE consistently improves the performance of SAC across all environments and outperforms the state-of-the-art MBRL baselines like PETS and POPLIN.\n\n3/N https://t.co/RfHyNmIq4o', 'Can SUNRISE be useful for continuous control tasks on high-dimensional environments?\n\nYes, SUNRISE improves the performance of RAD on all environments from DM Control Suite and achieves SOTA in almost all environments against existing pixel-based RL methods.\n\n4/N https://t.co/Pna340c03V', 'Can SUNRISE be useful for discrete control tasks on high-dimensional environments?\n\nYes, SUNRISE improves the performance of Rainbow in Atari environments, and achieves state-of-the-art performance on 12 out of 26 environments\n\n5/N https://t.co/krMDV8bjEn', 'Ablation study:\n\nRemark that the performance gain from SUNRISE only with bootstrap, which corresponds to a naive extension of Bootstrap DQN (https://t.co/dCoQvEoWPu), is marginal compared to other techniques, such as weighted Bellman backup and UCB exploration.\n\n6/N https://t.co/PopXyyzsOq', 'Special thanks to collaborators: @MishaLaskin @AravSrinivas and @pabbeel !', 'code is available at https://t.co/fypvke0Iqr', ""@tesslerc Thank you for the suggestion! I'm gonna update the draft accordingly 😀"", '@agarwl_ @AravSrinivas Thank you for the pointer!!! I think finding in REM is very interesting and related to SUNRISE 😀']",20,07,1856
247,83,1328238068961144832,1204724799211220993,Christian Rohrhofer 🇺🇦,Let me show you our recent paper on the U(1) anomaly at high T: <LINK> More statistics and proper control over the chirality of fermions reinforce previous findings that the effects of U(1) anomaly disappear. Bonus: fancy new fits for screening masses!,http://arxiv.org/abs/2011.01499,"We investigate the axial U(1) anomaly of two-flavor QCD at temperatures 190--330 MeV. In order to preserve precise chiral symmetry on the lattice, we employ the Mobius domain-wall fermion action as well as overlap fermion action implemented with a stochastic reweighting technique. Compared to our previous studies, we reduce the lattice spacing to 0.07 fm, simulate larger multiple volumes to estimate finite size effect, and take more than four quark mass points, including one below physical point to investigate the chiral limit. We measure the topological susceptibility, axial U(1) susceptibility, and examine the degeneracy of U(1) partners in meson and baryon correlators. All the data above the critical temperature indicate that the axial U(1) violation is consistent with zero within statistical errors. The quark mass dependence suggests disappearance of the U(1) anomaly at a rate comparable to that of the SU(2)_L x SU(2)_R symmetry breaking. ","Study of the axial U(1) anomaly at high temperature with lattice chiral
  fermions",1,['Let me show you our recent paper on the U(1) anomaly at high T: <LINK>\n\nMore statistics and proper control over the chirality of fermions reinforce previous findings that the effects of U(1) anomaly disappear.\n\nBonus: fancy new fits for screening masses!'],20,11,252
248,63,1506688502603431940,1246070462679040000,Randall Balestriero,"Happy to share our #CVPR2022 paper w/ @imtiazprio, @rbaraniuk providing a simple solution to provably sample from the (anti-)modes of pre-trained generative networks... also leading to new StyleGAN2/3/BigGAN FID SOTAs 🧵(1/4) <LINK> colab: <LINK> <LINK> Because the modes of the learned distribution correspond to high-confidence samples, the corresponding samples tend to be high-quality (precision). Vice-versa, focusing on the anti-modes provides more diversity (recall) 🧵(2/4) <LINK> As a by-product of controlling the quality and diversity of the samples, it is possible to find a sweet spot that produces better FID than the original pre-trained DGN, improving current SOTA on many popular pre-trained models e.g. StyleGAN2/3, BigGAN 🧵(3/4) <LINK> This work extends our previous #iclr2022 paper (<LINK>) that was also based on spline theory to provide uniform sampling on the learned distribution... showing that splines are a powerful theoretical tool to study and improve deep networks! 🧵(4/4)",https://arxiv.org/abs/2203.01993,"We present Polarity Sampling, a theoretically justified plug-and-play method for controlling the generation quality and diversity of pre-trained deep generative networks DGNs). Leveraging the fact that DGNs are, or can be approximated by, continuous piecewise affine splines, we derive the analytical DGN output space distribution as a function of the product of the DGN's Jacobian singular values raised to a power $\rho$. We dub $\rho$ the $\textbf{polarity}$ parameter and prove that $\rho$ focuses the DGN sampling on the modes ($\rho < 0$) or anti-modes ($\rho > 0$) of the DGN output-space distribution. We demonstrate that nonzero polarity values achieve a better precision-recall (quality-diversity) Pareto frontier than standard methods, such as truncation, for a number of state-of-the-art DGNs. We also present quantitative and qualitative results on the improvement of overall generation quality (e.g., in terms of the Frechet Inception Distance) for a number of state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different conditional and unconditional image generation tasks. In particular, Polarity Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the AFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-samp ","Polarity Sampling: Quality and Diversity Control of Pre-Trained
  Generative Networks via Singular Values",4,"['Happy to share our #CVPR2022 paper w/ @imtiazprio, @rbaraniuk providing a simple solution to provably sample from the (anti-)modes of pre-trained generative networks... also leading to new StyleGAN2/3/BigGAN FID SOTAs\n🧵(1/4)\n<LINK>\ncolab: <LINK> <LINK>', 'Because the modes of the learned distribution correspond to high-confidence samples, the corresponding samples tend to be high-quality (precision). Vice-versa, focusing on the anti-modes provides more diversity (recall)\n\n🧵(2/4) https://t.co/xJwQad9ah6', 'As a by-product of controlling the quality and diversity of the samples, it is possible to find a sweet spot that produces better FID than the original pre-trained DGN, improving current SOTA on many popular pre-trained models e.g. StyleGAN2/3, BigGAN\n\n🧵(3/4) https://t.co/Fq7BSWic8t', 'This work extends our previous #iclr2022 paper (https://t.co/0JZpYShgNW) that was also based on spline theory to provide uniform sampling on the learned distribution... showing that splines are a powerful theoretical tool to study and improve deep networks!\n\n🧵(4/4)']",22,03,1000
249,194,1393268069703639040,713117884239753216,Xiaolong Wang,"Cycle comes again: we propose contrastive learning with cross-video cycle-consistency. Instead of learning by augmenting a single image, our method forms a cycle across different videos to provide positive training pairs from different instances.  <LINK> (1/n) <LINK> Start from one frame in a video, we find its soft nearest neighbor from other videos as a forward step. The cycle-consistency is achieved when the soft nearest neighbor finds its closest frame back within the same video as the start frame.  (2/n) <LINK> The self-supervised image representation can be generalized to multiple downstream tasks beyond action recognition in videos, including image classification and object tracking.  Project page: <LINK>… Joint work with @Happy_Wu (3/n) @kevin_zakka It needs to have two videos from the same action class to form the cycle. So in the sense of knowing the two videos are from the same action class, it is labeled. @kevin_zakka But this is just stating the fact in the TCC experiments. If you are using PennAction category labels to get the pairs, it is ""require human annotators to provide grouth-truth pairs"" no? We are not trying to downgrade TCC, it is great work, and the statement is also neutral. @kevin_zakka And I also think it makes perfect sense for TCC to align two videos from the same category since it is performing a frame-level alignment. It will not make sense for TCC to apply on aligning two completely different action videos. For us, we are not trying to align two videos.",https://arxiv.org/abs/2105.06463,"Recent works have advanced the performance of self-supervised representation learning by a large margin. The core among these methods is intra-image invariance learning. Two different transformations of one image instance are considered as a positive sample pair, where various tasks are designed to learn invariant representations by comparing the pair. Analogically, for video data, representations of frames from the same video are trained to be closer than frames from other videos, i.e. intra-video invariance. However, cross-video relation has barely been explored for visual representation learning. Unlike intra-video invariance, ground-truth labels of cross-video relation is usually unavailable without human labors. In this paper, we propose a novel contrastive learning method which explores the cross-video relation by using cycle-consistency for general image representation learning. This allows to collect positive sample pairs across different video instances, which we hypothesize will lead to higher-level semantics. We validate our method by transferring our image representation to multiple downstream tasks including visual object tracking, image classification, and action recognition. We show significant improvement over state-of-the-art contrastive learning methods. Project page is available at this https URL ","Contrastive Learning of Image Representations with Cross-Video
  Cycle-Consistency",6,"['Cycle comes again: we propose contrastive learning with cross-video cycle-consistency. Instead of learning by augmenting a single image, our method forms a cycle across different videos to provide positive training pairs from different instances. \n\n<LINK>\n(1/n) <LINK>', 'Start from one frame in a video, we find its soft nearest neighbor from other videos as a forward step. The cycle-consistency is achieved when the soft nearest neighbor finds its closest frame back within the same video as the start frame. \n\n(2/n) https://t.co/HYxWYKAcki', 'The self-supervised image representation can be generalized to multiple downstream tasks beyond action recognition in videos, including image classification and object tracking. \n\nProject page: https://t.co/Fd4lOmC97M…\n\nJoint work with @Happy_Wu\n\n(3/n)', '@kevin_zakka It needs to have two videos from the same action class to form the cycle. So in the sense of knowing the two videos are from the same action class, it is labeled.', '@kevin_zakka But this is just stating the fact in the TCC experiments. If you are using PennAction category labels to get the pairs, it is ""require human annotators to provide grouth-truth pairs"" no? We are not trying to downgrade TCC, it is great work, and the statement is also neutral.', '@kevin_zakka And I also think it makes perfect sense for TCC to align two videos from the same category since it is performing a frame-level alignment. It will not make sense for TCC to apply on aligning two completely different action videos. For us, we are not trying to align two videos.']",21,05,1510
250,154,1384512694183661572,65432023,Prof. Costas Andreopoulos,New #neutrino x-section pheno paper by my PhD student Jùlia Tena-Vidal <LINK>. First in a series of papers on the new tunes of the well-known #GENIE MC. Next: Hadronization. Funded by @livuniphysics LIV.DAT. Initial work supported by an @IPPP_Durham associateship,https://arxiv.org/abs/2104.09179,"We summarise the results of a study performed within the GENIE global analysis framework, revisiting the GENIE bare-nucleon cross-section tuning and, in particular, the tuning of a) the inclusive cross-section, b) the cross-section of low-multiplicity inelastic channels (single-pion and double-pion production), and c) the relative contributions of resonance and non-resonance processes to these final states. The same analysis was performed with several different comprehensive cross-section model sets available in GENIE Generator v3. In this work we performed a careful investigation of the observed tensions between exclusive and inclusive data, and installed analysis improvements to handle systematics in historic data. All tuned model configurations discussed in this paper are available through public releases of the GENIE Generator. With this paper we aim to support the consumers of these physics tunes by providing comprehensive summaries of our alternate model constructions, of the relevant datasets and their systematics, and of our tuning procedure and results. ",Neutrino-Nucleon Cross-Section Model Tuning in GENIE v3,1,['New #neutrino x-section pheno paper by my PhD student Jùlia Tena-Vidal <LINK>. First in a series of papers on the new tunes of the well-known #GENIE MC. Next: Hadronization. Funded by @livuniphysics LIV.DAT. Initial work supported by an @IPPP_Durham associateship'],21,04,263
251,6,1069582882715381761,216729597,Marcel S. Pawlowski,"First day in my new job and I already have a paper on the arXiv 😉 This one, lead by Tyler Kelley, presents his new Phat ELVIS suite of simulations which investigates how the potential of a MW-like disk affects dark matter substructures in their halos. <LINK> <LINK>",https://arxiv.org/abs/1811.12413,"We introduce an extension of the ELVIS project to account for the effects of the Milky Way galaxy on its subhalo population. Our simulation suite, Phat ELVIS, consists of twelve high-resolution cosmological dark matter-only (DMO) zoom simulations of Milky Way-size $\Lambda$CDM~ haloes ($M_{\rm v} = 0.7-2 \times 10^{12} \,\mathrm{M}_\odot$) along with twelve re-runs with embedded galaxy potentials grown to match the observed Milky Way disk and bulge today. The central galaxy potential destroys subhalos on orbits with small pericenters in every halo, regardless of the ratio of galaxy mass to halo mass. This has several important implications. 1) Most of the $\mathtt{Disk}$ runs have no subhaloes larger than $V_{\rm max} = 4.5$ km s$^{-1}$ within $20$ kpc and a significant lack of substructure going back $\sim 8$ Gyr, suggesting that local stream-heating signals from dark substructure will be rare. 2) The pericenter distributions of Milky Way satellites derived from $\mathit{Gaia}$ data are remarkably similar to the pericenter distributions of subhaloes in the $\mathtt{Disk}$ runs, while the DMO runs drastically over-predict galaxies with pericenters smaller than 20 kpc. 3) The enhanced destruction produces a tension opposite to that of the classic `missing satellites' problem: in order to account for ultra-faint galaxies known within $30$ kpc of the Galaxy, we must populate haloes with $V_\mathrm{peak} \simeq 7$ km s$^{-1}$ ($M \simeq 3 \times 10^{7} \,\mathrm{M}_\odot$ at infall), well below the atomic cooling limit of $V_\mathrm{peak} \simeq 16$ km s$^{-1}$ ($M \simeq 5 \times 10^{8} \,\mathrm{M}_\odot$ at infall). 4) If such tiny haloes do host ultra-faint dwarfs, this implies the existence of $\sim 1000$ satellite galaxies within 300 kpc of the Milky Way. ","Phat ELVIS: The inevitable effect of the Milky Way's disk on its dark
  matter subhaloes",1,"['First day in my new job and I already have a paper on the arXiv 😉\n\nThis one, lead by Tyler Kelley, presents his new Phat ELVIS suite of simulations which investigates how the potential of a MW-like disk affects dark matter substructures in their halos. <LINK> <LINK>']",18,11,265
252,110,1273269846944501770,423671718,Joshua Loftus #peace,"New #algorithmicfairness #fairAI paper with Ke Yang and @stoyanoj  We propose causal models with multiple sensitive attributes as a formal approach to intersectionality, and apply the framework to fair ranking tasks Preprint: <LINK> <LINK> These models may allow us to carefully disentangle the ""bundles of sticks,"" to use @maya_sen and @owasow's metaphor for sensitive attributes (<LINK>) Our ranking idea: treat everyone as though they belong to the same intersectional subgroup (equal treatment?) <LINK> We illustrate these ideas on several real and synthetic examples. In the one below, job applicants to a fictional moving company are required to pass a weight-lifting test. There is both direct and indirect discrimination on the basis of both race and gender. (end of thread) <LINK> (Addendum) In above figure the selection rate is % of subgroup ranked in the top 50, 100, or 200, i.e. would be hired. By removing direct discrimination but keeping the weight-lifting requirement, our method results in many more black men being hired in this example @lastpositivist @stoyanoj Thanks! Sorry we missed it earlier, not sure how I forgot because I definitely have it open in a tab somewhere... we'll be sure to cite you in the next update",https://arxiv.org/abs/2006.08688,"In this paper we propose a causal modeling approach to intersectional fairness, and a flexible, task-specific method for computing intersectionally fair rankings. Rankings are used in many contexts, ranging from Web search results to college admissions, but causal inference for fair rankings has received limited attention. Additionally, the growing literature on causal fairness has directed little attention to intersectionality. By bringing these issues together in a formal causal framework we make the application of intersectionality in fair machine learning explicit, connected to important real world effects and domain knowledge, and transparent about technical limitations. We experimentally evaluate our approach on real and synthetic datasets, exploring its behaviour under different structural assumptions. ",Causal intersectionality for fair ranking,5,"['New #algorithmicfairness #fairAI paper with Ke Yang and @stoyanoj \n\nWe propose causal models with multiple sensitive attributes as a formal approach to intersectionality, and apply the framework to fair ranking tasks\n\nPreprint: <LINK> <LINK>', 'These models may allow us to carefully disentangle the ""bundles of sticks,"" to use @maya_sen and @owasow\'s metaphor for sensitive attributes (https://t.co/RB05f0lBT0)\n\nOur ranking idea: treat everyone as though they belong to the same intersectional subgroup (equal treatment?) https://t.co/WMbSxVxvDG', 'We illustrate these ideas on several real and synthetic examples. In the one below, job applicants to a fictional moving company are required to pass a weight-lifting test. There is both direct and indirect discrimination on the basis of both race and gender.\n\n(end of thread) https://t.co/KTgi8rzLol', '(Addendum) In above figure the selection rate is % of subgroup ranked in the top 50, 100, or 200, i.e. would be hired.\n\nBy removing direct discrimination but keeping the weight-lifting requirement, our method results in many more black men being hired in this example', ""@lastpositivist @stoyanoj Thanks! Sorry we missed it earlier, not sure how I forgot because I definitely have it open in a tab somewhere... we'll be sure to cite you in the next update""]",20,06,1241
253,78,1372985207519735808,1727424900,Cory Hargus,"Diffusion is a statistical consequence of random motion. What happens when random motion is *chiral*, e.g. preferring left turns over right turns? In a new paper we show how a non-dissipative ""odd diffusivity"" emerges, including in chiral active matter. <LINK> Another team effort with coauthors @jmlepstein and Kranthi Mandadapu!",https://arxiv.org/abs/2103.09958,"Diffusive transport is characterized by a diffusivity tensor which may, in general, contain both a symmetric and an antisymmetric component. Although the latter is often neglected, we derive Green-Kubo relations showing it to be a general characteristic of random motion breaking time-reversal and parity symmetries, as encountered in chiral active matter. In analogy with the odd viscosity appearing in chiral active fluids, we term this component the odd diffusivity. We show how odd diffusivity emerges in a chiral random walk model, and demonstrate the applicability of the Green-Kubo relations through molecular dynamics simulations of a passive tracer particle diffusing in a chiral active bath. ",Odd Diffusivity of Chiral Random Motion,2,"['Diffusion is a statistical consequence of random motion. What happens when random motion is *chiral*, e.g. preferring left turns over right turns? In a new paper we show how a non-dissipative ""odd diffusivity"" emerges, including in chiral active matter. <LINK>', 'Another team effort with coauthors @jmlepstein and Kranthi Mandadapu!']",21,03,330
254,94,1259009592287277056,90131577,Noam Slonim 🟢,"Can we automatically find an opinion article that specifically counters an article we just read? Check out a new ACL-2020 paper, coming out from our #ProjectDebater team. And we also release 3.6k (!) recorded debate speeches! Congrats to the authors! <LINK>",https://arxiv.org/abs/2005.01157,"An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in ""echo chambers"" and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns -- that of detecting articles that most effectively counter the arguments -- and not just the stance -- made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research. ",Out of the Echo Chamber: Detecting Countering Debate Speeches,1,"['Can we automatically find an opinion article that specifically counters an article we just read? Check out a new ACL-2020 paper, coming out from our #ProjectDebater team. And we also release 3.6k (!) recorded debate speeches! Congrats to the authors!\n\n<LINK>']",20,05,257
255,68,1130721831177080832,16434310,chrislintott,New paper on arXiv this morning from @mike_w_ai and the @galaxyzoo team - Bayesian machine learning for galaxy classification. <LINK> (Less technical explanation coming shortly!) @anais_moller @mike_w_ai @galaxyzoo I've just found your paper - I'll have a look!,https://arxiv.org/abs/1905.07424,"We use Bayesian convolutional neural networks and a novel generative model of Galaxy Zoo volunteer responses to infer posteriors for the visual morphology of galaxies. Bayesian CNN can learn from galaxy images with uncertain labels and then, for previously unlabelled galaxies, predict the probability of each possible label. Our posteriors are well-calibrated (e.g. for predicting bars, we achieve coverage errors of 11.8% within a vote fraction deviation of 0.2) and hence are reliable for practical use. Further, using our posteriors, we apply the active learning strategy BALD to request volunteer responses for the subset of galaxies which, if labelled, would be most informative for training our network. We show that training our Bayesian CNNs using active learning requires up to 35-60% fewer labelled galaxies, depending on the morphological feature being classified. By combining human and machine intelligence, Galaxy Zoo will be able to classify surveys of any conceivable scale on a timescale of weeks, providing massive and detailed morphology catalogues to support research into galaxy evolution. ","Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active
  Learning",2,"['New paper on arXiv this morning from @mike_w_ai and the @galaxyzoo team - Bayesian machine learning for galaxy classification. <LINK> (Less technical explanation coming shortly!)', ""@anais_moller @mike_w_ai @galaxyzoo I've just found your paper - I'll have a look!""]",19,05,261
256,10,1488218563894730752,1003652696723873792,Max Gaspari,"New exciting paper on probing multiphase gas condensation in non-central galaxies with SOFIA, ALMA, MUSE, VLA observations and simulations. The #BlackHoleWeather develops also here but in a more compressed core, rather than an extended rain! <LINK> #astrophysics <LINK> Some key numerical predictions and comparison with the #ChaoticColdAccretion (CCA) theory. Most of the cold/warm clouds (left) in non-central galaxies can be consistently explained via CCA (internal condensation), with a nuclear rain traced via the C-ratio (right). #astronomy <LINK>",https://arxiv.org/abs/2201.09330,"We investigate the cold and warm gas content, kinematics, and spatial distribution of six local massive elliptical galaxies to probe the origin of the multiphase gas in their atmospheres. We report new observations, including SOFIA [CII], ALMA CO, MUSE H$\alpha$+[NII] and VLA radio observations. These are complemented by a large suite of multiwavelength archival datasets, including thermodynamical properties of the hot gas and radio jets, which are leveraged to investigate the role of AGN feeding/feedback in regulating the multiphase gas content. Our galaxy sample shows a significant diversity in cool gas content, spanning filamentary and rotating structures. In our non-central galaxies, the distribution of such gas is often concentrated, at variance with the more extended features observed in central galaxies. Misalignment between the multiphase gas and stars suggest that stellar mass loss is not the primary driver. A fraction of the cool gas might be acquired via galaxy interactions, but we do not find quantitative evidence of mergers in most of our systems. Instead, key evidence supports the origin via condensation out of the diffuse halo. Comparing with Chaotic Cold Accretion (CCA) simulations, we find that our cool gas-free galaxies are likely in the overheated phase of the self-regulated AGN cycle, while for our galaxies with cool gas the k-plot and AGN power correlation corroborate the phase of CCA feeding in which the condensation rain is triggering more vigorous AGN heating. The related C-ratio further shows that central/non-central galaxies are expected to generate an extended/inner rain, consistent with our sample. ","Probing multiphase gas in local massive elliptical galaxies via
  multiwavelength observations",2,"['New exciting paper on probing multiphase gas condensation in non-central galaxies with SOFIA, ALMA, MUSE, VLA observations and simulations. The #BlackHoleWeather develops also here but in a more compressed core, rather than an extended rain!\n<LINK>\n#astrophysics <LINK>', 'Some key numerical predictions and comparison with the #ChaoticColdAccretion (CCA) theory. Most of the cold/warm clouds (left) in non-central galaxies can be consistently explained via CCA (internal condensation), with a nuclear rain traced via the C-ratio (right).\n#astronomy https://t.co/9HLhi3Wt5F']",22,01,553
257,62,997184401606893569,3317340592,Eran Segal,Regularization Network Learning: Our new paper on how to adapt Deep Neural Networks to settings of tabular data where the relative importance of different inputs can greatly vary We show applications for predicting human traits from gut microbiome <LINK> <LINK>,https://arxiv.org/abs/1805.06440,"Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at this https URL ",Regularization Learning Networks: Deep Learning for Tabular Datasets,1,['Regularization Network Learning: Our new paper on how to adapt Deep Neural Networks to settings of tabular data where the relative importance of different inputs can greatly vary\nWe show applications for predicting human traits from gut microbiome\n<LINK> <LINK>'],18,05,261
258,272,1412922727145435136,504239269,Vu Nguyen,1⃣Tuning RL hypers is expensive 2⃣Do you want to optimize them without retraining the whole agent each time? 3⃣Hypers can be continuous or mixed categorical-continuous? We propose to optimize mixed categorical-continuous hypers on-the-fly for AutoRL 👉<LINK> <LINK>,https://arxiv.org/abs/2106.15883,"Despite a series of recent successes in reinforcement learning (RL), many RL algorithms remain sensitive to hyperparameters. As such, there has recently been interest in the field of AutoRL, which seeks to automate design decisions to create more general algorithms. Recent work suggests that population based approaches may be effective AutoRL algorithms, by learning hyperparameter schedules on the fly. In particular, the PB2 algorithm is able to achieve strong performance in RL tasks by formulating online hyperparameter optimization as time varying GP-bandit problem, while also providing theoretical guarantees. However, PB2 is only designed to work for continuous hyperparameters, which severely limits its utility in practice. In this paper we introduce a new (provably) efficient hierarchical approach for optimizing both continuous and categorical variables, using a new time-varying bandit algorithm specifically designed for the population based training regime. We evaluate our approach on the challenging Procgen benchmark, where we show that explicitly modelling dependence between data augmentation and other hyperparameters improves generalization. ","Tuning Mixed Input Hyperparameters on the Fly for Efficient Population
  Based AutoRL",1,['1⃣Tuning RL hypers is expensive \n2⃣Do you want to optimize them without retraining the whole agent each time? \n3⃣Hypers can be continuous or mixed categorical-continuous?\n\nWe propose to optimize mixed categorical-continuous hypers on-the-fly for AutoRL\n\n👉<LINK> <LINK>'],21,06,264
259,75,1080962165706641408,2285825876,Johanna,"&lt;AHEM&gt; New paper out today! Led by ⁦@cabridelle⁩, ⁦@M_N_Guenther⁩ of ⁦@MIT⁩, and friends (incl. me)!⁩ [1901.00051] The Longest Period TESS Planet Yet: A Sub-Neptune Transiting A Bright, Nearby K Dwarf Star (1/2) <LINK> Exciting b/c (1) 1st transiting planet system found amongst our @LCOAstro Magellan/PFS long-term RV survey for planets (we’ve been watching this star years!), (2) one planet is fairly long-period, was 1st only a single-transit detection, (3) smaller candidate planet is ~**1 R🌍**",https://arxiv.org/abs/1901.00051,"The future of exoplanet science is bright, as TESS once again demonstrates with the discovery of its longest-period confirmed planet to date. We hereby present HD 21749b (TOI 186.01), a sub-Neptune in a 36-day orbit around a bright (V = 8.1) nearby (16 pc) K4.5 dwarf. TESS measures HD21749b to be 2.61$^{+0.17}_{-0.16}$ $R_{\oplus}$, and combined archival and follow-up precision radial velocity data put the mass of the planet at $22.7^{+2.2}_{-1.9}$ $M_{\oplus}$. HD 21749b contributes to the TESS Level 1 Science Requirement of providing 50 transiting planets smaller than 4 $R_{\oplus}$ with measured masses. Furthermore, we report the discovery of HD 21749c (TOI 186.02), the first Earth-sized ($R_p = 0.892^{+0.064}_{-0.058} R_{\oplus}$) planet from TESS. The HD21749 system is a prime target for comparative studies of planetary composition and architecture in multi-planet systems. ",TESS delivers its first Earth-sized planet and a warm sub-Neptune,2,"['&lt;AHEM&gt; New paper out today! Led by \u2066@cabridelle\u2069, \u2066@M_N_Guenther\u2069 of \u2066@MIT\u2069, and friends (incl. me)!\u2069 [1901.00051] The Longest Period TESS Planet Yet: A Sub-Neptune Transiting A Bright, Nearby K Dwarf Star (1/2) <LINK>', 'Exciting b/c (1) 1st transiting planet system found amongst our @LCOAstro Magellan/PFS long-term RV survey for planets (we’ve been watching this star years!), (2) one planet is fairly long-period, was 1st only a single-transit detection, (3) smaller candidate planet is ~**1 R🌍**']",19,01,504
260,294,1320561969796161536,2506570218,Julian,We recently open-sourced SMARTS (<LINK>); a simulation platform for RL and multi-agent research on autonomous driving. Focus is on realistic + diverse interactions. The associated paper (<LINK>) was accepted to CoRL. Hope you find it interesting! <LINK>,https://arxiv.org/abs/2010.09776,"Multi-agent interaction is a fundamental aspect of autonomous driving in the real world. Despite more than a decade of research and development, the problem of how to competently interact with diverse road users in diverse scenarios remains largely unsolved. Learning methods have much to offer towards solving this problem. But they require a realistic multi-agent simulator that generates diverse and competent driving interactions. To meet this need, we develop a dedicated simulation platform called SMARTS (Scalable Multi-Agent RL Training School). SMARTS supports the training, accumulation, and use of diverse behavior models of road users. These are in turn used to create increasingly more realistic and diverse interactions that enable deeper and broader research on multi-agent interaction. In this paper, we describe the design goals of SMARTS, explain its basic architecture and its key features, and illustrate its use through concrete multi-agent experiments on interactive scenarios. We open-source the SMARTS platform and the associated benchmark tasks and evaluation metrics to encourage and empower research on multi-agent learning for autonomous driving. Our code is available at this https URL ","SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for
  Autonomous Driving",1,['We recently open-sourced SMARTS (<LINK>); a simulation platform for RL and multi-agent research on autonomous driving. Focus is on realistic + diverse interactions. The associated paper (<LINK>) was accepted to CoRL. Hope you find it interesting! <LINK>'],20,10,253
261,176,1400149998831128576,446694758,Julian Eisenschlos,"Transformers work well for tasks involving tabular data, but what if the table requires more tokens than the model can fit? In our new #ACL2021 Findings short paper <LINK> by Syrine Krichene, @muelletm and myself, we tackle this problem by doubling down ‼️ 1/3 A first *small* Transformer based on TAPAS selects the important parts of the input in an end-to-end differentiable way. The output of the first model is mapped to a score from -∞ to 0 for each token and then added to the attention mask of a second model before the softmax. 2/3 <LINK> On the limit, DoT (Double Transformer) coincides exactly with removing the tokens from the input. Hence we can implement it efficiently and obtain speedups from 1.5x to 4.6x in TabFact and WikiSQL with negligible accuracy losses. Check out the code in <LINK> 3/3",http://arxiv.org/abs/2106.00479,"Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model. ",DoT: An efficient Double Transformer for NLP tasks with tables,3,"['Transformers work well for tasks involving tabular data, but what if the table requires more tokens than the model can fit? In our new #ACL2021 Findings short paper <LINK> by Syrine Krichene,\n@muelletm and myself, we tackle this problem by doubling down ‼️\n\n1/3', 'A first *small* Transformer based on TAPAS selects the important parts of the input in an end-to-end differentiable way. The output of the first model is mapped to a score from -∞ to 0 for each token and then added to the attention mask of a second model before the softmax.\n\n2/3 https://t.co/4zTOvOsc3g', 'On the limit, DoT (Double Transformer) coincides exactly with removing the tokens from the input. Hence we can implement it efficiently and obtain speedups from 1.5x to 4.6x in TabFact and WikiSQL with negligible accuracy losses. Check out the code in https://t.co/1VxCSrATvT\n\n3/3']",21,06,809
262,144,1456180048222998528,569063423,Heino Falcke,"New official paper by @ehtelescope led by @azstewobs group. Low variability we find on large interferometry baseline triangles is well explained if the ring we see in M87* ist indeed gravitational and not due to plasma effects. Technical but promising. <LINK> <LINK> Followed by investigation of how general we can relate size of the black hole shadow to the bright ring we see, even if the theory of gravity deviates from general relativity. Turns out the shadow is a robust measure of spacetime properties of black holes <LINK> <LINK> Maybe useful to say that we introduced the concept of a black hole shadow in a paper in 2000 with @AgolEric and Melia and you can find a more intuitive description in this recent paper with @thomasbronzwaer <LINK> <LINK>",https://arxiv.org/abs/2111.01317,"The black-hole images obtained with the Event Horizon Telescope (EHT) are expected to be variable at the dynamical timescale near their horizons. For the black hole at the center of the M87 galaxy, this timescale (5-61 days) is comparable to the 6-day extent of the 2017 EHT observations. Closure phases along baseline triangles are robust interferometric observables that are sensitive to the expected structural changes of the images but are free of station-based atmospheric and instrumental errors. We explored the day-to-day variability in closure phase measurements on all six linearly independent non-trivial baseline triangles that can be formed from the 2017 observations. We showed that three triangles exhibit very low day-to-day variability, with a dispersion of $\sim3-5^\circ$. The only triangles that exhibit substantially higher variability ($\sim90-180^\circ$) are the ones with baselines that cross visibility amplitude minima on the $u-v$ plane, as expected from theoretical modeling. We used two sets of General Relativistic magnetohydrodynamic simulations to explore the dependence of the predicted variability on various black-hole and accretion-flow parameters. We found that changing the magnetic field configuration, electron temperature model, or black-hole spin has a marginal effect on the model consistency with the observed level of variability. On the other hand, the most discriminating image characteristic of models is the fractional width of the bright ring of emission. Models that best reproduce the observed small level of variability are characterized by thin ring-like images with structures dominated by gravitational lensing effects and thus least affected by turbulence in the accreting plasmas. ","The Variability of the Black-Hole Image in M87 at the Dynamical Time
  Scale",3,"['New official paper by @ehtelescope led by @azstewobs group. Low variability we find on large interferometry baseline triangles is well explained if the ring we see in M87* ist indeed gravitational and not due to plasma effects. Technical but  promising. <LINK> <LINK>', 'Followed by investigation of how general we can relate size of the black hole shadow to the bright ring we see, even if the theory of gravity deviates from general relativity. Turns out the shadow is a robust measure of spacetime properties of black holes https://t.co/ItzuROxg7T https://t.co/hlAnqrKYEa', 'Maybe useful to say that we introduced the concept of a black hole shadow in a paper in 2000 with @AgolEric and Melia and you can find a more intuitive description in this recent paper with @thomasbronzwaer https://t.co/8cQgkm1uKs https://t.co/niJSXhWWLt']",21,11,757
263,60,1493898228042973190,621147651,zpenoyre,"new paper - <LINK> - a little over 20,000 astrometric binary(+) candidates in the local 100 parsecs <LINK> these are Gaia sources with high astrometric error (specifically UWE) - most ubiquitously caused by binarity - figure from paper I - <LINK> <LINK> Gaia is sensitive to binaries with a quite specific range of periods, months to decades - so we're only seeing 20-30% of all expected binary systems <LINK> all we have to work with is a single number, the error, not the full astrometric time-series - so the classification is approximate, and in June we'll be able compare our candidates to the few 100,000 non-single stars in Data Release 3 <LINK> <LINK>",https://arxiv.org/abs/2202.06963,"We examine the capacity to identify binary systems from astrometric deviations alone. We apply our analysis to the Gaia eDR3 and DR2 data, specifically the Gaia Catalogue of Nearby Stars. We show we must renormalize (R)UWE over the local volume to avoid biasing local observations, giving a Local Unit Weight Error (LUWE). We use the simple criterion of LUWE>2, along with a handful of quality cuts to remove likely contaminants, to identify unresolved binary candidates. We identify 22,699 binary candidates within 100 pc of the Sun (just under 10% of sources in this volume). We find an astrometric binary candidate fraction of around 20% for giant stars, 10% on the Main Sequence and lower than 1% for White Dwarfs. We also look for Variability Induced Movers, by computing the correlation between photometric variability and astrometric noise -- and show that VIMs may dominate the binary population of sub-Solar mass MS stars. We discuss the possibility and limitations of identifying non-luminous massive companions from astrometry alone, but find that our method is insensitive to these. Finally, we compare the astrometric deviations of MS binaries to the simulated sample from paper I, which show excellent agreement, and compute the astrometric candidate binary fraction as a function of absolute magnitude. ","Astrometric identification of nearby binary stars II: Astrometric
  binaries in the Gaia Catalogue of Nearby Stars",4,"['new paper - <LINK> - a little over 20,000 astrometric binary(+) candidates in the local 100 parsecs <LINK>', 'these are Gaia sources with high astrometric error (specifically UWE) - most ubiquitously caused by binarity - figure from paper I - https://t.co/oRwduTBke8 https://t.co/vpxg9swgc9', ""Gaia is sensitive to binaries with a quite specific range of periods, months to decades - so we're only seeing 20-30% of all expected binary systems https://t.co/dyvunRBvJv"", ""all we have to work with is a single number, the error, not the full astrometric time-series - so the classification is approximate, and in June we'll be able compare our candidates to the few 100,000 non-single stars in Data Release 3 https://t.co/U9Uq1OQbMB https://t.co/aAbkmumP5d""]",22,02,659
264,82,1394264179993808907,369569444,Takahiro TERADA (寺田 隆広),"Our new paper on ""Massless Preheating and Electroweak Vacuum Metastability"" <LINK> Precise measurements of Standard Model parameters suggest that the electroweak vacuum is metastable. We need to ensure the (meta)stability throughout cosmological history. Quantum fluctuations of the Higgs field during cosmic inflation and parametric/tachyonic instability after that might destabilize the electroweak vacuum. We can stabilize the Higgs by introducing an effective mass term in the form of Higgs-inflaton and/or gravitational couplings. However, the stability during and after inflation is typically in a trade-off relation. It is important to study the condition that ensures stability throughout these epochs. <LINK> In our work, we focus on (quasi) scale-invariant models with quartic potentials and non-minimal gravitational couplings. (Why scale invariance? 1. It can explain the flatness of the inflaton potential consistently with observations. 2. It might explain the hierarchy problem.) <LINK> Naively, scale invariance implies unimpeded growth of resonance and hence unavoidable destabilization of the vacuum, which is a cosmological catastrophe. However, we find this is not the case taking into account the perturbative Higgs decay and backreaction of produced particles. <LINK> We find nontrivial and dynamical interplays between the effects of quartic and curvature couplings, which can partially cancel each other in some cases. We finally find disjoint ""islands of (meta)stability"" in the couplings parameter space. <LINK>",http://arxiv.org/abs/2105.06939,"Current measurements of Standard Model parameters suggest that the electroweak vacuum is metastable. This metastability has important cosmological implications, because large fluctuations in the Higgs field could trigger vacuum decay in the early universe. For the false vacuum to survive, interactions which stabilize the Higgs during inflation -- e.g., inflaton-Higgs interactions or non-minimal couplings to gravity -- are typically necessary. However, the post-inflationary preheating dynamics of these same interactions could also trigger vacuum decay, thereby recreating the problem we sought to avoid. This dynamics is often assumed catastrophic for models exhibiting scale invariance since these generically allow for unimpeded growth of fluctuations. In this paper, we examine the dynamics of such ""massless preheating"" scenarios and show that the competing threats to metastability can nonetheless be balanced to ensure viability. We find that fully accounting for both the backreaction from particle production and the effects of perturbative decays reveals a large number of disjoint ""islands of (meta)stability"" over the parameter space of couplings. Ultimately, the interplay among Higgs-stabilizing interactions plays a significant role, leading to a sequence of dynamical phases that effectively extend the metastable regions to large Higgs-curvature couplings. ",Massless Preheating and Electroweak Vacuum Metastability,6,"['Our new paper on ""Massless Preheating and Electroweak Vacuum Metastability"" <LINK>\nPrecise measurements of Standard Model parameters suggest that the electroweak vacuum is metastable. We need to ensure the (meta)stability throughout cosmological history.', 'Quantum fluctuations of the Higgs field during cosmic inflation and parametric/tachyonic instability after that might destabilize the electroweak vacuum. We can stabilize the Higgs by introducing an effective mass term in the form of Higgs-inflaton and/or gravitational couplings.', 'However, the stability during and after inflation is typically in a trade-off relation. It is important to study the condition that ensures stability throughout these epochs. https://t.co/yXRKN1rELr', 'In our work, we focus on (quasi) scale-invariant models with quartic potentials and non-minimal gravitational couplings.  (Why scale invariance?  1. It can explain the flatness of the inflaton potential consistently with observations.  2. It might explain the hierarchy problem.) https://t.co/PX66DX1WKU', 'Naively, scale invariance implies unimpeded growth of resonance and hence unavoidable destabilization of the vacuum, which is a cosmological catastrophe. However, we find this is not the case taking into account the perturbative Higgs decay and backreaction of produced particles. https://t.co/onEH7axRcP', 'We find nontrivial and dynamical interplays between the effects of quartic and curvature couplings, which can partially cancel each other in some cases. We finally find disjoint ""islands of (meta)stability"" in the couplings parameter space. https://t.co/7SnuXWK6NK']",21,05,1537
265,247,1434905466656727046,794303906901880840,Ozan Oktay,"Check out our recent study exploring why data quality matters -- its impact on (I) model's predictive performance, (II) model evaluation and (III) selection for deployment purposes. Here we explore resource-effective solutions to tackle these challenges: <LINK> <LINK> <LINK>",https://arxiv.org/abs/2109.00574,"Imperfections in data annotation, known as label noise, are detrimental to the training of machine learning models and have an often-overlooked confounding effect on the assessment of model performance. Nevertheless, employing experts to remove label noise by fully re-annotating large datasets is infeasible in resource-constrained settings, such as healthcare. This work advocates for a data-driven approach to prioritising samples for re-annotation - which we term ""active label cleaning"". We propose to rank instances according to estimated label correctness and labelling difficulty of each sample, and introduce a simulation framework to evaluate relabelling efficacy. Our experiments on natural images and on a new medical imaging benchmark show that cleaning noisy labels mitigates their negative impact on model training, evaluation, and selection. Crucially, the proposed active label cleaning enables correcting labels up to 4 times more effectively than typical random selection in realistic conditions, making better use of experts' valuable time for improving dataset quality. ","Active label cleaning for improved dataset quality under resource
  constraints",1,"[""Check out our recent study exploring why data quality matters -- its impact on (I) model's predictive performance, (II) model evaluation and (III) selection for deployment purposes.\n\nHere we explore resource-effective solutions to tackle these challenges: <LINK> <LINK> <LINK>""]",21,09,275
266,102,1325811929475129345,2766925212,Andrew Childs,"New paper with @JinPengLiu__Sky, Kolden, Krovi, Loureiro, and Trivisa gives an efficient quantum for nonlinear differential equations with strong enough dissipation, shows hardness for weak dissipation. [1/2] <LINK> Leyton & Osborne gave a quantum algorithm for nonlinear diff eqs 10+ years ago (<LINK>). Their cost is exponential in evolution time. We improve this to polynomial with enough dissipation and rule out improvement for weak dissipation. [2/2]",http://arxiv.org/abs/2011.03185,"Nonlinear differential equations model diverse phenomena but are notoriously difficult to solve. While there has been extensive previous work on efficient quantum algorithms for linear differential equations, the linearity of quantum mechanics has limited analogous progress for the nonlinear case. Despite this obstacle, we develop a quantum algorithm for dissipative quadratic $n$-dimensional ordinary differential equations. Assuming $R < 1$, where $R$ is a parameter characterizing the ratio of the nonlinearity and forcing to the linear dissipation, this algorithm has complexity $T^2 q~\mathrm{poly}(\log T, \log n, \log 1/\epsilon)/\epsilon$, where $T$ is the evolution time, $\epsilon$ is the allowed error, and $q$ measures decay of the solution. This is an exponential improvement over the best previous quantum algorithms, whose complexity is exponential in $T$. While exponential decay precludes efficiency, driven equations can avoid this issue despite the presence of dissipation. Our algorithm uses the method of Carleman linearization, for which we give a novel convergence theorem. This method maps a system of nonlinear differential equations to an infinite-dimensional system of linear differential equations, which we discretize, truncate, and solve using the forward Euler method and the quantum linear system algorithm. We also provide a lower bound on the worst-case complexity of quantum algorithms for general quadratic differential equations, showing that the problem is intractable for $R \ge \sqrt{2}$. Finally, we discuss potential applications, showing that the $R < 1$ condition can be satisfied in realistic epidemiological models and giving numerical evidence that the method may describe a model of fluid dynamics even for larger values of $R$. ","Efficient quantum algorithm for dissipative nonlinear differential
  equations",2,"['New paper with @JinPengLiu__Sky, Kolden, Krovi, Loureiro, and Trivisa gives an efficient quantum for nonlinear differential equations with strong enough dissipation, shows hardness for weak dissipation. [1/2] <LINK>', 'Leyton &amp; Osborne gave a quantum algorithm for nonlinear diff eqs 10+ years ago (https://t.co/gYT0x3aZNT). Their cost is exponential in evolution time. We improve this to polynomial with enough dissipation and rule out improvement for weak dissipation. [2/2]']",20,11,456
267,39,1442522254051405829,1116002690604130305,Juliette Becker,"New on arXiv last night (and accepted to AJ last week): undergraduate Lucas Brefka’s first first-author paper: <LINK> In this paper, he studied how multi-planet systems change as their stars evolve and secular resonances sweep through the systems. In this work started as part of the @UROPumich program, he used a combination of secular theory and numerical simulations to show that for systems with ultra-short-period planets and extra outer planets, this dynamical process can recreate the observed geometry of the system. Lucas went from first-day @UROPumich freshman to published author in less than two years. Lucas is applying to grad school this fall, even though he is only a third-year undergrad (graduating a year early). If you’re looking for grad students this fall, watch for his application!",https://arxiv.org/abs/2109.12054,"Ultra-short period (USP) planets are exoplanets which have orbital periods of less than one day and are unique because they orbit inside the nominal magnetic truncation gap of their host stars. In some cases, USP planets have also been observed to exhibit unique dynamical parameters such as significant misalignments in inclination angle with respect to nearby planets. In this paper, we explore how the geometry of a multi-planet system hosting a USP planet can be expected to evolve as a star ages. In particular, we explore the relationship between the mutual inclination of the USP planet and the quadrupole moment ($J_2$) of the host star. We use secular perturbation theory to predict the past evolution of the example TOI-125 system, and then confirm the validity of our results using long-term N-body simulations. Through investigating how the misalignment between the candidate USP planet and the three other short-period planets in the TOI-125 system arose, we intend to derive a better understanding of the population of systems with misaligned USP planets and how their observed parameters can be explained in the context of their dynamical histories. ","A General Origin for Multi-Planetary Systems With Significantly
  Misaligned USP Planets",3,"['New on arXiv last night (and accepted to AJ last week): undergraduate Lucas Brefka’s first first-author paper: <LINK> In this paper, he studied how multi-planet systems change as their stars evolve and secular resonances sweep through the systems.', 'In this work started as part of the @UROPumich  program, he used a combination of secular theory and numerical simulations to show that for systems with ultra-short-period planets and extra outer planets, this dynamical process can recreate the observed geometry of the system.', 'Lucas went from first-day @UROPumich freshman to published author in less than two years. Lucas is applying to grad school this fall, even though he is only a third-year undergrad (graduating a year early). If you’re looking for grad students this fall, watch for his application!']",21,09,805
268,237,1435152181817188355,2917879835,Claudia Cicone,"When we first looked at the data, fresh out of the archive, we almost could not believe it. We suspected this quasar hosted extended CO-emitting gas beyond the ISM, but we did not expect to find such a giant molecular gas halo: <LINK> @almaobs @ESO_Chile @EsoARC The molecular halo, detected in CO(3-2), extends out to a radius of ~200 kpc from the quasar position, the largest molecular CGM ever imaged. We do not know how it originated, but we think powerful AGN-driven outflows may have helped seeding metal enriched gas out to large scales Understanding this kind of structures requires a new, large sub-mm single dish such as AtLAST, and new high-res cosmological simulations capable of tracing cold molecular gas on large scales (both are goals of @atlast_design) This work is part of the SUPER project <LINK>, a huge collaborative effort led by @VincenzoMainie2 @ESO with the help of @DARSHANKAKKAD Chiara Circosta and many more amazing collaborators",https://arxiv.org/abs/2109.02269,"We present the discovery of copious molecular gas in the halo of cid_346, a $z=2.2$ quasar studied as part of the SINFONI survey for Unveiling the Physics and Effect of Radiative feedback (SUPER). New Atacama Compact Array (ACA) CO(3-2) observations detect a much higher flux (by a factor of $14\pm5$) than measured on kiloparsec scales ($r\lesssim8$ kpc) using previous snapshot Atacama Large Millimeter/submillimeter Array data. Such additional CO(3-2) emission traces a structure that extends out to $r\sim200$ kpc in projected size, as inferred through direct imaging and confirmed by an analysis of the uv visibilities. This is the most extended molecular circumgalactic medium (CGM) reservoir that has ever been mapped. It shows complex kinematics, with an overall broad line profile (FWHM $= 1000$ km/s) that is skewed towards redshifted velocities up to at least $v\sim1000$ km/s. Using the optically thin assumption, we estimate a strict lower limit for the total molecular CGM mass observed by ACA of $M_{mol}^{CGM}>10^{10}~M_{\odot}$. There is however room for up to $M^{CGM}_{mol}\sim 1.7\times 10^{12}$ $M_{\odot}$, once optically thick CO emission with $\alpha_{\rm CO}=3.6$ $\rm M_{\odot}~(K~km~s^{-1}~pc^2)^{-1}$ and $L^{\prime}_{CO(3-2)}/L^{\prime}_{CO(1-0)}=0.5$ are assumed. Since cid_346 hosts quasar-driven ionised outflows and since there is no evidence of merging companions or an overdensity, we suggest that outflows may have played a crucial rule in seeding metal-enriched, dense gas on halo scales. However, the origin of such an extended molecular CGM remains unclear. ",SUPER. VI. A giant molecular halo around a z~2 quasar,4,"['When we first looked at the data, fresh out of the archive, we almost could not believe it. We suspected this quasar hosted extended CO-emitting gas beyond the ISM, but we did not expect to find such a giant molecular gas halo: <LINK> @almaobs @ESO_Chile @EsoARC', 'The molecular halo, detected in CO(3-2), extends out to a radius of ~200 kpc from the quasar position, the largest molecular CGM ever imaged. We do not know how it originated, but we think powerful AGN-driven outflows may have helped seeding metal enriched gas out to large scales', 'Understanding this kind of structures requires a new, large sub-mm single dish such as AtLAST, and new high-res cosmological simulations capable of tracing cold molecular gas on large scales (both are goals of @atlast_design)', 'This work is part of the SUPER project https://t.co/3COkwBVjil, a huge collaborative effort led by @VincenzoMainie2 @ESO with the help of @DARSHANKAKKAD Chiara Circosta and many more amazing collaborators']",21,09,957
269,140,1126332235215294464,4475055297,Ming-Yu Liu,"Check out our new #GAN work on translating images to unseen domains in the test time with few example images. Live demo <LINK> Project page <LINK> Paper <LINK> Video <LINK> #NVIDIA <LINK> Brought to you by @xunhuang1995 @arunmallya #TeroKarras, #TimoAila of #StyleGAN, @jaakkolehtinen, and @jankautz @NvidiaAI The web demo might be buggy. I know nothing about Javascript until last week. So please read the instruction carefully for run the demo. It currently works only on Chrome and Firefox and you have to click ""Load unsafe scripts"" or ""Disable protection for now"" buttons. Check out our paper for more results including translating all kinds of foods to Chowmein. <LINK> PetSwap demo video <LINK> live demo available at <LINK> It works for non standard pet too. <LINK> The PetSwap model is trained using carnivorous animals. It might be funny when you input images of other kinds of animals. <LINK> @chris_j_beckham Man. I am just having fun. :)",https://arxiv.org/abs/1905.01723,"Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at this https URL . ",Few-Shot Unsupervised Image-to-Image Translation,8,"['Check out our new #GAN work on translating images to unseen domains in the test time with few example images.\nLive demo <LINK>\nProject page <LINK>\nPaper <LINK>\nVideo <LINK>\n#NVIDIA <LINK>', 'Brought to you by @xunhuang1995  @arunmallya #TeroKarras, #TimoAila of #StyleGAN, @jaakkolehtinen, and @jankautz @NvidiaAI', 'The web demo might be buggy. I know nothing about Javascript until last week. So please read the instruction carefully for run the demo. It currently works only on Chrome and Firefox and you have to click  ""Load unsafe scripts"" or  ""Disable protection for now"" buttons.', 'Check out our paper for more results including translating all kinds of foods to Chowmein. https://t.co/iYwv3wh5Ts', 'PetSwap demo video\nhttps://t.co/CRAoLundVy\n\nlive demo available at https://t.co/KeYHIDpgcx', 'It works for non standard pet too. https://t.co/GcYOPA3Oix', 'The PetSwap model is trained using carnivorous animals. It might be funny when you input images of other kinds of animals. https://t.co/0Lq1BVuJUa', '@chris_j_beckham Man. I am just having fun. :)']",19,05,950
270,194,1288534528030576640,1282643620059897856,Lukasz Cincio,"Our first 2020 Quantum Summer School paper is out! Congrats @samson_wang, @EnricoFontana19, @MvsCerezo, @kunal_phy , @SoneAkira, @ColesQuantum We study how noise affects variational algorithms. Noise will make your deep ansatz untrainable. Details here: <LINK>",https://arxiv.org/abs/2007.14384,"Variational Quantum Algorithms (VQAs) may be a path to quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) computers. A natural question is whether noise on NISQ devices places fundamental limitations on VQA performance. We rigorously prove a serious limitation for noisy VQAs, in that the noise causes the training landscape to have a barren plateau (i.e., vanishing gradient). Specifically, for the local Pauli noise considered, we prove that the gradient vanishes exponentially in the number of qubits $n$ if the depth of the ansatz grows linearly with $n$. These noise-induced barren plateaus (NIBPs) are conceptually different from noise-free barren plateaus, which are linked to random parameter initialization. Our result is formulated for a generic ansatz that includes as special cases the Quantum Alternating Operator Ansatz and the Unitary Coupled Cluster Ansatz, among others. For the former, our numerical heuristics demonstrate the NIBP phenomenon for a realistic hardware noise model. ",Noise-Induced Barren Plateaus in Variational Quantum Algorithms,1,"['Our first 2020 Quantum Summer School paper is out!\nCongrats \n@samson_wang, @EnricoFontana19, @MvsCerezo, @kunal_phy , @SoneAkira, @ColesQuantum\nWe study how noise affects variational algorithms. Noise will make your deep ansatz untrainable. Details here:\n<LINK>']",20,07,260
271,184,1337133680670756865,4350016234,Dr Fleur Meddens,"Are you the firstborn child in your family? You're likely to have higher educational attainment (EA) than your younger siblings. In our new GxE preprint, we find that you only benefit from being firstborn if you have a higher EA polygenic score! See <LINK> Congrats to @DilyaMuslimova on the first chapter of her Phd thesis! 🎉🎉 In this paper, we are able to extract exogenous information in both G and E by (a) using N = 15,019 full siblings of the @uk_biobank and (b) using birth order, which is unrelated to EA PGS within families. Hence, both G and E can be considered random shocks in this setting. On average, firstborns complete ~4.5 months more schooling than their younger sibs. Firstborns with an EA PGS of +1 SD enjoy an *additional* ~1.9 months of schooling. Firstborns with below average EA PGS do not benefit from being firstborn at all. Why is the case? Firstly, firstborns enjoy their parents' undivided attention until the birth of younger siblings. Other research shows that firstborns receive 20-30 minutes more daily quality time compared to their younger siblings! We are not able to test this, however. Secondly, we think kids with above average EA PGS benefit disproportionally more from this supposed time investment because of ""dynamic complementarity"" in skill formation, where subsequent ""investments"" pay off more for kids with higher endowments. Please note that this a pre-print, so any comments and suggestions are especially welcome! @Scientific_Bird We definitely did not consider that, thank you! @LEdeRuiter We only considered families with siblings in our within fam analyses, but that’s definitely something we could check out! @jessicabakerphd We only considered families with siblings in our within fam analyses, but that’s definitely something we could check out! @sihamsikander I think we tried that, @DilyaMuslimova ? @MOEENRIAZ Great question for @DilyaMuslimova, more details in section D of the paper. This was a bit tricky, because the age gaps may not be constant within one family (different age gaps between sibs).",https://arxiv.org/abs/2012.05021,"The birth order literature emphasizes the role of parental investments in explaining why firstborns have higher human capital outcomes than their laterborn siblings. We use birth order as a proxy for investments and interact it with genetic endowments. Exploiting only within-family variation in both ensures they are exogenous as well as orthogonal to each other. As such, our setting is informative about the existence of dynamic complementarity in skill production. Our empirical analysis exploits data from 15,019 full siblings in the UK Biobank. We adopt a family-fixed effects strategy combined with instrumental variables to deal with endogeneity issues arising from omitted variables and measurement error. We find that birth order and genetic endowments interact: those with above-average genetic endowments benefit disproportionally more from being firstborn compared to those with below-average genetic endowments. This finding is a clean example of how genetic endowments (nature) and the environment (nurture) interact in producing educational attainment. Moreover, our results are consistent with the existence of dynamic complementarity in skill formation: additional parental investments associated with being firstborn are more effective for those siblings who randomly inherited higher genetic endowments for educational attainment. ","Dynamic complementarity in skill production: Evidence from genetic
  endowments and birth order",12,"[""Are you the firstborn child in your family? You're likely to have higher educational attainment (EA) than your younger siblings. In our new GxE preprint, we find that you only benefit from being firstborn if you have a higher EA polygenic score! See <LINK>"", 'Congrats to @DilyaMuslimova on the first chapter of her Phd thesis! 🎉🎉', 'In this paper, we are able to extract exogenous information in both G and E by (a) using N = 15,019 full siblings of the @uk_biobank and (b) using birth order, which is unrelated to EA PGS within families. Hence, both G and E can be considered random shocks in this setting.', 'On average, firstborns complete ~4.5 months more schooling than their younger sibs. Firstborns with an EA PGS of +1 SD enjoy an *additional* ~1.9 months of schooling. Firstborns with below average EA PGS do not benefit from being firstborn at all.', ""Why is the case? Firstly, firstborns enjoy their parents' undivided attention until the birth of younger siblings. Other research shows that firstborns receive 20-30 minutes more daily quality time compared to their younger siblings! We are not able to test this, however."", 'Secondly, we think kids with above average EA PGS benefit disproportionally more from this supposed time investment because of ""dynamic complementarity"" in skill formation, where subsequent ""investments"" pay off more for kids with higher endowments.', 'Please note that this a pre-print, so any comments and suggestions are especially welcome!', '@Scientific_Bird We definitely did not consider that, thank you!', '@LEdeRuiter We only considered families with siblings in our within fam analyses, but that’s definitely something we could check out!', '@jessicabakerphd We only considered families with siblings in our within fam analyses, but that’s definitely something we could check out!', '@sihamsikander I think we tried that, @DilyaMuslimova ?', '@MOEENRIAZ Great question for @DilyaMuslimova, more details in section D of the paper. This was a bit tricky, because the age gaps may not be constant within one family (different age gaps between sibs).']",20,12,2062
272,138,1412105767545716742,1556664198,Kyle Cranmer,"New paper, it's a beast! Huge effort led by Dan Hackett. We investigate different ways of training flows (reverse & forwards KL) for multi-modal distributions (eg. scalar field theory), combining them with MCMC samplers, & pros/cons of performance metrics <LINK> <LINK>",https://arxiv.org/abs/2107.00734,"Recent results have demonstrated that samplers constructed with flow-based generative models are a promising new approach for configuration generation in lattice field theory. In this paper, we present a set of methods to construct flow models for targets with multiple separated modes (i.e. theories with multiple vacua). We demonstrate the application of these methods to modeling two-dimensional real scalar field theory in its symmetry-broken phase. In this context we investigate the performance of different flow-based sampling algorithms, including a composite sampling algorithm where flow-based proposals are occasionally augmented by applying updates using traditional algorithms like HMC. ",Flow-based sampling for multimodal distributions in lattice field theory,1,"[""New paper, it's a beast! Huge effort led by Dan Hackett. We investigate different ways of training flows (reverse &amp; forwards KL) for multi-modal distributions (eg. scalar field theory), combining them with MCMC samplers, &amp; pros/cons of performance metrics\n<LINK> <LINK>""]",21,07,269
273,75,1294100692525694983,2416760538,Peter Gao,"New co-authored paper! Led by Yayaati Chachan, superstar grad student @Caltech, we add another super-puff to the Flat Transmission Spectrum Club [tm]. Along with the super-puffs in the Kepler-51 system, are we beginning to see a trend? <LINK> As a reminder, Xi Zhang and I predicted that some super-puffs may actually be ""normal"" sub-Neptunes with high altitude hazes that make them appear bigger than they really are: <LINK>  Kep-51bd and now Kep-79d fit this idea. What about the others?",https://arxiv.org/abs/2008.05480,"Extremely low density planets ('super-puffs') are a small but intriguing subset of the transiting planet population. With masses in the super-Earth range ($1-10$ M$_{\oplus}$) and radii akin to those of giant planets ($>4$ R$_{\oplus}$), their large envelopes may have been accreted beyond the water snow line and many appear to be susceptible to catastrophic mass loss. Both the presence of water and the importance of mass loss can be explored using transmission spectroscopy. Here, we present new HST WFC3 spectroscopy and updated Kepler transit depth measurements for the super-puff Kepler-79d. We do not detect any molecular absorption features in the $1.1-1.7$ $\mu$m WFC3 bandpass and the combination of Kepler and WFC3 data are consistent with a flat line model, indicating the presence of aerosols in the atmosphere. We compare the shape of Kepler-79d's transmission spectrum to predictions from a microphysical haze model that incorporates an outward particle flux due to ongoing mass loss. We find that photochemical hazes offer an attractive explanation for the observed properties of super-puffs like Kepler-79d, as they simultaneously render the near-infrared spectrum featureless and reduce the inferred envelope mass loss rate by moving the measured radius (optical depth unity surface during transit) to lower pressures. We revisit the broader question of mass loss rates for super-puffs and find that the age estimates and mass loss rates for the majority of super-puffs can be reconciled if hazes move the photosphere from the typically assumed pressure of $\sim 10$ mbar to $\sim 10 \; \mu$bar. ","A Featureless Infrared Transmission Spectrum for the Super-Puff Planet
  Kepler-79d",2,"['New co-authored paper! Led by Yayaati Chachan, superstar grad student @Caltech, we add another super-puff to the Flat Transmission Spectrum Club [tm]. Along with the super-puffs in the Kepler-51 system, are we beginning to see a trend? <LINK>', 'As a reminder, Xi Zhang and I predicted that some super-puffs may actually be ""normal"" sub-Neptunes with high altitude hazes that make them appear bigger than they really are: https://t.co/H4vifgXB2V \n\nKep-51bd and now Kep-79d fit this idea. What about the others?']",20,08,489
274,71,1493324906443902976,60893773,James Bullock,"A quick summary of thoughts on our new FIRE et al. paper showing that dark-matter-free low-mass galaxies arise naturally and fairly frequently around massive galaxies in a cosmological-volume simulation. Paper led by @jorgito__moreno using FIREbox sim <LINK> <LINK> van Dokkum and @DanieliShany discovery of low-mass dm-poor galaxies DF2 and DF4 (red bars) v. surprising, since low-mass galaxies are usually DM-dominated. Remarkably we find several low-mass galaxies (yellow) in these sim have less DM than stars within their stellar radii. <LINK> Every one of them is a satellite of a massive (~1.e11 Mstar) host that is on an orbit that brought it within the core of the galaxy (~10 kpc from the center). Much more DM lost than stars. <LINK> Most of them have very faint tidal features <LINK> This work follows a long line of work that has shown that close encounters between low-mass galaxies and massive hosts could do this kind of thing: Haslbauer et al., Carleton et al., Sales et al., etc. Our sims do a very good job reproducing many properties of DF2 and DF4: velocity dispersion, sizes, etc. We predict that ~30% of massive hosts should have a satellite that is DM deficient. One thing that still concerns me about our work is that we find our objects are still fairly metal rich compared to DF2 and DF4. Could be scatter in Fe/H vs. Mstar -- will need to discover more DM-def. galaxies to test these things! @azifattahi @jorgito__moreno Yes, basically. Here is relevant section of table. <LINK>",https://arxiv.org/abs/2202.05836,"The standard cold dark matter plus cosmological constant model predicts that galaxies form within dark-matter haloes, and that low-mass galaxies are more dark-matter dominated than massive ones. The unexpected discovery of two low-mass galaxies lacking dark matter immediately provoked concerns about the standard cosmology and ignited explorations of alternatives, including self-interacting dark matter and modified gravity. Apprehension grew after several cosmological simulations using the conventional model failed to form adequate numerical analogues with comparable internal characteristics (stellar masses, sizes, velocity dispersions and morphologies). Here we show that the standard paradigm naturally produces galaxies lacking dark matter with internal characteristics in agreement with observations. Using a state-of-the-art cosmological simulation and a meticulous galaxy-identification technique, we find that extreme close encounters with massive neighbours can be responsible for this. We predict that approximately 30 percent of massive central galaxies (with at least 1e11 solar masses in stars) harbour at least one dark-matter-deficient satellite (with 1e8 - 1e9 solar masses in stars). This distinctive class of galaxies provides an additional layer in our understanding of the role of interactions in shaping galactic properties. Future observations surveying galaxies in the aforementioned regime will provide a crucial test of this scenario. ","Galaxies lacking dark matter produced by close encounters in a
  cosmological simulation",8,"['A quick summary of thoughts on our new FIRE et al. paper showing that dark-matter-free low-mass galaxies  arise naturally and fairly frequently around massive galaxies in a cosmological-volume simulation. Paper led by @jorgito__moreno using FIREbox sim\n<LINK> <LINK>', 'van Dokkum and @DanieliShany discovery of low-mass dm-poor galaxies DF2 and DF4 (red bars) v. surprising, since low-mass galaxies are usually DM-dominated.  Remarkably we find several low-mass galaxies (yellow) in these sim have less DM than stars within their stellar radii. https://t.co/h0FF88if6W', 'Every one of them is a satellite of a massive (~1.e11 Mstar) host that is on an orbit that brought it within the core of the galaxy (~10 kpc from the center).  Much more DM lost than stars. https://t.co/P6vyzZsBxy', 'Most of them have very faint tidal features https://t.co/8zlzdGkiKX', 'This work follows a long line of work that has shown that close encounters between low-mass galaxies and massive hosts could do this kind of thing:  Haslbauer et al., Carleton et al., Sales et al., etc.', 'Our sims do a very good job reproducing many properties of DF2 and DF4: velocity dispersion, sizes, etc.  We predict that ~30% of massive hosts should have a satellite that is DM deficient.', 'One thing that still concerns me about our work is that we find our objects are still fairly metal rich compared to DF2 and DF4.  Could be scatter in Fe/H vs. Mstar -- will need to discover more DM-def. galaxies to test these things!', '@azifattahi @jorgito__moreno Yes, basically.  Here is relevant section of table. https://t.co/1TLuruwFFc']",22,02,1505
275,30,1443195708614475776,21859920,Ben Rubinstein,"🎺 new work with @NGMarchant @ScottAlfeld on machine *un*learning 🤖 and an unusual kind of adversarial learning attack that increases compute time 😴. ""Hard to Forget: Poisoning Attacks on Certified Machine Unlearning"". Paper link: <LINK> A short 🧵⬇️ 1/ <LINK> The right to erasure is enshrined in legislation (GDPR) ⚖️. While the FTC this year ruled that an app developer had to not only remove photos from its possession, but also any downstream algorithms/models trained on them 🗑️. <LINK> 2/ When consent is withdrawn from previous shared data, an organisation may be faced with the task of retraining machine learning models 🤖. Some of these models are big and expense to train 💸⌛️. <LINK> What to do? 3/ Enter ""machine unlearning"": learn an approx model similar to an original model (utility! 😎), updatable with minimal fuss (speed! 🏎️), can remove a requested training point - update is *certifiably* indistinguishable from retrained model (privacy! 🕵️) <LINK> 4/ Not so fast! We find a problem: privacy-preserving noise accumulates in the unlearned model over time, can mean full retraining 😓. While still a net time savings in ideal settings, we find strategically chosen points can poison unlearning, forcing frequent retrainings ⌛️. 5/ What does this mean? We can still have privacy through retraining (yay we love privacy!), but this might come at more compute cost than previously thought. 6/6",https://arxiv.org/abs/2109.08266,"The right to erasure requires removal of a user's information from data held by organizations, with rigorous interpretations extending to downstream products such as learned models. Retraining from scratch with the particular user's data omitted fully removes its influence on the resulting model, but comes with a high computational cost. Machine ""unlearning"" mitigates the cost incurred by full retraining: instead, models are updated incrementally, possibly only requiring retraining when approximation errors accumulate. Rapid progress has been made towards privacy guarantees on the indistinguishability of unlearned and retrained models, but current formalisms do not place practical bounds on computation. In this paper we demonstrate how an attacker can exploit this oversight, highlighting a novel attack surface introduced by machine unlearning. We consider an attacker aiming to increase the computational cost of data removal. We derive and empirically investigate a poisoning attack on certified machine unlearning where strategically designed training data triggers complete retraining when removed. ",Hard to Forget: Poisoning Attacks on Certified Machine Unlearning,6,"['🎺 new work with @NGMarchant @ScottAlfeld on machine *un*learning 🤖 and an unusual kind of adversarial learning attack that increases compute time 😴. ""Hard to Forget: Poisoning Attacks on Certified Machine Unlearning"". Paper link: <LINK> A short 🧵⬇️ 1/ <LINK>', 'The right to erasure is enshrined in legislation (GDPR) ⚖️. While the FTC this year ruled that an app developer had to not only remove photos from its possession, but also any downstream algorithms/models trained on them 🗑️. https://t.co/PAJsJkNVBd 2/', 'When consent is withdrawn from previous shared data, an organisation may be faced with the task of retraining machine learning models 🤖. Some of these models are big and expense to train 💸⌛️. https://t.co/EEY9oKbzrb What to do? 3/', 'Enter ""machine unlearning"": learn an approx model similar to an original model (utility! 😎), updatable with minimal fuss (speed! 🏎️), can remove a requested training point - update is *certifiably* indistinguishable from retrained model (privacy! 🕵️) https://t.co/sMsc104rqP 4/', 'Not so fast! We find a problem: privacy-preserving noise accumulates in the unlearned model over time, can mean full retraining 😓. While still a net time savings in ideal settings, we find strategically chosen points can poison unlearning, forcing frequent retrainings ⌛️. 5/', 'What does this mean? We can still have privacy through retraining (yay we love privacy!), but this might come at more compute cost than previously thought. 6/6']",21,09,1404
