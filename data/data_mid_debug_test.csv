,Unnamed: 0.1,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title,Thread_length,Tweets_coarse,year,month,tweet_length
0,68,1383068160329449481,1215310334,Timo Schick,"🎉 New paper 🎉 In ""Generating Datasets with Pretrained Language Models"", we introduce DINO🦕 and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models #NLProc 📄 Paper: <LINK> <LINK>",https://arxiv.org/abs/2104.07540,"To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets. ",Generating Datasets with Pretrained Language Models,1,"['🎉 New paper 🎉 In ""Generating Datasets with Pretrained Language Models"", we introduce DINO🦕 and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models #NLProc\n\n📄 Paper: <LINK> <LINK>']",21,04,265
1,12,900758384122748928,2180768821,Erik Hoel,"What is causation? How do you measure it? What causes what? I’m on a new paper that just went up on arXiv about this <LINK> this is truly excellent work by my colleagues Larissa Albantakis, William Marshall, and Giulio Tononi @ChristianHoney_ @nattyover In IIT, information is integrated by the system itself. But the observer can measure this using information theory and causal analysis @ChristianHoney_ @nattyover There purposefully is no observer in the theory, unlike in traditional information theory. An observer can measure it but does not create it @ChristianHoney_ @nattyover Sure thing - DM me or email me at hoelerik at gmail",https://arxiv.org/abs/1708.06716,"Actual causation is concerned with the question ""what caused what?"" Consider a transition between two states within a system of interacting elements, such as an artificial neural network, or a biological brain circuit. Which combination of synapses caused the neuron to fire? Which image features caused the classifier to misinterpret the picture? Even detailed knowledge of the system's causal network, its elements, their states, connectivity, and dynamics does not automatically provide a straightforward answer to the ""what caused what?"" question. Counterfactual accounts of actual causation based on graphical models, paired with system interventions, have demonstrated initial success in addressing specific problem cases in line with intuitive causal judgments. Here, we start from a set of basic requirements for causation (realization, composition, information, integration, and exclusion) and develop a rigorous, quantitative account of actual causation that is generally applicable to discrete dynamical systems. We present a formal framework to evaluate these causal requirements that is based on system interventions and partitions, and considers all counterfactuals of a state transition. This framework is used to provide a complete causal account of the transition by identifying and quantifying the strength of all actual causes and effects linking the two consecutive system states. Finally, we examine several exemplary cases and paradoxes of causation and show that they can be illuminated by the proposed framework for quantifying actual causation. ","What caused what? A quantitative account of actual causation using
  dynamical causal networks",5,"['What is causation? How do you measure it? What causes what? I’m on a new paper that just went up on arXiv about this <LINK>', 'this is truly excellent work by my colleagues Larissa Albantakis, William Marshall, and Giulio Tononi', '@ChristianHoney_ @nattyover In IIT, information is integrated by the system itself. But the observer can measure this using information theory and causal analysis', '@ChristianHoney_ @nattyover There purposefully is no observer in the theory, unlike in traditional information theory. An observer can measure it but does not create it', '@ChristianHoney_ @nattyover Sure thing - DM me or email me at hoelerik at gmail']",17,08,637
2,80,963983121027878912,19510090,Julian Togelius,"Who killed Albert Einstein? That's up to you to find out. @GabbBarros @Bumblebor @SentientDesigns and I proudly present our new paper on generating murder mystery games from open data. <LINK> <LINK> It works like this: you input the name of a person with a Wikipedia profile, and the system creates a whodunnit for you to solve. This involves a lot of crawling @Wikipedia , @WikiCommons and @openstreetmap, evolutionary algorithms and a bunch of other tricks. The paper includes a ""blooper reel"" of sorts, as you would not believe the kind of weird stuff you can find on Wikipedia and the ways in which combining innocuous data can lead to troublesome content. @FlorianHollandt @Wikipedia @WikiCommons @openstreetmap Thanks! Not yet, we're working on a new version which we hope will be stable enough to release publicly this spring. @samim @GabbBarros @Bumblebor @SentientDesigns Thanks! The current version will not be available, but we are working on a new version which will be public this spring.",https://arxiv.org/abs/1802.05219,"This paper presents a framework for generating adventure games from open data. Focusing on the murder mystery type of adventure games, the generator is able to transform open data from Wikipedia articles, OpenStreetMap and images from Wikimedia Commons into WikiMysteries. Every WikiMystery game revolves around the murder of a person with a Wikipedia article and populates the game with suspects who must be arrested by the player if guilty of the murder or absolved if innocent. Starting from only one person as the victim, an extensive generative pipeline finds suspects, their alibis, and paths connecting them from open data, transforms open data into cities, buildings, non-player characters, locks and keys and dialog options. The paper describes in detail each generative step, provides a specific playthrough of one WikiMystery where Albert Einstein is murdered, and evaluates the outcomes of games generated for the 100 most influential people of the 20th century. ",Who Killed Albert Einstein? From Open Data to Murder Mystery Games,5,"[""Who killed Albert Einstein? That's up to you to find out. @GabbBarros @Bumblebor @SentientDesigns and I proudly present our new paper on generating murder mystery games from open data.\n<LINK> <LINK>"", 'It works like this: you input the name of a person with a Wikipedia profile, and the system creates a whodunnit for you to solve. This involves a lot of crawling @Wikipedia , @WikiCommons and @openstreetmap, evolutionary algorithms and a bunch of other tricks.', 'The paper includes a ""blooper reel"" of sorts, as you would not believe  the kind of weird stuff you can find on Wikipedia and the ways in which  combining innocuous data can lead to troublesome content.', ""@FlorianHollandt @Wikipedia @WikiCommons @openstreetmap Thanks! Not yet, we're working on a new version which we hope will be stable enough to release publicly this spring."", '@samim @GabbBarros @Bumblebor @SentientDesigns Thanks! The current version will not be available, but we are working on a new version which will be public this spring.']",18,02,1001
3,34,1376841955443929092,4878011,Matthias Rosenkranz ✨,"Our new paper ""Variational inference with a #quantum computer"" has been out for a few days 🎉. We develop the methods, then demonstrate them using a few graphical models (e.g. hidden Markov).  <LINK> #QuantumComputing #MachineLearning <LINK> Variational inference aims at finding an optimal, approximate posterior distribution of latent variables. This is useful for generative models or reasoning under uncertainty. Our main idea is to use a quantum-circuit Born machine as a flexible and expressive family of approximate posterior distributions. The forward model remains classical. <LINK> The Born machine only provides sample access so we develop two methods suitable for this situation. Both are inspired by advances in classical VI. The first one is based on an adversarial objective, the second one on the kernelized Stein discrepancy. <LINK> We demonstrate the methods on a simulator and an IBM quantum device. Essentially everything works. We hope to collect a few more shots from the real device for the next update. <LINK> Thank you to my collaborators Marcello Benedetti, @BrianC2095, @matt_fiorentini and Michal Lubasch, all at @cambridgecqc We also provide a more high-level introduction to the ideas in this blog post <LINK>",https://arxiv.org/abs/2103.06720,"Inference is the task of drawing conclusions about unobserved variables given observations of related variables. Applications range from identifying diseases from symptoms to classifying economic regimes from price movements. Unfortunately, performing exact inference is intractable in general. One alternative is variational inference, where a candidate probability distribution is optimized to approximate the posterior distribution over unobserved variables. For good approximations, a flexible and highly expressive candidate distribution is desirable. In this work, we use quantum Born machines as variational distributions over discrete variables. We apply the framework of operator variational inference to achieve this goal. In particular, we adopt two specific realizations: one with an adversarial objective and one based on the kernelized Stein discrepancy. We demonstrate the approach numerically using examples of Bayesian networks, and implement an experiment on an IBM quantum computer. Our techniques enable efficient variational inference with distributions beyond those that are efficiently representable on a classical computer. ",Variational inference with a quantum computer,7,"['Our new paper ""Variational inference with a #quantum computer"" has been out for a few days 🎉.\n\nWe develop the methods, then demonstrate them using a few graphical models (e.g. hidden Markov).\n \n<LINK>\n#QuantumComputing #MachineLearning <LINK>', 'Variational inference aims at finding an optimal, approximate posterior distribution of latent variables. This is useful for generative models or reasoning under uncertainty.', 'Our main idea is to use a quantum-circuit Born machine as a flexible and expressive family of approximate posterior distributions. The forward model remains classical. https://t.co/vCPpMQs76h', 'The Born machine only provides sample access so we develop two methods suitable for this situation. Both are inspired by advances in classical VI. The first one is based on an adversarial objective, the second one on the kernelized Stein discrepancy. https://t.co/V7tluShbbT', 'We demonstrate the methods on a simulator and an IBM quantum device. Essentially everything works. We hope to collect a few more shots from the real device for the next update. https://t.co/8uhCEgHgOw', 'Thank you to my collaborators Marcello Benedetti, @BrianC2095, @matt_fiorentini and Michal Lubasch, all at @cambridgecqc', 'We also provide a more high-level introduction to the ideas in this blog post\nhttps://t.co/TZYFz4tfLm']",21,03,1238
4,110,1503380960259178497,990478024188485633,Yukei Murakami,New paper on arxiv! We present lightcurves and analyses of 70 stripped-envelope supernovae. This is one of the first large analyses of the rise-time of such SNe. We discuss the implied ejecta mass and velocity through multi-band light curve modeling. <LINK>,https://arxiv.org/abs/2203.05596,"We present BVRI and unfiltered Clear light curves of 70 stripped-envelope supernovae (SESNe), observed between 2003 and 2020, from the Lick Observatory Supernova Search (LOSS) follow-up program. Our SESN sample consists of 19 spectroscopically normal SNe~Ib, two peculiar SNe Ib, six SN Ibn, 14 normal SNe Ic, one peculiar SN Ic, ten SNe Ic-BL, 15 SNe IIb, one ambiguous SN IIb/Ib/c, and two superluminous SNe. Our follow-up photometry has (on a per-SN basis) a mean coverage of 81 photometric points (median of 58 points) and a mean cadence of 3.6d (median of 1.2d). From our full sample, a subset of 38 SNe have pre-maximum coverage in at least one passband, allowing for the peak brightness of each SN in this subset to be quantitatively determined. We describe our data collection and processing techniques, with emphasis toward our automated photometry pipeline, from which we derive publicly available data products to enable and encourage further study by the community. Using these data products, we derive host-galaxy extinction values through the empirical colour evolution relationship and, for the first time, produce accurate rise-time measurements for a large sample of SESNe in both optical and infrared passbands. By modeling multiband light curves, we find that SNe Ic tend to have lower ejecta masses and lower ejecta velocities than SNe~Ib and IIb, but higher $^{56}$Ni masses. ","The Lick Observatory Supernova Search follow-up program: photometry data
  release of 70 stripped-envelope supernovae",1,['New paper on arxiv!\nWe present lightcurves and analyses of 70 stripped-envelope supernovae. This is one of the first large analyses of the rise-time of such SNe. We discuss the implied ejecta mass and velocity through multi-band light curve modeling. \n<LINK>'],22,03,257
5,30,1143488651579600896,3813580887,Paul Bürkner,"Pushing the limits of importance sampling to improve accuracy and efficiency of approximate Bayesian cross-validation. Our new paper (by @PaananenTopi, Juho Piironen, @avehtari, and me) is now on arXiv (<LINK>)! @bachlaw @PaananenTopi @avehtari Topi (@PaananenTopi) did most of this and I am very impressed by his work!",https://arxiv.org/abs/1906.08850,"Adaptive importance sampling is a class of techniques for finding good proposal distributions for importance sampling. Often the proposal distributions are standard probability distributions whose parameters are adapted based on the mismatch between the current proposal and a target distribution. In this work, we present an implicit adaptive importance sampling method that applies to complicated distributions which are not available in closed form. The method iteratively matches the moments of a set of Monte Carlo draws to weighted moments based on importance weights. We apply the method to Bayesian leave-one-out cross-validation and show that it performs better than many existing parametric adaptive importance sampling methods while being computationally inexpensive. ",Implicitly Adaptive Importance Sampling,2,"['Pushing the limits of importance sampling to improve accuracy and efficiency of approximate Bayesian cross-validation. Our new paper (by @PaananenTopi, Juho Piironen, @avehtari, and me) is now on arXiv (<LINK>)!', '@bachlaw @PaananenTopi @avehtari Topi (@PaananenTopi) did most of this and I am very impressed by his work!']",19,06,319
6,102,1448364157498060802,1331816112502104130,DeWeese Lab,"We’re thrilled to announce our new paper, in which we derive and test a ✨first-principles theory of generalization in deep learning!✨ We affectionately call it the Theory of Eigenlearning. 🧵⤵ <LINK> Recent breakthroughs have shown that ∞-width nets are actually very simple: their evolution’s described by a “neural tangent kernel” (NTK), and their final learned functions (when trained on MSE loss) are given by kernel regression with that NTK. (2/n) We show that wide net learning is easily understood in the eigenbasis of the NTK: the network more easily learns high-eigenvalue eigenfunctions, which typically correspond to low spatial freq, quantifying the well-known but vague notion of “spectral bias” to low freqs. (3/n) <LINK> We derive simple expressions for the generalization of NTK regression, then show that they accurately predict generalization *even in finite nets!* We focus on a simple new measure of generalization we call ""learnability""; in Fig, exp (dots) agrees v well w/theory (curves) (4/n) <LINK> We also prove a fundamental conservation law governing the inductive bias of wide nets: all NTKs have the same fixed budget of “learnability” that they must divvy up among their eigenmodes. Fig shows how the sum of these learnabilities is always the trainset size. (5/n) <LINK> This work was done by lab members Jamie Simon and Maddie Dickens. It builds on this pioneering work from @blake__bordelon, @canatar_a, and @CPehlevan, which you should also check out! <LINK> If you have any questions, drop us a tweet or an email! (7/7)",https://arxiv.org/abs/2110.03922,"Kernel regression is an important nonparametric learning algorithm with an equivalence to neural networks in the infinite-width limit. Understanding its generalization behavior is thus an important task for machine learning theory. In this work, we provide a theory of the inductive bias and generalization of kernel regression using a new measure characterizing the ""learnability"" of a given target function. We prove that a kernel's inductive bias can be characterized as a fixed budget of learnability, allocated to its eigenmodes, that can only be increased with the addition of more training data. We then use this rule to derive expressions for the mean and covariance of the predicted function and gain insight into the overfitting and adversarial robustness of kernel regression and the hardness of the classic parity problem. We show agreement between our theoretical results and both kernel regression and wide finite networks on real and synthetic learning tasks. ","A Theory of the Inductive Bias and Generalization of Kernel Regression
  and Wide Neural Networks",7,"['We’re thrilled to announce our new paper, in which we derive and test a ✨first-principles theory of generalization in deep learning!✨ We affectionately call it the Theory of Eigenlearning. 🧵⤵\n\n<LINK>', 'Recent breakthroughs have shown that ∞-width nets are actually very simple: their evolution’s described by a “neural tangent kernel” (NTK), and their final learned functions (when trained on MSE loss) are given by kernel regression with that NTK. (2/n)', 'We show that wide net learning is easily understood in the eigenbasis of the NTK: the network more easily learns high-eigenvalue eigenfunctions, which typically correspond to low spatial freq, quantifying the well-known but vague notion of “spectral bias” to low freqs. (3/n) https://t.co/Imki19Nzll', 'We derive simple expressions for the generalization of NTK regression, then show that they accurately predict generalization *even in finite nets!* We focus on a simple new measure of generalization we call ""learnability""; in Fig, exp (dots) agrees v well w/theory (curves) (4/n) https://t.co/zPP9omyrEb', 'We also prove a fundamental conservation law governing the inductive bias of wide nets: all NTKs have the same fixed budget of “learnability” that they must divvy up among their eigenmodes. Fig shows how the sum of these learnabilities is always the trainset size. (5/n) https://t.co/5U6T9Snpw6', 'This work was done by lab members Jamie Simon and Maddie Dickens. It builds on this pioneering work from @blake__bordelon, @canatar_a, and @CPehlevan, which you should also check out! https://t.co/QeDKdT6U7E', 'If you have any questions, drop us a tweet or an email! (7/7)']",21,10,1552
7,56,1329725647468572678,526872160,Pablo Moreno-Muñoz,"A novel application of the probabilistic ML methods that we develop is focused on mental health. Heterogeneous data, latent variables, change-point detection, all that is put into practice in our new paper at the #ML4MH Workshop @ NeurIPS 2020 paper: <LINK> <LINK> Fantastic collaboration with A. Moreno, L. Romero-Medrano and J. Herrera-López. Also thanks to @eBasedBehavior, @aar_2009 and E. Baca-García for the support.",http://arxiv.org/abs/2011.09848,"More than one million people commit suicide every year worldwide. The costs of daily cares, social stigma and treatment issues are still hard barriers to overcome in mental health. Most symptoms of mental disorders are related to the behavioral state of a patient, such as the mobility or social activity. Mobile-based technologies allow the passive collection of patients data, which supplements conventional assessments that rely on biased questionnaires and occasional medical appointments. In this work, we present a non-invasive machine learning (ML) model to detect behavioral shifts in psychiatric patients from unobtrusive data collected by a smartphone app. Our clinically validated results shed light on the idea of an early detection mobile tool for the task of suicide attempt prevention. ",Passive detection of behavioral shifts for suicide attempt prevention,2,"['A novel application of the probabilistic ML methods that we develop is focused on mental health.\n\nHeterogeneous data, latent variables, change-point detection, all that is put into practice in our new paper at the #ML4MH Workshop @ NeurIPS 2020\n\npaper: <LINK> <LINK>', 'Fantastic collaboration with A. Moreno, L. Romero-Medrano and J. Herrera-López. Also thanks to @eBasedBehavior, @aar_2009 and E. Baca-García for the support.']",20,11,422
8,7,1433455998086352899,1317208236257312770,Alyx Burns,"Providing metadata about a visualization might build trust or help readers understand what is shown. But how do you decide what to say without overwhelming people? Check out the pre-print of our new @vis4good paper: <LINK> @thaiqOn @thecindyxiong @nargesmahyar We discuss potential pros and cons of providing different kinds of metadata (very broadly defined) including info on where the data came from, the methods used to clean and analyze data, the visual encoding used, the people who were involved, and the people the vis was made for.",https://arxiv.org/abs/2108.13270,"Accompanying a data visualization with metadata may benefit readers by facilitating content understanding, strengthening trust, and providing accountability. However, providing this kind of information may also have negative, unintended consequences, such as biasing readers' interpretations, a loss of trust as a result of too much transparency, and the possibility of opening visualization creators with minoritized identities up to undeserved critique. To help future visualization researchers and practitioners decide what kinds of metadata to include, we discuss some of the potential benefits and risks of disclosing five kinds of metadata: metadata about the source of the underlying data; the cleaning and processing conducted; the marks, channels, and other design elements used; the people who directly created the visualization; and the people for whom the visualization was created. We conclude by proposing a few open research questions related to how to communicate metadata about visualizations. ","Making the Invisible Visible: Risks and Benefits of Disclosing Metadata
  in Visualization",2,"['Providing metadata about a visualization might build trust or help readers understand what is shown. But how do you decide what to say without overwhelming people?\n\nCheck out the pre-print of our new @vis4good paper: <LINK>\n\n@thaiqOn @thecindyxiong @nargesmahyar', 'We discuss potential pros and cons of providing different kinds of metadata (very broadly defined) including info on where the data came from, the methods used to clean and analyze data, the visual encoding used, the people who were involved, and the people the vis was made for.']",21,08,540
9,140,1427842534626119681,2909395381,Andrey 🔬🧠🐠💤,"Physical data storage is still a major problem for microscopists - and biologist in general. We propose how it can be improved: modern data infrastructure supported by funding: <LINK> @KristinBriney @temorrell @sandragesing @manorlaboratory 1/5 We reviewed common ""solutions"" for data storage: cloud, USB drives, dedicated servers, university-supported clusters. Data plumbing needs major improvement! We also need your feedback and input. Let's solve this together. 2/5 Physical data storage is a common problem. Astronomers face similar issues, and have come up with some good ideas! We present short section about their problems and successes. 3/5 Our proposal: admit that we have data plumbing problem and that it is not too expensive to solve. Then, convince funding agencies and universities to work together to establish shared funds to support upgrades to data infra. It must support new projects and collaborations. 4/5 This position statement was the result of more than a year of talking to wonderful, helpful people. Special thanks to @damiandn for major contribution. Thanks to @valonychus, Dan Koo, @fracutrale, Jeremy Weimer, @Campbell_JD_PhD, Sarah Nusser, @JamesJonkman ... ... [cont'] thanks to Eric Wait and Blair Rossetti of @AICjanelia, @BenSteventon2, @Daniel_Wa19, @viktri08, Henry Neeman, @mjuric, and Alexander Szalay for talking with us and sharing your experience! 5/5 The most important part: we need your input. Please email us with your experience and opinion, so we can improve the plan and make it happen.",https://arxiv.org/abs/2108.07631,"Modern tools for biological research, especially microscopy, have rapidly advanced in recent years, which has led to the generation of increasingly large amounts of data on a regular basis. The result is that scientists desperately need state-of-the-art technical infrastructure for raw data storage, transfer, and processing. These scientists currently rely on outdated ways to move and store data, costing valuable time and risking loss of valuable data. While the community is aware of modern approaches to data management and high-level principles (including FAIR), highly-trained and highly-paid scientists are forced to spend time dealing with technical problems, which can ultimately costs more than providing storage and a fast network on campus. Here we provide concise arguments for better infrastructure, blueprints of possible solutions, and advice in navigating the political process of solving this issue. We suggest, as a broad solution, separate NIH-managed fund for supporting universities and institutes in deployment of data storage and long-term data sharing for all funded projects. This position statement is open for more contributors from imaging, life sciences, and other disciplines. Please contact us with your experience and perspective. ",Biologists need modern data infrastructure on campus,7,"['Physical data storage is still a major problem for microscopists - and biologist in general.\n\nWe propose how it can be improved: modern data infrastructure supported by funding:\n\n<LINK>\n\n@KristinBriney @temorrell @sandragesing @manorlaboratory \n1/5', 'We reviewed common ""solutions"" for data storage: cloud, USB drives, dedicated servers, university-supported clusters.\nData plumbing needs major improvement!\nWe also need your feedback and input. Let\'s solve this together.\n\n2/5', 'Physical data storage is a common problem. Astronomers face similar issues, and have come up with some good ideas! We present short section about their problems and successes.\n\n3/5', 'Our proposal: admit that we have data plumbing problem and that it is not too expensive to solve. Then, convince funding agencies and universities to work together to establish shared funds to support upgrades to data infra. It must support new projects and collaborations.\n\n4/5', 'This position statement was the result of more than a year of talking to wonderful, helpful people.\nSpecial thanks to @damiandn for major contribution.\n\nThanks to @valonychus, Dan Koo, @fracutrale, Jeremy Weimer, @Campbell_JD_PhD, Sarah Nusser, @JamesJonkman ...', ""... [cont'] thanks to Eric Wait and Blair Rossetti of @AICjanelia, @BenSteventon2, @Daniel_Wa19, @viktri08, Henry Neeman, @mjuric, and Alexander Szalay for talking with us and sharing your experience!\n\n5/5"", 'The most important part:\nwe need your input. Please email us with your experience and opinion, so we can improve the plan and make it happen.']",21,08,1537
10,88,1504639823184809984,22148802,Leo C. Stein 🦁,"🎉 New paper day! 🎉  Tidally-induced nonlinear resonances in EMRIs with an analogue model (<LINK>) This is David's first paper! So, what did we study? 1/6 <LINK> Orbits around spinning black holes have 3 frequencies, so there can be resonances. Adding a perturbation—e.g. a distant 3rd body—can ""break"" resonant tori, creating nonlinear resonances. Here's what it looks like on a Poincaré section. Our phase space is 6d → 4d Poinc. sect. 2/6 <LINK> To visualize a 4-dimensional Poincaré section, use 3 spatial dimensions, and color as a 4th dimension. Here is one for our system, a resonant torus that broke into a nonlinear resonance because of an external perturbation (the gravitational field of some distant stuff). 3/6 <LINK> Inside one of these resonances, we get libration of the ""resonance angle"" on a new time scale, seen below. So, what's the big idea in our paper? If we don't model this, will it screw up the ability of the LISA mission to detect systems that pass through resonance? 4/6 <LINK> We computed the mismatch between signals where we do or don't attempt to model the nonlinear resonance, over a range of parameter space, to find out: how strong must the external perturbation be so that it *must be* modeled to get things right? 5/6 In the end, we found a simple approximate region of parameter space where the resonance must be modeled: ε ≳ 300q², where q is the small mass ratio, and ε is a dimensionless measure of the strength of the perturbation. Read all about it here ➡️ <LINK> 6/6ish If you want to learn more about Poincaré sections, check out this interactive web toy: <LINK> You can see our progress in this project by when I was tweeting about it long ago: <LINK>",https://arxiv.org/abs/2203.08841,"One of the important classes of targets for the future space-based gravitational wave observatory LISA is extreme mass ratio inspirals (EMRIs), where long and accurate waveform modeling is necessary for detection and characterization. When modeling the dynamics of an EMRI, several effects need to be included, such as the modifications caused by an external tidal field. The effects of such perturbations will generally break integrability at resonance, and can produce significant dephasing from an unperturbed system. In this paper, we use a Newtonian analogue of a Kerr black hole to study the effect of an external tidal field on the dynamics and the gravitational waveform. We have developed a numerical framework that takes advantage of the integrability of the background system to evolve it with a symplectic splitting integrator, and compute approximate gravitational waveforms to estimate the time scale over which the perturbation affects the dynamics. We find that different entry points into the resonance in phase-space can produce substantially different dynamics. Finally, by comparing this time scale with the inspiral time, we find tidal effects will need to be included when modeling EMRI gravitational waves when $\varepsilon \gtrsim 300\, q^2$, where $q$ is the small mass ratio, and $\varepsilon$ measures the strength of the external tidal field. ",Tidally-induced nonlinear resonances in EMRIs with an analogue model,8,"[""🎉 New paper day! 🎉 \n\nTidally-induced nonlinear resonances in EMRIs with an analogue model (<LINK>)\n\nThis is David's first paper! So, what did we study?\n1/6 <LINK>"", 'Orbits around spinning black holes have 3 frequencies, so there can be resonances. Adding a perturbation—e.g. a distant 3rd body—can ""break"" resonant tori, creating nonlinear resonances. Here\'s what it looks like on a Poincaré section. Our phase space is 6d → 4d Poinc. sect.\n2/6 https://t.co/HQl3lOLS09', 'To visualize a 4-dimensional Poincaré section, use 3 spatial dimensions, and color as a 4th dimension. Here is one for our system, a resonant torus that broke into a nonlinear resonance because of an external perturbation (the gravitational field of some distant stuff).\n3/6 https://t.co/AGnHAyU4oJ', 'Inside one of these resonances, we get libration of the ""resonance angle"" on a new time scale, seen below. So, what\'s the big idea in our paper? If we don\'t model this, will it screw up the ability of the LISA mission to detect systems that pass through resonance?\n4/6 https://t.co/26AnYhT0Hy', ""We computed the mismatch between signals where we do or don't attempt to model the nonlinear resonance, over a range of parameter space, to find out: how strong must the external perturbation be so that it *must be* modeled to get things right?\n\n5/6"", 'In the end, we found a simple approximate region of parameter space where the resonance must be modeled: ε ≳ 300q², where q is the small mass ratio, and ε is a dimensionless measure of the strength of the perturbation.\n\nRead all about it here ➡️ https://t.co/GrWbb8uJ0F\n\n6/6ish', 'If you want to learn more about Poincaré sections, check out this interactive web toy: https://t.co/AIGfyCcJoT', 'You can see our progress in this project by when I was tweeting about it long ago: https://t.co/CVzoT7n0zO']",22,03,1697
11,75,1203009449838993413,8514822,asim kadav,"Our new paper ""15 Keypoints Is All You Need"" <LINK> is on arXiv. It describes ""KeyTrack"" which has been #1 on the PoseTrack '17 leaderboard since last month. With just 0.43M parameters, it can be trained on 1 GPU in under two hours. #computervision <LINK> Paper also includes comparisons of transformer and convolutions, and at low resolutions, transformers can outperform convolutions for the tracking task.",http://arxiv.org/abs/1912.02323,"Pose tracking is an important problem that requires identifying unique human pose-instances and matching them temporally across different frames of a video. However, existing pose tracking methods are unable to accurately model temporal relationships and require significant computation, often computing the tracks offline. We present an efficient Multi-person Pose Tracking method, KeyTrack, that only relies on keypoint information without using any RGB or optical flow information to track human keypoints in real-time. Keypoints are tracked using our Pose Entailment method, in which, first, a pair of pose estimates is sampled from different frames in a video and tokenized. Then, a Transformer-based network makes a binary classification as to whether one pose temporally follows another. Furthermore, we improve our top-down pose estimation method with a novel, parameter-free, keypoint refinement technique that improves the keypoint estimates used during the Pose Entailment step. We achieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks while using only a fraction of the computation required by most other methods for computing the tracking information. ",15 Keypoints Is All You Need,2,"['Our new paper ""15 Keypoints Is All You Need"" <LINK> is on arXiv. It describes ""KeyTrack"" which has been #1 on the PoseTrack \'17 leaderboard since last month. With just 0.43M parameters, it can be trained on 1 GPU in under two hours. #computervision <LINK>', 'Paper also includes comparisons of transformer and convolutions, and at low resolutions, transformers can outperform convolutions for the tracking task.']",19,12,408
12,3,1003698175042048001,965383857775300608,Yichuan Zhang,"In a few days, I will release a new version of my paper on variational measure preserving flows <LINK> and the demo code on my github <LINK> @viettran86 Yes, it is very likely to reach global optimum. check out the experiment section for Guassian and half moon target example. Soon, you can verify that with my code. @viettran86 I am not sure what do you mean by mixture model. But in my experiment, my methods achieved much better results than current SOTA on mnist with deep convolutional decoder. I think that is more challenging than most mixture models",https://arxiv.org/abs/1805.10377,"Statistical inference methods are fundamentally important in machine learning. Most state-of-the-art inference algorithms are variants of Markov chain Monte Carlo (MCMC) or variational inference (VI). However, both methods struggle with limitations in practice: MCMC methods can be computationally demanding; VI methods may have large bias. In this work, we aim to improve upon MCMC and VI by a novel hybrid method based on the idea of reducing simulation bias of finite-length MCMC chains using gradient-based optimisation. The proposed method can generate low-biased samples by increasing the length of MCMC simulation and optimising the MCMC hyper-parameters, which offers attractive balance between approximation bias and computational efficiency. We show that our method produces promising results on popular benchmarks when compared to recent hybrid methods of MCMC and VI. ",Ergodic Inference: Accelerate Convergence by Optimisation,3,"['In a few days, I will release a new version of my paper on variational measure preserving flows <LINK> and the demo code on my github <LINK>', '@viettran86 Yes, it is very likely to reach global optimum. check out the experiment section for Guassian and half moon target example. Soon, you can verify that with my code.', '@viettran86 I am not sure what do you mean by mixture model. But in my experiment, my methods achieved much better results than current SOTA on mnist with deep convolutional decoder. I think that is more challenging than most mixture models']",18,05,557
13,203,1392109863636017155,942238055607435264,Luca Carlone,Traditional approaches for category-level object pose and shape estimation are sensitive to outliers and get stuck in local minima. We propose the first approach that avoids local minima and is robust to 70-90% outliers: <LINK> #computervision #mitsparklab accepted at RSS 2021: <LINK>,https://arxiv.org/abs/2104.08383,"We consider a category-level perception problem, where one is given 3D sensor data picturing an object of a given category (e.g. a car), and has to reconstruct the pose and shape of the object despite intra-class variability (i.e. different car models have different shapes). We consider an active shape model, where -- for an object category -- we are given a library of potential CAD models describing objects in that category, and we adopt a standard formulation where pose and shape estimation are formulated as a non-convex optimization. Our first contribution is to provide the first certifiably optimal solver for pose and shape estimation. In particular, we show that rotation estimation can be decoupled from the estimation of the object translation and shape, and we demonstrate that (i) the optimal object rotation can be computed via a tight (small-size) semidefinite relaxation, and (ii) the translation and shape parameters can be computed in closed-form given the rotation. Our second contribution is to add an outlier rejection layer to our solver, hence making it robust to a large number of misdetections. Towards this goal, we wrap our optimal solver in a robust estimation scheme based on graduated non-convexity. To further enhance robustness to outliers, we also develop the first graph-theoretic formulation to prune outliers in category-level perception, which removes outliers via convex hull and maximum clique computations; the resulting approach is robust to 70%-90% outliers. Our third contribution is an extensive experimental evaluation. Besides providing an ablation study on a simulated dataset and on the PASCAL3D+ dataset, we combine our solver with a deep-learned keypoint detector, and show that the resulting approach improves over the state of the art in vehicle pose estimation in the ApolloScape datasets. ","Optimal Pose and Shape Estimation for Category-level 3D Object
  Perception",2,"['Traditional approaches for category-level object pose and shape estimation are sensitive to outliers and get stuck in local minima. We propose the first approach that avoids local minima and is robust to 70-90% outliers: <LINK> \n#computervision #mitsparklab', 'accepted at RSS 2021: https://t.co/zhC7MTjaCJ']",21,04,285
14,152,1231863479293874177,57640264,Ariane Nunes Alves,".@Rebecca_Wade_C , @DariaKokh and I wrote a review of computational methods to study ligand-protein binding kinetics. We assessed the performance of methods considering two benchmark systems, T4 lysozyme (T4L) and N-HSP90. (1/3) @HITStudies #compchem <LINK> <LINK> @Rebecca_Wade_C @DariaKokh @HITStudies The figure below shows the times required for different methods to simulate complexes with different exp. residence times (RT). In the last two years many methods to compute relative RT were published. An increase of four orders of magnitude in exp. RT leads to a smaller, (2/3) <LINK> @Rebecca_Wade_C @DariaKokh @HITStudies one order of magnitude increase in comp. time. For T4L, results indicate that good RT estimation can be achieved without exhaustive path sampling so long as the most probable paths are sampled (table below). Another highlight: two works aiming at computing RT prospectively. (3/3) <LINK> @_Maicol_ @Rebecca_Wade_C @DariaKokh @HITStudies thanks! 😊 @sowmyaindrakum1 @Rebecca_Wade_C @DariaKokh @HITStudies what i showed you in your visit is related, but not the same thing. @Jmondal_tifrh @Rebecca_Wade_C @DariaKokh @HITStudies thanks! 😊 @Herr_Flow @Rebecca_Wade_C @DariaKokh @HITStudies you are welcome! 😀 is the arXiv paper published? send me the link, so I can correct the citation later. our work is under review on COSB right now.",https://arxiv.org/abs/2002.08983,"Due to the contribution of drug-target binding kinetics to drug efficacy, there is a high level of interest in developing methods to predict drug-target binding kinetic parameters. During the review period, a wide range of enhanced sampling molecular dynamics simulation-based methods has been developed for computing drug-target binding kinetics and studying binding and unbinding mechanisms. Here, we assess the performance of these methods considering two benchmark systems in detail: mutant T4 lysozyme-ligand complexes and a large set of N-HSP90-inhibitor complexes. The results indicate that some of the simulation methods can already be usefully applied in drug discovery or lead optimization programs but that further studies on more high-quality experimental benchmark datasets are necessary to improve and validate computational methods. ","Recent progress in molecular simulation methods for drug binding
  kinetics",7,"['.@Rebecca_Wade_C , @DariaKokh  and I wrote a review of computational methods to study ligand-protein binding kinetics. We assessed the performance of methods considering two benchmark systems, T4 lysozyme (T4L) and N-HSP90. (1/3)\n@HITStudies  #compchem\n<LINK> <LINK>', '@Rebecca_Wade_C @DariaKokh @HITStudies The figure below shows the times required for different methods to simulate complexes with different exp. residence times (RT). In the last two years many methods to compute relative RT were published. An increase of four orders of magnitude in exp. RT leads to a smaller,  (2/3) https://t.co/xfMw29mTgi', '@Rebecca_Wade_C @DariaKokh @HITStudies one order of magnitude increase in comp. time.\nFor T4L, results indicate that good RT estimation can be achieved without exhaustive path sampling so long as the most probable paths are sampled (table below).\nAnother highlight: two works aiming at computing RT prospectively. (3/3) https://t.co/Eaom4mwQPN', '@_Maicol_ @Rebecca_Wade_C @DariaKokh @HITStudies thanks!\n😊', '@sowmyaindrakum1 @Rebecca_Wade_C @DariaKokh @HITStudies what i showed you in your visit is related, but not the same thing.', '@Jmondal_tifrh @Rebecca_Wade_C @DariaKokh @HITStudies thanks!\n😊', '@Herr_Flow @Rebecca_Wade_C @DariaKokh @HITStudies you are welcome!\n😀\nis the arXiv paper published? send me the link, so I can correct the citation later. our work is under review on COSB right now.']",20,02,1361
15,75,1373899529871851523,1297174706622205955,Andrea Palermo,"Our new paper is out today on the arXiv! We found that, for a relativistic fluid at local equilibrium, polarization is not only induced by thermal vorticity, but also by the symmetric gradient of temperature, the thermal shear. Check it out! <LINK> 1/5 It is well known that spin couples to the vorticity of a relativistic fluid, which also has a striking experimental confirmation in the measurement of the global polarization of Lambda hyperon. At local equilibrium, thermal vorticity is present , but it is not alone... 2/5 The symmetric derivative of the temperature is also non vanishing! This term is called thermal shear tensor. Using linear response theory we found that thermal shear (here \xi) contributes to the polarization of fermions! 3/5 <LINK> This new contribution was neglected so far, and might help in solving the so called ""sign puzzle"" of the local Lambda polarization. 4/5 Similar studies were also carried out by another group, only few days ago. <LINK> <LINK> All of these recent results, hint that we might be in the right direction... 5/5",https://arxiv.org/abs/2103.10917,"We show that spin polarization of a fermion in a relativistic fluid at local thermodynamic equilibrium can be generated by the symmetric derivative of the four-temperature vector, defined as thermal shear. As a consequence, besides vorticity, acceleration and temperature gradient, also the shear tensor contributes to the polarization of particles in a fluid. This contribution to the spin polarization vector, which is entirely non-dissipative, adds to the well known term proportional to thermal vorticity and may thus have important consequences for the solution of the local polarization puzzles observed in relativistic heavy ion collisions. ",Spin-thermal shear coupling in a relativistic fluid,5,"['Our new paper is out today on the arXiv!\n\nWe found that, for a relativistic fluid at local equilibrium, polarization is not only induced by thermal vorticity, but also by the symmetric gradient of temperature, the thermal shear. \nCheck it out!\n<LINK>\n1/5', 'It is well known that spin couples to the vorticity of a relativistic fluid, which also has a striking experimental confirmation in the measurement of the global polarization of Lambda hyperon. At local equilibrium, thermal vorticity is present , but it is not alone...\n2/5', 'The symmetric derivative of the temperature is also non vanishing! This term is called thermal shear tensor. Using linear response theory we found that thermal shear (here \\xi) contributes to the polarization of fermions!\n3/5 https://t.co/Nlc8CBUiMl', 'This new contribution was neglected so far, and might help in solving the so called ""sign puzzle"" of the local Lambda polarization. \n4/5', 'Similar studies were also carried out by another group, only few days ago. \nhttps://t.co/g9Aptc6RKf\nhttps://t.co/WHPbVUohpk\n\nAll of these recent results, hint that we might be in the right direction...\n\n5/5']",21,03,1065
16,214,1447979702442881024,212147661,Yu Bai,"🆕""When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?"" <LINK> We theoretically study what RL can learn in multi-player general-sum MGs without exp(# players) samples. Joint w/ Ziang Song (Peking U.) & @WispyMay.  🧵 For general-sum Markov games with m&gt;=2 players, it is known that learning the commonly studied Nash Equilibria requires exp(\Omega(m)) samples.  We instead look at two relaxed solution concepts: Correlated Equilibria (CE) and Coarse Correlated Equilibria (CCE). 2/n We design the first line of algorithms that learn CE/CCE in general-sum Markov games with samples that only depend on the max action space (over all players), instead of the product of the action spaces which is at least exp(\Omega(m)). 3/n We then turn to Markov Potential Games (MPGs), a well-studied subclass of general-sum MGs. In this case, there exist recent algorithms that can find Nash with poly(m, 1/eps) samples. We design an alternative algorithm with similar poly(m) and improved dependence on eps. 4/n Overall, we hope these results could shed light on what can be learned in general-sum MGs, when we do not have the luxury of observing exp(m) samples and recovering the full game. Comments and suggestions are welcome! 5/n, n=5",https://arxiv.org/abs/2110.04184,"Multi-agent reinforcement learning has made substantial empirical progresses in solving games with a large number of players. However, theoretically, the best known sample complexity for finding a Nash equilibrium in general-sum games scales exponentially in the number of players due to the size of the joint action space, and there is a matching exponential lower bound. This paper investigates what learning goals admit better sample complexities in the setting of $m$-player general-sum Markov games with $H$ steps, $S$ states, and $A_i$ actions per player. First, we design algorithms for learning an $\epsilon$-Coarse Correlated Equilibrium (CCE) in $\widetilde{\mathcal{O}}(H^5S\max_{i\le m} A_i / \epsilon^2)$ episodes, and an $\epsilon$-Correlated Equilibrium (CE) in $\widetilde{\mathcal{O}}(H^6S\max_{i\le m} A_i^2 / \epsilon^2)$ episodes. This is the first line of results for learning CCE and CE with sample complexities polynomial in $\max_{i\le m} A_i$. Our algorithm for learning CE integrates an adversarial bandit subroutine which minimizes a weighted swap regret, along with several novel designs in the outer loop. Second, we consider the important special case of Markov Potential Games, and design an algorithm that learns an $\epsilon$-approximate Nash equilibrium within $\widetilde{\mathcal{O}}(S\sum_{i\le m} A_i / \epsilon^3)$ episodes (when only highlighting the dependence on $S$, $A_i$, and $\epsilon$), which only depends linearly in $\sum_{i\le m} A_i$ and significantly improves over existing efficient algorithm in the $\epsilon$ dependence. Overall, our results shed light on what equilibria or structural assumptions on the game may enable sample-efficient learning with many players. ","When Can We Learn General-Sum Markov Games with a Large Number of
  Players Sample-Efficiently?",5,"['🆕""When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?""\n\n<LINK>\n\nWe theoretically study what RL can learn in multi-player general-sum MGs without exp(# players) samples.\n\nJoint w/ Ziang Song (Peking U.) &amp; @WispyMay. \n\n🧵', 'For general-sum Markov games with m&gt;=2 players, it is known that learning the commonly studied Nash Equilibria requires exp(\\Omega(m)) samples. \n\nWe instead look at two relaxed solution concepts: Correlated Equilibria (CE) and Coarse Correlated Equilibria (CCE).\n\n2/n', 'We design the first line of algorithms that learn CE/CCE in general-sum Markov games with samples that only depend on the max action space (over all players), instead of the product of the action spaces which is at least exp(\\Omega(m)).\n\n3/n', 'We then turn to Markov Potential Games (MPGs), a well-studied subclass of general-sum MGs. In this case, there exist recent algorithms that can find Nash with poly(m, 1/eps) samples.\n\nWe design an alternative algorithm with similar poly(m) and improved dependence on eps.\n\n4/n', 'Overall, we hope these results could shed light on what can be learned in general-sum MGs, when we do not have the luxury of observing exp(m) samples and recovering the full game.\n\nComments and suggestions are welcome!\n\n5/n, n=5']",21,10,1268
17,143,1427436677698670592,1219708000790876161,Datta Lab,"Interested in self-propelled living & active systems? In <LINK>, we describe recent progress in the study of active transport in complex environments, focusing on two key biological systems—bacteria & eukaryotic cells—as archetypes of active matter. (1/6) Active transport is fundamentally interesting in biology, physics, & engineering, and is important to biomedical, environmental, & industrial processes. How do complexities such as geometric constraints, mechanical cues, and external stimuli influence transport? (2/6) In this chapter to be published in a book by @RoySocChem press, we review research highlighting how such environmental factors can fundamentally alter cellular motility, hindering or promoting active transport in unexpected ways, & giving rise to fascinating new behaviors. (3/6) In parallel, we describe open questions and promising avenues for future research, and describe connections to other active systems & more general theoretical/computational models of transport processes in complex environments. (4/6) Our goal in writing this chapter was not to present a comprehensive overview of all the literature in the field, but rather, to highlight some active (pun intended) areas of research whose growth has been particularly rapid recently. (5/6) It was a lot of fun to work on this with postdocs Alejandro Martínez-Calvo and Carolina Trenado-Yuste. Please RT/share with anyone who might be interested interested. As always, any and all feedback is welcome! (6/6)",http://arxiv.org/abs/2108.07011,"The ability of many living systems to actively self-propel underlies critical biomedical, environmental, and industrial processes. While such active transport is well-studied in uniform settings, environmental complexities such as geometric constraints, mechanical cues, and external stimuli such as chemical gradients and fluid flow can strongly influence transport. In this chapter, we describe recent progress in the study of active transport in such complex environments, focusing on two prominent biological systems -- bacteria and eukaryotic cells -- as archetypes of active matter. We review research findings highlighting how environmental factors can fundamentally alter cellular motility, hindering or promoting active transport in unexpected ways, and giving rise to fascinating behaviors such as directed migration and large-scale clustering. In parallel, we describe specific open questions and promising avenues for future research. Furthermore, given the diverse forms of active matter -- ranging from enzymes and driven biopolymer assemblies, to microorganisms and synthetic microswimmers, to larger animals and even robots -- we also describe connections to other active systems as well as more general theoretical/computational models of transport processes in complex environments. ",Active transport in complex environments,6,"['Interested in self-propelled living &amp; active systems? In <LINK>, we describe recent progress in the study of active transport in complex environments, focusing on two key biological systems—bacteria &amp; eukaryotic cells—as archetypes of active matter. (1/6)', 'Active transport is fundamentally interesting in biology, physics, &amp; engineering, and is important to biomedical, environmental, &amp; industrial processes. How do complexities such as geometric constraints, mechanical cues, and external stimuli influence transport? (2/6)', 'In this chapter to be published in a book by @RoySocChem press, we review research highlighting how such environmental factors can fundamentally alter cellular motility, hindering or promoting active transport in unexpected ways, &amp; giving rise to fascinating new behaviors. (3/6)', 'In parallel, we describe open questions and promising avenues for future research, and describe connections to other active systems &amp; more general theoretical/computational models of transport processes in complex environments. (4/6)', 'Our goal in writing this chapter was not to present a comprehensive overview of all the literature in the field, but rather, to highlight some active (pun intended) areas of research whose growth has been particularly rapid recently.  (5/6)', 'It was a lot of fun to work on this with postdocs Alejandro Martínez-Calvo and Carolina Trenado-Yuste. Please RT/share with anyone who might be interested interested. As always, any and all feedback is welcome! (6/6)']",21,08,1495
18,165,1433526764710596608,358306755,Jani Kastikainen,New paper today with Sanjit Shashi! We studied boundary CFT 1- and 2-point functions in a holographic model with an end-of-the-world brane. For this we used the geodesic approximation that requires taking into account geodesics reflecting off the brane. <LINK>,https://arxiv.org/abs/2109.00079,"We compute correlation functions, specifically 1-point and 2-point functions, in holographic boundary conformal field theory (BCFT) using geodesic approximation. The holographic model consists of a massive scalar field coupled to a Karch-Randall brane -- a rigid boundary in the bulk AdS space. Geodesic approximation requires the inclusion of paths reflecting off of this brane, which we show in detail. For the 1-point function, we find agreement between geodesic approximation and the harder $\Delta$-exact calculation, and we give a novel derivation of boundary entropy using the result. For the 2-point function, we find a factorization phase transition and a mysterious set of anomalous boundary-localized BCFT operators. We also discuss some puzzles concerning these operators. ",Structure of Holographic BCFT Correlators from Geodesics,1,['New paper today with Sanjit Shashi! We studied boundary CFT 1- and 2-point functions in a holographic model with an end-of-the-world brane. For this we used the geodesic approximation that requires taking into account geodesics reflecting off the brane.\n\n<LINK>'],21,09,260
19,137,1248232346785988608,29682697,Robin Scheibler,"We just released a new preprint about MM algorithms for joint independent subspace analysis (JISA-MM), that is blind source separation where some sources are correlated. We apply it to blind audio source(s) extraction. Paper: <LINK> Code: <LINK> <LINK>",https://arxiv.org/abs/2004.03926,"In this work, we propose efficient algorithms for joint independent subspace analysis (JISA), an extension of independent component analysis that deals with parallel mixtures, where not all the components are independent. We derive an algorithmic framework for JISA based on the majorization-minimization (MM) optimization technique (JISA-MM). We use a well-known inequality for super-Gaussian sources to derive a surrogate function of the negative log-likelihood of the observed data. The minimization of this surrogate function leads to a variant of the hybrid exact-approximate diagonalization problem, but where multiple demixing vectors are grouped together. In the spirit of auxiliary function based independent vector analysis (AuxIVA), we propose several updates that can be applied alternately to one, or jointly to two, groups of demixing vectors. Recently, blind extraction of one or more sources has gained interest as a reasonable way of exploiting larger microphone arrays to achieve better separation. In particular, several MM algorithms have been proposed for overdetermined IVA (OverIVA). By applying JISA-MM, we are not only able to rederive these in a general manner, but also find several new algorithms. We run extensive numerical experiments to evaluate their performance, and compare it to that of full separation with AuxIVA. We find that algorithms using pairwise updates of two sources, or of one source and the background have the fastest convergence, and are able to separate target sources quickly and precisely from the background. In addition, we characterize the performance of all algorithms under a large number of noise, reverberation, and background mismatch conditions. ","MM Algorithms for Joint Independent Subspace Analysis with Application
  to Blind Single and Multi-Source Extraction",1,"['We just released a new preprint about MM algorithms for joint independent subspace analysis (JISA-MM), that is blind source separation where some sources are correlated. We apply it to blind audio source(s) extraction. Paper: <LINK> Code: <LINK> <LINK>']",20,04,252
20,223,1280543336076582912,767094649181704192,Seth Neel 🇱🇰🇺🇸,"New Preprint out today with @aaroth, and Saeed Sharifi-Malvajerdi! “Descent-to-Delete: Gradient-Based Methods for Machine Unlearning” (<LINK>) Motivated by GDPR's ""Right to be Forgotten"" we study the problem of efficiently deleting user data from AI models (1/n) We give the first efficient data deletion algorithms that are able to handle long sequences of updates while promising both per-deletion run-time and error that do not grow with the length of the sequence...allowing deployed models to maintain a steady state (2/n) Privacy laws like GDPR guarantee users a “right to be forgotten.” This is why large tech companies support user requests to delete their data, and a host of startups have sprung up to help enterprise companies do this (@transcend_io , @mineapp_company )… (3/n) But what about the privacy risk of sensitive data that has already been used to train AI models? Research shows that AI models themselves can encode sensitive user data (<LINK>), and so we must support user requests to delete their data from trained models! (4/n) A naïve soln would be to remove the users data from the training set and retrain from scratch – however this could be infeasible due to time, financial, and computational constraints. We design training algorithms in the convex setting that support efficient data deletion: (5/n) Both a simple gradient descent based method, and a distributed optimization algorithm that leverages techniques from reservoir sampling to satisfy rigorous deletion guarantees, and can perform better in high dimensions (6/n) We thank the paper of ginart, guan, valiant, and zou (<LINK>) for giving key motivations and definitions for this problem, where they study the problem of k-means clustering. (n/n) Cc @WarrenCntrPenn @Wharton @HarvardHBS @LISHarvard",https://arxiv.org/abs/2007.02923,"We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion. ",Descent-to-Delete: Gradient-Based Methods for Machine Unlearning,8,"['New Preprint out today with @aaroth, and Saeed Sharifi-Malvajerdi! “Descent-to-Delete: Gradient-Based Methods for Machine Unlearning” (<LINK>) Motivated by GDPR\'s ""Right to be Forgotten"" we study the problem of efficiently deleting user data from AI models (1/n)', 'We give the first efficient data deletion algorithms that are able to handle long sequences of updates while promising both per-deletion run-time and error that do not grow with the length of the sequence...allowing deployed models to maintain a steady state (2/n)', 'Privacy laws like GDPR guarantee users a “right to be forgotten.” This is why large tech companies support user requests to delete their data, and a host of startups have sprung up to help enterprise companies do this (@transcend_io , @mineapp_company )… (3/n)', 'But what about the privacy risk of sensitive data that has already been used to train AI models? Research shows that AI models themselves can encode sensitive user data (https://t.co/Y5OMbFZz0i), and so we must support user requests to delete their data from trained models! (4/n)', 'A naïve soln would be to remove the users data from the training set and retrain from scratch – however this could be infeasible due to time, financial, and computational constraints. We design training algorithms in the convex setting that support efficient data deletion: (5/n)', 'Both a simple gradient descent based method, and a distributed optimization algorithm that leverages techniques from reservoir sampling to satisfy rigorous deletion guarantees, and can perform better in high dimensions (6/n)', 'We thank the paper of ginart, guan, valiant, and zou (https://t.co/vJzmAWv4kb) for giving key motivations and definitions for this problem, where they study the problem of k-means clustering. (n/n)', 'Cc @WarrenCntrPenn @Wharton @HarvardHBS @LISHarvard']",20,07,1790
21,5,1455664708032282624,3018222517,Samuel Stanton,"Happy to share a new #NeurIPS2021 paper on scalable online Gaussian processes with Wesley Maddox and @andrewgwils  <LINK> (1/n) <LINK> Online data collection (eg BayesOpt) relies on acquisition functions to choose new points  Better acq fns == smarter choices Exact GPs are slow but work well with newer acq fns Sparse GPs are fast but harder to use (2/n) We rederive and generalize streaming SGPR (proposed by @thdbui in <LINK>) to any variational GP, relying on Laplace approximations to extend to non-Gaussian likelihoods. As a result we can combine SVGPs with lookahead acq. fns like knowledge gradient. (3/n) To bring everything together we demonstrate that SVGPs can be very effective in the context of active learning, black-box optimization, and adaptive control.  code: <LINK> (4/n)",https://arxiv.org/abs/2110.15172,"With a principled representation of uncertainty and closed form posterior updates, Gaussian processes (GPs) are a natural choice for online decision making. However, Gaussian processes typically require at least $\mathcal{O}(n^2)$ computations for $n$ training points, limiting their general applicability. Stochastic variational Gaussian processes (SVGPs) can provide scalable inference for a dataset of fixed size, but are difficult to efficiently condition on new data. We propose online variational conditioning (OVC), a procedure for efficiently conditioning SVGPs in an online setting that does not require re-training through the evidence lower bound with the addition of new data. OVC enables the pairing of SVGPs with advanced look-ahead acquisition functions for black-box optimization, even with non-Gaussian likelihoods. We show OVC provides compelling performance in a range of applications including active learning of malaria incidence, and reinforcement learning on MuJoCo simulated robotic control tasks. ","Conditioning Sparse Variational Gaussian Processes for Online
  Decision-making",4,"['Happy to share a new #NeurIPS2021 paper on scalable online Gaussian processes with Wesley Maddox and @andrewgwils \n\n<LINK>  (1/n) <LINK>', 'Online data collection (eg BayesOpt) relies on acquisition functions to choose new points \n\nBetter acq fns == smarter choices\n\nExact GPs are slow but work well with newer acq fns Sparse GPs are fast but harder to use (2/n)', 'We rederive and generalize streaming SGPR (proposed by @thdbui in https://t.co/osAuIpwq7x) to any variational GP, relying on Laplace approximations to extend to non-Gaussian likelihoods.\n\nAs a result we can combine SVGPs with lookahead acq. fns like knowledge gradient.\n (3/n)', 'To bring everything together we demonstrate that SVGPs can be very effective in the context of active learning, black-box optimization, and adaptive control. \n\ncode: https://t.co/p7T92QWPR6\n(4/n)']",21,10,791
22,11,1498767764831539200,20865039,Tristan Deleu,"New paper 📄 ""Bayesian Structure Learning with Generative Flow Networks"", with @AntGois, @ChrisEmezue, Mansi Rankawat, @SimonLacosteJ, Stefan Bauer & Yoshua Bengio @Mila_Quebec <LINK> <LINK> For that, we use a new class of probabilistic models, called GFlowNets, that induce a probability distribution over discrete & composite objects (which is perfect for graphs!). We wrote a paper about the foundations of these models a few months ago. <LINK>",https://arxiv.org/abs/2202.13903,"In Bayesian structure learning, we are interested in inferring a distribution over the directed acyclic graph (DAG) structure of Bayesian networks, from data. Defining such a distribution is very challenging, due to the combinatorially large sample space, and approximations based on MCMC are often required. Recently, a novel class of probabilistic models, called Generative Flow Networks (GFlowNets), have been introduced as a general framework for generative modeling of discrete and composite objects, such as graphs. In this work, we propose to use a GFlowNet as an alternative to MCMC for approximating the posterior distribution over the structure of Bayesian networks, given a dataset of observations. Generating a sample DAG from this approximate distribution is viewed as a sequential decision problem, where the graph is constructed one edge at a time, based on learned transition probabilities. Through evaluation on both simulated and real data, we show that our approach, called DAG-GFlowNet, provides an accurate approximation of the posterior over DAGs, and it compares favorably against other methods based on MCMC or variational inference. ",Bayesian Structure Learning with Generative Flow Networks,2,"['New paper 📄 ""Bayesian Structure Learning with Generative Flow Networks"", with @AntGois, @ChrisEmezue, Mansi Rankawat, @SimonLacosteJ,  Stefan Bauer &amp; Yoshua Bengio @Mila_Quebec \n<LINK> <LINK>', 'For that, we use a new class of probabilistic models, called GFlowNets, that induce a probability distribution over discrete &amp; composite objects (which is perfect for graphs!). We wrote a paper about the foundations of these models a few months ago.  https://t.co/PR4iGzxtDm']",22,02,446
23,45,1430410364987052036,809472,Janet Yi-Ching Huang,"🚨New Preprint Alert 🚨 Super excited to share our incoming #CSCW2021 paper: ""Thing Constellation Visualizer: Exploring Emergent Relationships of Everyday Objects"" by me, Yu-Ting Cheng, Rung-Huei Liang, @yjhsu , Lin-Lin Chen. @ACM_CSCW #CSCW preprint: <LINK> <LINK> This work presents a novel tool that empowers designers to change their original perspectives to perceive the world and gain new insights by playing with data and AI. The work contributes a new approach and tool to support More-Than Human-Centred Design of IoT ecosystems.",https://arxiv.org/abs/2108.09448,"Designing future IoT ecosystems requires new approaches and perspectives to understand everyday practices. While researchers recognize the importance of understanding social aspects of everyday objects, limited studies have explored the possibilities of combining data-driven patterns with human interpretations to investigate emergent relationships among objects. This work presents Thing Constellation Visualizer (thingCV), a novel interactive tool for visualizing the social network of objects based on their co-occurrence as computed from a large collection of photos. ThingCV enables perspective-changing design explorations over the network of objects with scalable links. Two exploratory workshops were conducted to investigate how designers navigate and make sense of a network of objects through thingCV. The results of eight participants showed that designers were actively engaged in identifying interesting objects and their associated clusters of related objects. The designers projected social qualities onto the identified objects and their communities. Furthermore, the designers changed their perspectives to revisit familiar contexts and to generate new insights through the exploration process. This work contributes a novel approach to combining data-driven models with designerly interpretations of thing constellation towards More-Than Human-Centred Design of IoT ecosystems. ","Thing Constellation Visualizer: Exploring Emergent Relationships of
  Everyday Objects",2,"['🚨New Preprint Alert 🚨\nSuper excited to share our incoming #CSCW2021 paper: ""Thing Constellation Visualizer: Exploring Emergent Relationships of Everyday Objects"" by me, Yu-Ting Cheng, Rung-Huei Liang, @yjhsu , Lin-Lin Chen. @ACM_CSCW #CSCW\n\npreprint: <LINK> <LINK>', 'This work presents a novel tool that empowers designers to change their original perspectives to perceive the world and gain new insights by playing with data and AI. The work contributes a new approach and tool to support More-Than Human-Centred Design of IoT ecosystems.']",21,08,536
24,69,1306934928765079554,1282679296843288577,Jonathan Ullman,"Really pleased with this new paper with my PhD student Albert Cheu. We prove strong lower bounds for two ""intermediate models"" of differential privacy: the shuffle model and the pan-private model. 1/3 <LINK> Our work builds in an essential way on Albert's awesome paper with Victor Balcer, @mgtjoseph, and Jieming Mao. 2/3 <LINK> It's awesome to have such a productive and independent PhD student! I can't believe he's graduating in the Spring. I'm jealous of whoever gets to be his next boss and/or post-doc advisor! *wink wink* 3/3",https://arxiv.org/abs/2009.08000,"There has been a recent wave of interest in intermediate trust models for differential privacy that eliminate the need for a fully trusted central data collector, but overcome the limitations of local differential privacy. This interest has led to the introduction of the shuffle model (Cheu et al., EUROCRYPT 2019; Erlingsson et al., SODA 2019) and revisiting the pan-private model (Dwork et al., ITCS 2010). The message of this line of work is that, for a variety of low-dimensional problems -- such as counts, means, and histograms -- these intermediate models offer nearly as much power as central differential privacy. However, there has been considerably less success using these models for high-dimensional learning and estimation problems. In this work, we show that, for a variety of high-dimensional learning and estimation problems, both the shuffle model and the pan-private model inherently incur an exponential price in sample complexity relative to the central model. For example, we show that, private agnostic learning of parity functions over $d$ bits requires $\Omega(2^{d/2})$ samples in these models, and privately selecting the most common attribute from a set of $d$ choices requires $\Omega(d^{1/2})$ samples, both of which are exponential separations from the central model. Our work gives the first non-trivial lower bounds for these problems for both the pan-private model and the general multi-message shuffle model. ","The Limits of Pan Privacy and Shuffle Privacy for Learning and
  Estimation",3,"['Really pleased with this new paper with my PhD student Albert Cheu.  We prove strong lower bounds for two ""intermediate models"" of differential privacy: the shuffle model and the pan-private model. 1/3\n\n<LINK>', ""Our work builds in an essential way on Albert's awesome paper with Victor Balcer, @mgtjoseph, and Jieming Mao. 2/3\n\nhttps://t.co/F1jA6pN8KZ"", ""It's awesome to have such a productive and independent PhD student!  I can't believe he's graduating in the Spring.  I'm jealous of whoever gets to be his next boss and/or post-doc advisor! *wink wink* 3/3""]",20,09,533
25,33,1463595777498955782,79968415,Rahul Kashyap,"What happens when two neutron stars merge? There remains either a neutron star (NS) or a black hole (BH)...eventually. But if the system collapse to form a BH on a short timescale (~1-2 milliseconds), we call them prompt collapse event. Our new paper <LINK> (1) <LINK>  We put several constraints and combination of different datasets in a single plot. Here is a video to describe it sequentially -- Please read ahead to know what this plot is about. <LINK> The prompt collapse happens only when the combined mass of two NS goes above a certain limit, called threshold mass, otherwise the matter can withstand the crushing force of gravity, at least for a little while, and remain as hypermassive NS. (2) Now the question comes -- Can we learn about the properties of NS matter from it? Since the properties of matter (commonly known as the equation of state) is unknown, the game is to use different theories about it and see which one matches the observations the best. (3) We recently finished a work showing that it can be used to constrain the maximum mass of NS in a narrow range which will be extremely useful for our understanding of NS matter. (4) The crucial assumption that undergoes here is that the ratio of threshold mass (Mth) and maximum mass (Mmax) of NS for a given candidate equation of state is proportional to the compactness of maximum mass NS (Cmax). (5) Amazing! 'cause it connects threshold mass to Mmax and radius of maximum mass NS. On the other hand, there exists an upper and lower bound on Cmax using all (or as much as!) possible EOSs i.e. pressure-density curves to obtain the limits of radii (Rmax) of maximum masses. (6) We use this information to obtain the limits on threshold masses because we can write a function -- Mth(Mmax,Rmax). These limits as a function of Mmax further show that, just like GW170817, we can use future delayed collapse event to constrain the lower limit of maximum mass. (7) One can use a future prompt collapse event to constrain the upper limit of maximum mass. It's easy to distinguish between a prompt and delayed collapse using their gravitational wave -- prompt collapse events don't have post-merger signal while the delayed collapse events do.(8) It is not exceedingly difficult to know for a good BNS detection whether it's a prompt or delayed collapse in future GW detectors. We are excited to obtain the limits on maximum mass using our methodology. Please follow the video and paper to know about it in more detail. (9)",https://arxiv.org/abs/2111.05183,"We determine the threshold mass for prompt (no bounce) black hole formation in equal-mass neutron star (NS) mergers using a new set of 227 numerical relativity simulations. We consider 23 phenomenological and microphysical finite temperature equations of state (EOS), including models with hyperons and first-order phase transitions to deconfined quarks. We confirm the existence of EOS-insensitive relations between the threshold mass, the binary tidal parameter at the threshold ($\Lambda_{th}$), the maximum mass of nonrotating NSs, and the radii of reference mass NSs. We correct the systematic errors in previously reported fitting coefficients that were obtained with approximate general-relativity simulations. We combine the EOS-insensitive relations, phenomenological constraints on NS properties and observational data from GW170817 to derive an improved lower limit on radii of maximum mass and 1.6 M$_\odot$ NS of 9.81 km and 10.90 km, respectively. We also constrain the radius and quadrupolar tidal deformability ($\Lambda$) of a 1.4 $M_\odot$ NS to be larger than 10.74 km and 172, respectively. We consider uncertainties in all independent parameters -- fitting coefficients as well as GW170817 masses while reporting the range of radii constraints. We introduce new methods to constrain the upper as well as lower limit of NS maximum mass using future BNS detections and their identification as prompt or delayed collapse. With future observations it will be possible to derive even tighter constraints on the properties of matter at and above nuclear density using the method proposed in this work. ","Numerical relativity simulations of prompt collapse mergers: threshold
  mass and phenomenological constraints on neutron star properties after
  GW170817",11,"['What happens when two neutron stars merge?  There remains either a neutron star (NS) or a black hole (BH)...eventually. But if the system collapse to form a BH on a short timescale (~1-2 milliseconds), we call them prompt collapse event. Our new paper\n<LINK> (1)', 'https://t.co/g0fWqQoerK  \nWe put several constraints and combination of different datasets in a single plot. Here is a video to describe it sequentially --', 'Please read ahead to know what this plot is about. https://t.co/7UcRAHqvq4', 'The prompt collapse happens only when the combined mass of two NS goes above a certain limit, called threshold mass, otherwise the matter can withstand the crushing force of gravity, at least for a little while, and remain as hypermassive NS. (2)', 'Now the question comes -- Can we learn about the properties of NS matter from it? Since the properties of matter (commonly known as the equation of state) is unknown, the game is to use different theories about it and see which one matches the observations the best. (3)', 'We recently finished a work showing that it can be used to constrain the maximum mass of NS in a narrow range which will be extremely useful for our understanding of NS matter. (4)', 'The crucial assumption that undergoes here is that the ratio of threshold mass (Mth) and maximum mass (Mmax) of NS for a given candidate equation of state is proportional to the compactness of maximum mass NS (Cmax). (5)', ""Amazing! 'cause it connects threshold mass to Mmax and radius of maximum mass NS. On the other hand, there exists an upper and lower bound on Cmax using all (or as much as!) possible EOSs i.e. pressure-density curves to obtain the limits of radii (Rmax) of maximum masses. (6)"", 'We use this information to obtain the limits on threshold masses because we can write a function -- Mth(Mmax,Rmax). These limits as a function of Mmax further show that, just like GW170817, we can use future delayed collapse event to constrain the lower limit of maximum mass. (7)', ""One can use a future prompt collapse event to constrain the upper limit of maximum mass. It's easy to distinguish between a prompt and delayed collapse using their gravitational wave -- prompt collapse events don't have post-merger signal while the delayed collapse events do.(8)"", ""It is not exceedingly difficult to know for a good BNS detection whether it's a prompt or delayed collapse in future GW detectors. We are excited to obtain the limits on maximum mass using our methodology. \nPlease follow the video and paper to know about it in more detail. (9)""]",21,11,2492
26,164,1497149816933027847,1024520749258817537,Jan A. Krzywda,"23.2.22 #arXivaria <LINK> Turning stationary qubit into flying one will allow for coherent #quantum link between arrays of Si spin qubits. Here we propose scalable method of coherent Spin Qubit Shuttle (SQuS) in realistic Si/SiGe device (length~10mum, v~10m/s) <LINK>",http://arxiv.org/abs/2202.11793,"Silicon spin qubits stand out due to their very long coherence times, compatibility with industrial fabrication, and prospect to integrate classical control electronics. To achieve a truly scalable architecture, a coherent mid-range link between qubit registers has been suggested to solve the signal fan-out problem. Here we present a blueprint of such a $\sim 10\,\mu$m long link, called a spin qubit shuttle, which is based on connecting an array of gates into a small number of sets. The number of these gate sets and thus the number of required control signal is independent of the link distance to coherently shuttle the electron qubit. We discuss two different operation modes for the spin qubit shuttle: A qubit conveyor, i.e. a potential minimum that smoothly moves laterally, and a bucket brigade, in which the electron is transported through a series of tunnel-coupled quantum dots by adiabatic passage. We find the former approach more promising considering a realistic Si/SiGe device including potential disorder from the charged defects at the Si/SiO$_2$ layer, as well as typical charge noise. Focusing on the qubit transfer fidelity of the conveyor shuttling mode, motional narrowing, the interplay between orbital and valley excitation and relaxation in presence of $g$-factors that depend on orbital and valley state of the electron, and effects from spin-hotspots are discussed in detail. We find that a transfer fidelity of 99.9 \% is feasible in Si/SiGe at a speed of $\sim$10 m/s, if the average valley splitting and its inhomogeneity stay within realistic bounds. Operation at low global magnetic field $\approx 20$ mT and material engineering towards high valley splitting is favourable to reach high transfer fidelities. ","Blueprint of a scalable spin qubit shuttle device for coherent mid-range
  qubit transfer in disordered Si/SiGe/SiO$_2$",1,"['23.2.22 #arXivaria \n<LINK>\nTurning stationary qubit into flying one will allow for coherent #quantum link between arrays of Si spin qubits. Here we propose scalable method of coherent Spin Qubit Shuttle (SQuS) in realistic Si/SiGe device (length~10mum, v~10m/s) <LINK>']",22,02,267
27,11,1466081824925442059,41446149,Nenad Tomasev,"Happy to announce our new paper ""Advancing mathematics by guiding human intuition with AI"" <LINK> that was just published in Nature, and one of the associated maths papers ""The signature and cusp geometry of hyperbolic knots"" <LINK> One of the greatest opportunities for AI in the coming years will be in accelerating scientific discovery, and we need to find useful ways of incorporating machine learning in the scientific process across different fields, having it complement human brilliance It is a mutually beneficial iterative process through which AI can help guide the intuition of human domain experts, and where the domain experts can in turn help steer the lens of AI. We hope that these early breakthroughs are merely one of the first steps on an exciting journey of using AI for helping accelerate future discoveries in mathematics. This work wouldn't have been possible without @ADaviesAI, @PetarV_93, Lars Buesing, Sam Blackwell, Daniel Zheng, @weballergy, Richard Tanburn, @PeterWBattaglia, @BlundellCharles, @demishassabis, @pushmeet, Geordie Williamson, Marc Lackenby & Andras Juhasz.",https://arxiv.org/abs/2111.15323,"We introduce a new real-valued invariant called the natural slope of a hyperbolic knot in the 3-sphere, which is defined in terms of its cusp geometry. We show that twice the knot signature and the natural slope differ by at most a constant times the hyperbolic volume divided by the cube of the injectivity radius. This inequality was discovered using machine learning to detect relationships between various knot invariants. It has applications to Dehn surgery and to 4-ball genus. We also show a refined version of the inequality where the upper bound is a linear function of the volume, and the slope is corrected by terms corresponding to short geodesics that link the knot an odd number of times. ",The signature and cusp geometry of hyperbolic knots,5,"['Happy to announce our new paper ""Advancing mathematics by guiding human intuition with AI"" <LINK> that was just published in Nature, and one of the associated maths papers ""The signature and cusp geometry of hyperbolic knots""\xa0<LINK>', 'One of the greatest opportunities for AI in the coming years will be in accelerating scientific discovery, and we need to find useful ways of incorporating machine learning in the scientific process across different fields, having it complement human brilliance', 'It is a mutually beneficial iterative process through which AI can help guide the intuition of human domain experts, and where the domain experts can in turn help steer the lens of AI.', 'We hope that these early breakthroughs are merely one of the first steps on an exciting journey of using AI for helping accelerate future discoveries in mathematics.', ""This work wouldn't have been possible without @ADaviesAI, @PetarV_93, Lars Buesing, Sam Blackwell, Daniel Zheng, @weballergy, Richard Tanburn, @PeterWBattaglia, @BlundellCharles, @demishassabis,  @pushmeet, Geordie Williamson, Marc Lackenby &amp; Andras Juhasz.""]",21,11,1102
28,76,1450070861709561871,1278590255285796864,Ido Irani 🇺🇦,"Check out our new paper on core-collpase supernovae exploding in elliptical host galaxies! (1/3) <LINK> Core-collapse supernovae are thought to be the terminal explosions of young and massive stars. These are normally not found in ellipticals, which have an old stellar population, depleted of young and massive stars. (2/3) In our paper, we show 3 supernovae exploding near ellipticals - a rare subpopulation of supernovae. We discuss different ways to use these rare supernovae to study elliptical galaxies.",https://arxiv.org/abs/2110.02252,"We present observations of three Core-collapse supernovae (CCSNe) in elliptical hosts, detected by the Zwicky Transient Facility Bright Transient Survey (BTS). SN 2019ape is a SN Ic that exploded in the main body of a typical elliptical galaxy. Its properties are consistent with an explosion of a regular SN Ic progenitor. A secondary g-band light curve peak could indicate interaction of the ejecta with circumstellar material (CSM). An H$\alpha$-emitting source at the explosion site suggests a residual local star formation origin. SN 2018fsh and SN 2020uik are SNe II which exploded in the outskirts of elliptical galaxies. SN 2020uik shows typical spectra for SNe II, while SN 2018fsh shows a boxy nebular H$\alpha$ profile, a signature of CSM interaction. We combine these 3 SNe with 7 events from the literature and analyze their hosts as a sample. We present multi-wavelength photometry of the hosts, and compare this to archival photometry of all BTS hosts. Using the spectroscopically complete BTS we conclude that $0.3\%^{+0.3}_{-0.1}$ of all CCSNe occur in elliptical galaxies. We derive star-formation rates and stellar masses for the host-galaxies and compare them to the properties of other SN hosts. We show that CCSNe in ellipticals have larger physical separations from their hosts compared to SNe Ia in elliptical galaxies, and discuss implications for star-forming activity in elliptical galaxies. ","Less than 1% of Core-Collapse Supernovae in the local universe occur in
  elliptical galaxies",3,"['Check out our new paper on core-collpase supernovae exploding in elliptical host galaxies! (1/3) <LINK>', 'Core-collapse supernovae are thought to be the terminal explosions of young and massive stars. These are normally not found in ellipticals, which have an old stellar population, depleted of young and massive stars.  (2/3)', 'In our paper, we show 3 supernovae exploding near ellipticals - a rare subpopulation of supernovae. We discuss different ways to use these rare supernovae to study elliptical galaxies.']",21,10,509
29,173,1252792195297562624,1082541680811601920,Tanya Chowdhury,"Check out our new paper ""Neural Abstractive Summarization with Structural Attention"" now accepted at #IJCAI2020. Work in collaboration with @shocheen and @Tanmoy_Chak, from when I was a part of @lcs2iiitd Paper Link : <LINK> <LINK> We present a hierarchical encoder, based on structural attention, to model complex inter-sentence and document dependencies. We do so by implicitly modelling documents as non projective parse trees within end-to-end training. We see significant gains on unstructured MDS corpora.",https://arxiv.org/abs/2004.09739,"Attentional, RNN-based encoder-decoder architectures have achieved impressive performance on abstractive summarization of news articles. However, these methods fail to account for long term dependencies within the sentences of a document. This problem is exacerbated in multi-document summarization tasks such as summarizing the popular opinion in threads present in community question answering (CQA) websites such as Yahoo! Answers and Quora. These threads contain answers which often overlap or contradict each other. In this work, we present a hierarchical encoder based on structural attention to model such inter-sentence and inter-document dependencies. We set the popular pointer-generator architecture and some of the architectures derived from it as our baselines and show that they fail to generate good summaries in a multi-document setting. We further illustrate that our proposed model achieves significant improvement over the baselines in both single and multi-document summarization settings -- in the former setting, it beats the best baseline by 1.31 and 7.8 ROUGE-1 points on CNN and CQA datasets, respectively; in the latter setting, the performance is further improved by 1.6 ROUGE-1 points on the CQA dataset. ",Neural Abstractive Summarization with Structural Attention,2,"['Check out our new paper ""Neural Abstractive Summarization with Structural Attention"" now accepted at #IJCAI2020.\n\nWork in collaboration with @shocheen and @Tanmoy_Chak, from when I was a part of @lcs2iiitd\n\nPaper Link : <LINK> <LINK>', 'We present a hierarchical encoder, based on structural attention, to model complex inter-sentence and document dependencies. We do so by implicitly modelling documents as non projective parse trees within end-to-end training. We see significant gains on unstructured MDS corpora.']",20,04,511
30,42,894951186939338752,38637367,Debidatta Dwibedi,"Want to detect objects but don't have bounding box annotations,check out our new paper:<LINK> #DeepLearning #computervision <LINK> Cut out objects, paste them in real scenes with different modes of blending and train an object detector on these synthetic images. <LINK> Important takeaways: 1. Synthesized scenes need not look 'realistic' 2. Randomization is essential for effective data augmentation <LINK> 3. Adding synthetic data helps: i) Adds complementary information to existing real images ii) Model needs fewer annotated real images",https://arxiv.org/abs/1708.01642,"A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10% real data outperforms models trained on all real data. ","Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection",4,"[""Want to detect objects but don't have bounding box annotations,check out our new paper:<LINK> #DeepLearning #computervision <LINK>"", 'Cut out objects, paste them in real scenes with different modes of blending and train an object detector on these synthetic images. https://t.co/mjHB0NjbsN', ""Important takeaways: 1. Synthesized scenes need not look 'realistic' 2. Randomization is essential for effective data augmentation https://t.co/hH9PBJZqca"", '3. Adding synthetic data helps: i) Adds complementary information to existing real images ii) Model needs fewer annotated real images']",17,08,541
31,79,1306626047383924737,82497649,Moin Nadeem,"New AACL paper! Sampling from a language model is a crucial task for generation. While many sampling algorithms exist, what properties are desirable in a good sampling algorithm? 🧐 Joint work w/ @TianxingH, @kchonyc, Jim Glass, and I Paper: <LINK> Thread 👇 What makes the current sampling algorithms (top-k, nucleus, tempered) perform well?  We inspected them and extracted three shared properties. All algs reduce entropy of the distribution, preserve the relative order of the logits, and preserve the “slope” of the distribution. <LINK> We design two algorithms that satisfy these properties, and three algorithms that violate these properties.  Most interestingly, we find that we can design new sampling algorithms that satisfy these properties, and obtain competitive performance 😲 <LINK> Conversely, we find that two of the three algorithms that violate these properties yields significant performance degradation.  For our random masked algorithm, we were surprised to see that randomly masking logits (other than the first) yields similar performance! 🤔 <LINK> We acknowledge the empirical limitations of our study, and emphasize that it is entirely possible for some crucial property that we have not discovered to exist! We are hopeful that this study may help guide the development of novel sampling algorithms in the future.",https://arxiv.org/abs/2009.07243,"This work studies the widely adopted ancestral sampling algorithms for auto-regressive language models, which is not widely studied in the literature. We use the quality-diversity (Q-D) trade-off to investigate three popular sampling algorithms (top-k, nucleus and tempered sampling). We focus on the task of open-ended language generation. We first show that the existing sampling algorithms have similar performance. After carefully inspecting the transformations defined by different sampling algorithms, we identify three key properties that are shared among them: entropy reduction, order preservation, and slope preservation. To validate the importance of the identified properties, we design two sets of new sampling algorithms: one set in which each algorithm satisfies all three properties, and one set in which each algorithm violates at least one of the properties. We compare their performance with existing sampling algorithms, and find that violating the identified properties could lead to drastic performance degradation, as measured by the Q-D trade-off. On the other hand, we find that the set of sampling algorithms that satisfies these properties performs on par with the existing sampling algorithms. Our data and code are available at this https URL ","A Systematic Characterization of Sampling Algorithms for Open-ended
  Language Generation",5,"['New AACL paper!\n\nSampling from a language model is a crucial task for generation. While many sampling algorithms exist, what properties are desirable in a good sampling algorithm? 🧐\n\nJoint work w/ @TianxingH, @kchonyc, Jim Glass, and I\nPaper: <LINK>\nThread 👇', 'What makes the current sampling algorithms (top-k, nucleus, tempered) perform well? \n\nWe inspected them and extracted three shared properties. All algs reduce entropy of the distribution, preserve the relative order of the logits, and preserve the “slope” of the distribution. https://t.co/vMct1pdl8s', 'We design two algorithms that satisfy these properties, and three algorithms that violate these properties. \n\nMost interestingly, we find that we can design new sampling algorithms that satisfy these properties, and obtain competitive performance 😲 https://t.co/FsuhW7eXV0', 'Conversely, we find that two of the three algorithms that violate these properties yields significant performance degradation. \n\nFor our random masked algorithm, we were surprised to see that randomly masking logits (other than the first) yields similar performance! 🤔 https://t.co/0ZoMOl6d4G', 'We acknowledge the empirical limitations of our study, and emphasize that it is entirely possible for some crucial property that we have not discovered to exist!\n\nWe are hopeful that this study may help guide the development of novel sampling algorithms in the future.']",20,09,1337
32,98,1146459699841134595,2445765961,Macartan Humphreys,"Happy to share a new working paper with Philip Dawid and Monica Musio on when you can use information on causal processes to make claims about ""causes of effects""  (is Y due to X? / process tracing / causal attribution) <LINK> Highlights: 1/n * Experiments focus on treatment effects but we often care about whether an outcome is due to a cause * That's a harder question and answer usually not identified by experimental data * But don't obsess about identification: you can learn lots even if estimands are not identified * Knowledge of mediation processes can tighten bounds even if you cannot observe the mediators. But: 1 Getting arbitrarily ""close"" to a causal process does not render causal effects observable 2 Process data better for disconfirming causal relations than for confirming them 3. Max learning arises from short processes (when X is a necessary condition for a sufficient condit'n for Y) 4. Understanding conditional fx can tighten bounds more than knowledge of mediation Reminder for me how much to learn working with great people outside your discipline",https://arxiv.org/abs/1907.00399,"Suppose X and Y are binary exposure and outcome variables, and we have full knowledge of the distribution of Y, given application of X. From this we know the average causal effect of X on Y. We are now interested in assessing, for a case that was exposed and exhibited a positive outcome, whether it was the exposure that caused the outcome. The relevant ""probability of causation"", PC, typically is not identified by the distribution of Y given X, but bounds can be placed on it, and these bounds can be improved if we have further information about the causal process. Here we consider cases where we know the probabilistic structure for a sequence of complete mediators between X and Y. We derive a general formula for calculating bounds on PC for any pattern of data on the mediators (including the case with no data). We show that the largest and smallest upper and lower bounds that can result from any complete mediation process can be obtained in processes with at most two steps. We also consider homogeneous processes with many mediators. PC can sometimes be identified as 0 with negative data, but it cannot be identified at 1 even with positive data on an infinite set of mediators. The results have implications for learning about causation from knowledge of general processes and of data on cases. ",Bounding Causes of Effects with Mediators,4,"['Happy to share a new working paper with Philip Dawid and Monica Musio on when you can use information on causal processes to make claims about ""causes of effects"" \n\n(is Y due to X? / process tracing / causal attribution)\n\n<LINK>\n\nHighlights:\n\n1/n', ""* Experiments focus on treatment effects but we often care about whether an outcome is due to a cause\n* That's a harder question and answer usually not identified by experimental data\n* But don't obsess about identification: you can learn lots even if estimands are not identified"", '* Knowledge of mediation processes can tighten bounds even if you cannot observe the mediators.\n\nBut:\n\n1 Getting arbitrarily ""close"" to a causal process does not render causal effects observable\n\n2 Process data better for disconfirming causal relations than for confirming them', ""3. Max learning arises from short processes (when X is a necessary condition for a sufficient condit'n for Y)\n\n4. Understanding conditional fx can tighten bounds more than knowledge of mediation\n\nReminder for me how much to learn working with great people outside your discipline""]",19,07,1076
33,20,1067599877813735425,2728209696,Xiao Ma,"We have a new paper on #trust! - Understanding Image Quality and Trust in Peer-to-Peer Marketplaces Our models predict image quality in online #marketplaces, and we show that high-quality user-generated images outperform stock imagery for generating trust. <LINK> <LINK> In collaboration with Lina Mezghani @Polytechnique, Kimberly Wilber @GoogleAI, Hui Hong and Robinson Piramuthu @eBay, and @informor and @SergeBelongie @cornell_tech @CornellInfoSci. The paper will be presented at #WACV19 <LINK>. See you in Hawaii! &lt;3",https://arxiv.org/abs/1811.10648,"As any savvy online shopper knows, second-hand peer-to-peer marketplaces are filled with images of mixed quality. How does image quality impact marketplace outcomes, and can quality be automatically predicted? In this work, we conducted a large-scale study on the quality of user-generated images in peer-to-peer marketplaces. By gathering a dataset of common second-hand products (~75,000 images) and annotating a subset with human-labeled quality judgments, we were able to model and predict image quality with decent accuracy (~87%). We then conducted two studies focused on understanding the relationship between these image quality scores and two marketplace outcomes: sales and perceived trustworthiness. We show that image quality is associated with higher likelihood that an item will be sold, though other factors such as view count were better predictors of sales. Nonetheless, we show that high quality user-generated images selected by our models outperform stock imagery in eliciting perceptions of trust from users. Our findings can inform the design of future marketplaces and guide potential sellers to take better product images. ",Understanding Image Quality and Trust in Peer-to-Peer Marketplaces,2,"['We have a new paper on #trust! - Understanding Image Quality and Trust in Peer-to-Peer Marketplaces\nOur models predict image quality in online #marketplaces, and we show that high-quality user-generated images outperform stock imagery for generating trust.\n<LINK> <LINK>', 'In collaboration with Lina Mezghani @Polytechnique, Kimberly Wilber @GoogleAI, Hui Hong and Robinson Piramuthu @eBay, and @informor and @SergeBelongie @cornell_tech @CornellInfoSci. The paper will be presented at #WACV19 https://t.co/vJaVQP8WMk. See you in Hawaii! &lt;3']",18,11,524
34,58,1070702916393156610,50343115,Thomas Kipf,"CompILE discovers composable segments & encodings of behavior from sequential data (unsupervised and differentiable). Codes can be recomposed to facilitate generalization and exploration in RL. New work with collaborators from @DeepMindAI Paper: <LINK> <LINK> If you're around at #NeurIPS2018, consider stopping by at our talk at the Learning By Instruction (LBI) Workshop, this Saturday at 10:00 -- <LINK>, or ping me if you want to have a chat!",https://arxiv.org/abs/1812.01483,"We introduce Compositional Imitation Learning and Execution (CompILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. CompILE uses a novel unsupervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-composed and executed to perform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We evaluate CompILE in a challenging 2D multi-task environment and a continuous control task, and show that it can find correct task boundaries and event encodings in an unsupervised manner. Latent codes and associated behavior policies discovered by CompILE can be used by a hierarchical agent, where the high-level policy selects actions in the latent code space, and the low-level, task-specific policies are simply the learned decoders. We found that our CompILE-based agent could learn given only sparse rewards, where agents without task-specific policies struggle. ",CompILE: Compositional Imitation Learning and Execution,2,"['CompILE discovers composable segments &amp; encodings of behavior from sequential data (unsupervised and differentiable). Codes can be recomposed to facilitate generalization and exploration in RL.\n\nNew work with collaborators from @DeepMindAI\nPaper: <LINK> <LINK>', ""If you're around at #NeurIPS2018, consider stopping by at our talk at the Learning By Instruction (LBI) Workshop, this Saturday at 10:00 -- https://t.co/T90GsZDz3q, or ping me if you want to have a chat!""]",18,12,446
