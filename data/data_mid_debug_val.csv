,Unnamed: 0.1,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title,Thread_length,Tweets_coarse,year,month,tweet_length
0,163,1367118232671387648,1101220947607146497,Alon Jacovi,"Check out our new paper üòä (w\ @swabhz @ravfogel @yanaiela @YejinChoinka @yoavgo ) Contrastive Explanations for Model Interpretability <LINK> This paper is about explaining classifier decisions contrastively against alternative decisions. <LINK> Explanations are inherently contrastive. This means that even if they're not explicitly contrasted against something, as humans we assume they are. This has implications on what it means for an explanation to be understandable/intuitive to humans (see pic). <LINK> Most simply, explanations are just easier to produce and to understand for all parties if they involve an explicit contrasted, alternative decision. <LINK> In our paper we explain contrastively by a process where we intervene on some aspect of the instance, and then measure how the model changed specifically only in respect to the explained decision, and the contrast decision. <LINK> We do this by measuring contrastive changes in the model + projecting its latent representation to a contrastive space that only includes the information that is used to differentiate two decisions. We use this for many experiments on BIOS and MNLI. Details in paper üëãüëã Thanks!!",https://arxiv.org/abs/2103.01378,"Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model's decision. ",Contrastive Explanations for Model Interpretability,5,"['Check out our new paper üòä (w\\ @swabhz @ravfogel @yanaiela @YejinChoinka @yoavgo )\n\nContrastive Explanations for Model Interpretability\n<LINK>\n\nThis paper is about explaining classifier decisions contrastively against alternative decisions. <LINK>', ""Explanations are inherently contrastive. This means that even if they're not explicitly contrasted against something, as humans we assume they are. This has implications on what it means for an explanation to be understandable/intuitive to humans (see pic). https://t.co/xiqsfol19v"", 'Most simply, explanations are just easier to produce and to understand for all parties if they involve an explicit contrasted, alternative decision. https://t.co/lwWN1oL8oo', 'In our paper we explain contrastively by a process where we intervene on some aspect of the instance, and then measure how the model changed specifically only in respect to the explained decision, and the contrast decision. https://t.co/3ygt51qWMZ', 'We do this by measuring contrastive changes in the model + projecting its latent representation to a contrastive space that only includes the information that is used to differentiate two decisions. We use this for many experiments on BIOS and MNLI. Details in paper üëãüëã Thanks!!']",21,03,1175
1,77,1148594342384295937,1091815542334337024,Scott Niekum,"Want to get up to speed on what‚Äôs happening in robot learning for manipulation? Oliver Kroemer, George Konidaris, and I have just released a new survey paper: <LINK>. And please let us know if there is important work we missed ‚Äî it is a huge and growing field! @animesh_garg Yes, quite a bit longer than we‚Äôd like to admit! Thanks for your feedback in early ideas! @RezaAhmadzadeh_ That was fast! We will update it if we missed anything but this is essentially the final version that we are submitting to a journal.",http://arxiv.org/abs/1907.03146,"A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges. ","A Review of Robot Learning for Manipulation: Challenges,
  Representations, and Algorithms",3,"['Want to get up to speed on what‚Äôs  happening in robot learning for manipulation? Oliver Kroemer, George Konidaris, and I have just released a new survey paper: <LINK>. And please let us know if there is important work we missed ‚Äî it is a huge and growing field!', '@animesh_garg Yes, quite a bit longer than we‚Äôd like to admit! Thanks for your feedback in early ideas!', '@RezaAhmadzadeh_ That was fast!  We will update it if we missed anything but this is essentially the final version that we are submitting to a journal.']",19,07,515
2,99,1503728410333392896,411037433,Alessandro Perelli,"New work ""Multi-Channel Convolutional Analysis Operator Learning for Dual-Energy CT Reconstruction"" with colleagues at @LatimU1101. MCAOL is able to exploit joint information in DECT reconstruction. Check out the paper <LINK> and the code <LINK> <LINK>",http://arxiv.org/abs/2203.05968,"Objective. Dual-energy computed tomography (DECT) has the potential to improve contrast, reduce artifacts and the ability to perform material decomposition in advanced imaging applications. The increased number or measurements results with a higher radiation dose and it is therefore essential to reduce either number of projections per energy or the source X-ray intensity, but this makes tomographic reconstruction more ill-posed. Approach. We developed the multi-channel convolutional analysis operator learning (MCAOL) method to exploit common spatial features within attenuation images at different energies and we propose an optimization method which jointly reconstructs the attenuation images at low and high energies with a mixed norm regularization on the sparse features obtained by pre-trained convolutional filters through the convolutional analysis operator learning (CAOL) algorithm. Main results. Extensive experiments with simulated and real computed tomography (CT) data were performed to validate the effectiveness of the proposed methods and we reported increased reconstruction accuracy compared to CAOL and iterative methods with single and joint total-variation (TV) regularization. Significance. Qualitative and quantitative results on sparse-views and low-dose DECT demonstrate that the proposed MCAOL method outperforms both CAOL applied on each energy independently and several existing state-of-the-art model-based iterative reconstruction (MBIR) techniques, thus paving the way for dose reduction. ","Multi-Channel Convolutional Analysis Operator Learning for Dual-Energy
  CT Reconstruction",1,"['New work ""Multi-Channel Convolutional Analysis Operator Learning for Dual-Energy CT Reconstruction"" with colleagues at @LatimU1101. MCAOL is able to exploit joint information in DECT reconstruction.\n\nCheck out the paper <LINK>\nand the code <LINK> <LINK>']",22,03,252
3,26,1453596795544563716,1926797899,Sagnik Chatterjee,"Yesterday we @BQiiit (my advisor Debajyoti Bera, and I) uploaded a new paper on arxiv on Quantum Boosting using Domain-Partitioning hypotheses. <LINK> <LINK> We give an adaptive boosting algorithm for converting a weak quantum PAC learner into a strong quantum PAC learner while achieving a speedup (in the VC dimension) over classical boosting algorithms. In our work, we provide provable guarantees for the Quantum RealBoost algorithm and also lay the foundations for quantizing an entire family of AdaBoost variants such as GentleBoost, ModestBoost, etc.",https://arxiv.org/abs/2110.12793,"Boosting is an ensemble learning method that converts a weak learner into a strong learner in the PAC learning framework. Freund and Schapire gave the first classical boosting algorithm for binary hypothesis known as AdaBoost, and this was recently adapted into a quantum boosting algorithm by Arunachalam et al. Their quantum boosting algorithm (which we refer to as Q-AdaBoost) is quadratically faster than the classical version in terms of the VC-dimension of the hypothesis class of the weak learner but polynomially worse in the bias of the weak learner. In this work we design a different quantum boosting algorithm that uses domain partitioning hypotheses that are significantly more flexible than those used in prior quantum boosting algorithms in terms of margin calculations. Our algorithm Q-RealBoost is inspired by the ""Real AdaBoost"" (aka. RealBoost) extension to the original AdaBoost algorithm. Further, we show that Q-RealBoost provides a polynomial speedup over Q-AdaBoost in terms of both the bias of the weak learner and the time taken by the weak learner to learn the target concept class. ",Quantum Boosting using Domain-Partitioning Hypotheses,3,"['Yesterday we @BQiiit  (my advisor Debajyoti Bera, and I) uploaded a new paper on arxiv on Quantum Boosting using Domain-Partitioning hypotheses.\n<LINK> <LINK>', 'We give an adaptive boosting algorithm for converting a weak quantum PAC learner into a strong quantum PAC learner while achieving a speedup (in the VC dimension) over classical boosting algorithms.', 'In our work, we provide provable guarantees for the Quantum RealBoost algorithm and also lay the foundations for quantizing an entire family of AdaBoost variants such as GentleBoost, ModestBoost, etc.']",21,10,557
4,7,1488421960824537089,948528424926220288,Johannes R√∏sok Eskilt,"Excited that my new paper is out! <LINK> I'm extremely grateful for the help I've received from so many people! In the paper, I look at the frequency dependence of the cosmic birefringence signal found in Planck data, and I include the LFI for the first time I sample the birefringence angle individually for each frequency channel, and I sample a power-law model. I get that the signal is very consistent with being frequency-independent, which rules out Faraday rotation as the cause of the signal <LINK> But more work needs to be done to understand the physics of the polarized foreground emission, both dust and synchrotron, before we can know the statistical significance of the measurements",https://arxiv.org/abs/2201.13347,"We present new constraints on the frequency dependence of the cosmic birefringence angle from the Planck data release 4 polarization maps. An axion field coupled to electromagnetism predicts a nearly frequency-independent birefringence angle, $\beta_\nu = \beta$, while Faraday rotation from local magnetic fields and Lorentz violating theories predict a cosmic birefringence angle that is proportional to the frequency, $\nu$, to the power of some integer $n$, $\beta_\nu \propto \nu^n$. In this work, we first sample $\beta_\nu$ individually for each polarized HFI frequency band in addition to the 70 GHz channel from the LFI. We also constrain a power-law formula for the birefringence angle, $\beta_\nu=\beta_0(\nu/\nu_0)^n$, with $\nu_0 = 150$ GHz. For a nearly full-sky measurement, $f_{\text{sky}}=0.93$, we find $\beta_0 = 0.26^{\circ}\pm0.11^\circ$ $(68\% \text{ C.L.})$ and $n=-0.45^{+0.61}_{-0.82}$ when we ignore the intrinsic $EB$ correlations of the polarized foreground emission, and $\beta_0 = 0.33^\circ \pm 0.12^\circ$ and $n=-0.37^{+0.49}_{-0.64}$ when we use a filamentary dust model for the foreground $EB$. Next, we use all the polarized Planck maps, including the 30 and 44 GHz frequency bands. These bands have a negligible foreground contribution from polarized dust emission. We, therefore, treat them separately. Without any modeling of the intrinsic $EB$ of the foreground, we generally find that the inclusion of the 30 and 44 GHz frequency bands raises the measured values of $\beta_\nu$ and tightens $n$. At nearly full-sky, we measure $\beta_0=0.29^{\circ+0.10^\circ}_{\phantom{\circ}-0.11^\circ}$ and $n=-0.35^{+0.48}_{-0.47}$. Assuming no frequency dependence, we measure $\beta=0.33^\circ \pm 0.10^\circ$. If our measurements have effectively mitigated the $EB$ of the foreground, our constraints are consistent with a mostly frequency-independent signal of cosmic birefringence. ","Frequency-Dependent Constraints on Cosmic Birefringence from the LFI and
  HFI Planck Data Release 4",3,"[""Excited that my new paper is out! <LINK> I'm extremely grateful for the help I've received from so many people! In the paper, I look at the frequency dependence of the cosmic birefringence signal found in Planck data, and I include the LFI for the first time"", 'I sample the birefringence angle individually for each frequency channel, and I sample a power-law model. I get that the signal is very consistent with being frequency-independent, which rules out Faraday rotation as the cause of the signal https://t.co/ZvA485jHfY', 'But more work needs to be done to understand the physics of the polarized foreground emission, both dust and synchrotron, before we can know the statistical significance of the measurements']",22,01,696
5,122,1292854888968474625,373525906,Weijie Su,"New interpretation of the *double descent* phenomenon: noise in features is ubiquitous, and we show using a random feature model that noise can lead to benign overfitting. Paper: <LINK>. w/ Zhu Li and Dino Sejdinovic. <LINK> @2prime_PKU Thanks for the reference! Will look into it. @roydanroy @jeffNegrea @KDziugaite Thanks for the reference. Very related",https://arxiv.org/abs/2008.02901,"Modern machine learning often operates in the regime where the number of parameters is much higher than the number of data points, with zero training loss and yet good generalization, thereby contradicting the classical bias-variance trade-off. This \textit{benign overfitting} phenomenon has recently been characterized using so called \textit{double descent} curves where the risk undergoes another descent (in addition to the classical U-shaped learning curve when the number of parameters is small) as we increase the number of parameters beyond a certain threshold. In this paper, we examine the conditions under which \textit{Benign Overfitting} occurs in the random feature (RF) models, i.e. in a two-layer neural network with fixed first layer weights. We adopt a new view of random feature and show that \textit{benign overfitting} arises due to the noise which resides in such features (the noise may already be present in the data and propagate to the features or it may be added by the user to the features directly) and plays an important implicit regularization role in the phenomenon. ",Benign Overfitting and Noisy Features,3,"['New interpretation of the *double descent* phenomenon: noise in features is ubiquitous, and we show using a random feature model that noise can lead to benign overfitting. Paper: <LINK>. w/ Zhu Li and Dino Sejdinovic. <LINK>', '@2prime_PKU Thanks for the reference! Will look into it.', '@roydanroy @jeffNegrea @KDziugaite Thanks for the reference. Very related']",20,08,355
6,312,1316658865157570560,750911820,Pablo Ouro,"In <LINK> we studied the scalability and performance of our CFD code on three HPC facilities: @Arm ThunderX2 @GW4Alliance Isambard, @AMD EPYC-Rome & @intelhpc Skylake @SuperCompWales . EPYC delivers best performance, SKL achieves best scalability& TX2 doing well! This is a collaboration with @ULopezNovoa from @upvehu and Martyn Guest from @ARCCACardiffUni @cardiffuni, and partly funded by @GW4Alliance and @EPSRC . Thanks to @simonmcs, @adefewins, @hpcchris, Ade Fewings and @zaptohome for their support and help!",https://arxiv.org/abs/2010.07111v1,"No area of computing is hungrier for performance than High Performance Computing (HPC), the demands of which continue to be a major driver for processor performance and adoption of accelerators, and also advances in memory, storage, and networking technologies. A key feature of the Intel processor domination of the past decade has been the extensive adoption of GPUs as coprocessors, whilst more recent developments have seen the increased availability of a number of CPU processors, including the novel ARM-based chips. This paper analyses the performance and scalability of a state-of-the-art Computational Fluid Dynamics (CFD) code on three HPC cluster systems equipped with AMD EPYC-Rome (EPYC, 4096 cores), ARM-based Marvell ThunderX2 (TX2, 8192 cores) and Intel Skylake (SKL, 8000 cores) processors. Three benchmark cases are designed with increasing computation-to-communication ratio and numerical complexity, namely lid-driven cavity flow, Taylor-Green vortex and a travelling solitary wave using the level-set method, adopted with $4^{th}$-order central-differences or a $5^{th}$-order WENO scheme. Our results show that the EPYC cluster delivers the best code performance for all the setups under consideration. In the first two benchmarks, the SKL cluster demonstrates faster computing times than the TX2 system, whilst in the solitary wave simulations, the TX2 cluster achieves good scalability and similar performance to the EPYC system, both improving on that obtained with the SKL cluster. These results suggest that while the Intel SKL cores deliver the best strong scalability, the associated cluster performance is lower compared to the EPYC system. The TX2 cluster performance is promising considering its recent addition to the HPC portfolio. ","] On the performance of a highly-scalable Computational Fluid Dynamics
  code on AMD, ARM and Intel processors",2,"['In <LINK> we studied the scalability and performance of our CFD code on three HPC facilities: @Arm ThunderX2 @GW4Alliance Isambard, @AMD EPYC-Rome &amp; @intelhpc Skylake @SuperCompWales . EPYC delivers best performance, SKL achieves best scalability&amp; TX2 doing well!', 'This is a collaboration with @ULopezNovoa from @upvehu and Martyn Guest from @ARCCACardiffUni @cardiffuni, and partly funded by @GW4Alliance and @EPSRC . Thanks to @simonmcs, @adefewins, @hpcchris, Ade Fewings and @zaptohome for their support and help!']",20,10,516
7,100,1258623604062695426,1006948726827507712,Seung-won Park,"We're excited to present a new approach for Voice Conversion: use alignment from pre-trained TTS. ""Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data"" paper: <LINK> audio samples: <LINK> (1/6) By teacher-forcing the mel spec. into the pre-trained multispeaker TTS model, the text-audio alignment can be obtained. Then, matmul of (1) text encoding and (2) text-audio alignment gives us the speaker-independent linguistic features. Let's call it Cotatron features. (2/6) <LINK> Similar to previous methods, we train a decoder to reconstruct the mel spec. from the Cotatron features (L) and a speaker representation (y^id, *=s). If the target speaker representation is fed (*=t), then we get the conversion result. A residual encoder is optional. (3/6) <LINK> Such a simple idea turns out to be very effective. Our MOS (naturalness) and DMOS (speaker similarity) result significantly outperform the previous method, Blow, when trained and evaluated with 108 speakers of the VCTK dataset. (4/6) <LINK> Our model can also: - convert speech from arbitrary speakers (even Charlie Chaplin's speech!), - utilize ASR to automate the transcription process. (5/6) <LINK> We expect that the Cotatron features can be used for: - lip motion synthesis, - any speech task that can benefit from using the transcription. We'll be releasing the code of Cotatron near InterSpeech 2020; please consider applying our idea to your task! Thank you. (6/6) One of our impressive conversion result here: (also available at our demo website) We converted one of the famous memes (?) in South Korea into Korean female speaker's speech. (KSS dataset) ""Ïò®Í∞ñ ÏùåÌï¥Ïóê ÏãúÎã¨Î†∏ÏäµÎãàÎã§. Ïó¨Îü¨Î∂Ñ Ïù¥Í±∞ Îã§ Í±∞ÏßìÎßêÏù∏Í±∞ ÏïÑÏãúÏ£†?"" <LINK>",https://arxiv.org/abs/2005.03295,"We propose Cotatron, a transcription-guided speech encoder for speaker-independent linguistic representation. Cotatron is based on the multispeaker TTS architecture and can be trained with conventional TTS datasets. We train a voice conversion system to reconstruct speech with Cotatron features, which is similar to the previous methods based on Phonetic Posteriorgram (PPG). By training and evaluating our system with 108 speakers from the VCTK dataset, we outperform the previous method in terms of both naturalness and speaker similarity. Our system can also convert speech from speakers that are unseen during training, and utilize ASR to automate the transcription with minimal reduction of the performance. Audio samples are available at this https URL, and the code with a pre-trained model will be made available soon. ","Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice
  Conversion without Parallel Data",7,"['We\'re excited to present a new approach for Voice Conversion: use alignment from pre-trained TTS.\n\n""Cotatron: Transcription-Guided Speech Encoder for Any-to-Many Voice Conversion without Parallel Data""\npaper: <LINK>\naudio samples: <LINK>\n\n(1/6)', ""By teacher-forcing the mel spec. into the pre-trained multispeaker TTS model, the text-audio alignment can be obtained. Then, matmul of\n(1) text encoding and (2) text-audio alignment \ngives us the speaker-independent linguistic features. Let's call it Cotatron features.\n\n(2/6) https://t.co/caF4aJm0Cv"", 'Similar to previous methods, we train a decoder to reconstruct the mel spec. from the Cotatron features (L) and a speaker representation (y^id, *=s).\n\nIf the target speaker representation is fed (*=t), then we get the conversion result. A residual encoder is optional.\n\n(3/6) https://t.co/5jcGWnYG4u', 'Such a simple idea turns out to be very effective.\nOur MOS (naturalness) and DMOS (speaker similarity) result significantly outperform the previous method, Blow, when trained and evaluated with 108 speakers of the VCTK dataset.\n\n(4/6) https://t.co/ZR5qL7l1tP', ""Our model can also:\n- convert speech from arbitrary speakers (even Charlie Chaplin's speech!),\n- utilize ASR to automate the transcription process.\n\n(5/6) https://t.co/C6Cpgy7s8L"", ""We expect that the Cotatron features can be used for:\n- lip motion synthesis,\n- any speech task that can benefit from using the transcription.\n\nWe'll be releasing the code of Cotatron near InterSpeech 2020; please consider applying our idea to your task! Thank you.\n\n(6/6)"", 'One of our impressive conversion result here:\n(also available at our demo website)\n\nWe converted one of the famous memes (?) in South Korea into Korean female speaker\'s speech. (KSS dataset)\n\n""Ïò®Í∞ñ ÏùåÌï¥Ïóê ÏãúÎã¨Î†∏ÏäµÎãàÎã§. Ïó¨Îü¨Î∂Ñ Ïù¥Í±∞ Îã§ Í±∞ÏßìÎßêÏù∏Í±∞ ÏïÑÏãúÏ£†?""\n\nhttps://t.co/7tV4R0PPGs']",20,05,1713
8,102,1135848920511320064,590944541,Gavin Gray,"Evidence in our new paper that there are better ways to do deep learning than matrix multiplication with a weight matrix: <LINK> tl;dr FC layers (and therefore pointwise convolutions) have many proposed efficient alternatives, and we show that they many work in large CNNs with a weight decay fix. What does this mean? Right now, to make a more efficient model use grouped pointwise convolutions, riffle shuffles and scale the weight decay in each layer by the compression factor. In future, architecture search has an entire new space to explore.",https://arxiv.org/abs/1906.00859,"In response to the development of recent efficient dense layers, this paper shows that something as simple as replacing linear components in pointwise convolutions with structured linear decompositions also produces substantial gains in the efficiency/accuracy tradeoff. Pointwise convolutions are fully connected layers and are thus prepared for replacement by structured transforms. Networks using such layers are able to learn the same tasks as those using standard convolutions, and provide Pareto-optimal benefits in efficiency/accuracy, both in terms of computation (mult-adds) and parameter count (and hence memory). Code is available at this https URL ",Separable Layers Enable Structured Efficient Linear Substitutions,3,"['Evidence in our new paper that there are better ways to do deep learning than matrix multiplication with a weight matrix: <LINK>', 'tl;dr FC layers (and therefore pointwise convolutions) have many proposed efficient alternatives, and we show that they many work in large CNNs with a weight decay fix.', 'What does this mean? Right now, to make a more efficient model use grouped pointwise convolutions, riffle shuffles and scale the weight decay in each layer by the compression factor. In future, architecture search has an entire new space to explore.']",19,06,547
9,50,1321459591809519622,720027280051957761,Oliver Newton,"I‚Äôve had my head buried in code bugs, so I nearly missed the chance to promote a new paper that I collaborated on which came out today! Wolfgang Enzi led on a joint analysis of several methods to tighten constraints on thermal relic dark matter models <LINK> 1/6 Enzi adopts a Bayesian approach to combine studies of gravitational lensing, the Lyman-alpha forest and MW satellite galaxies. From this, he obtains a lower limit on the thermal relic particle mass of 6.733 keV at 95 per cent confidence. 2/6 <LINK> This also rules out 7.1 keV sterile neutrino dark matter models for large values of the lepton asymmetry parameter, which is particularly interesting as it is one of the models proposed to explain the 3.55 keV excess observed in X-ray spectra of DM-dominated objects. 3/6 How can we improve the results further? Currently, the MW satellites and Lyman-alpha forest provide the strongest constraints on the half mode mass; however, both analyses are subject to assumptions about galaxy formation and feedback processes. 4/6 Other uncertainties peculiar to each approach also creep in (e.g. the MW mass affects the satellite galaxy luminosity function; the choice of IGM thermal history can make the Lyman-alpha forest consistent with both CDM and WDM models). So, plenty of areas for improvement! 5/6 It's a great piece of work and I encourage anyone interested in astrophysical constraints on DM models to have a read! 6/6",https://arxiv.org/abs/2010.13802,"We derive joint constraints on the warm dark matter (WDM) half-mode scale by combining the analyses of a selection of astrophysical probes: strong gravitational lensing with extended sources, the Lyman-$\alpha$ forest, and the number of luminous satellites in the Milky Way. We derive an upper limit of $\lambda_{\rm hm}=0.089{\rm~Mpc~h^{-1} }$ at the 95 per cent confidence level, which we show to be stable for a broad range of prior choices. Assuming a Planck cosmology and that WDM particles are thermal relics, this corresponds to an upper limit on the half-mode mass of $M_{\rm hm }< 3 \times 10^{7} {\rm~M_{\odot}~h^{-1}}$, and a lower limit on the particle mass of $m_{\rm th }> 6.048 {\rm~keV}$, both at the 95 per cent confidence level. We find that models with $\lambda_{\rm hm}> 0.223 {\rm~Mpc~h^{-1} }$ (corresponding to $m_{\rm th }> 2.552 {\rm~keV}$ and $M_{\rm hm }< 4.8 \times 10^{8} {\rm~M_{\odot}~h^{-1}}$) are ruled out with respect to the maximum likelihood model by a factor $\leq 1/20$. For lepton asymmetries $L_6>10$, we rule out the $7.1 {\rm~keV}$ sterile neutrino dark matter model, which presents a possible explanation to the unidentified $3.55 {\rm~keV}$ line in the Milky Way and clusters of galaxies. The inferred 95 percentiles suggest that we further rule out the ETHOS-4 model of self-interacting DM. Our results highlight the importance of extending the current constraints to lower half-mode scales. We address important sources of systematic errors and provide prospects for how the constraints of these probes can be improved upon in the future. ","Joint constraints on thermal relic dark matter from strong gravitational
  lensing, the Lyman-$\alpha$ forest, and Milky Way satellites",6,"['I‚Äôve had my head buried in code bugs, so I nearly missed the chance to promote a new paper that I collaborated on which came out today! Wolfgang Enzi led on a joint analysis of several methods to tighten constraints on thermal relic dark matter models\n<LINK>\n\n1/6', 'Enzi adopts a Bayesian approach to combine studies of gravitational lensing, the Lyman-alpha forest and MW satellite galaxies. From this, he obtains a lower limit on the thermal relic particle mass of 6.733 keV at 95 per cent confidence.\n\n2/6 https://t.co/feSkZCHkzK', 'This also rules out 7.1 keV sterile neutrino dark matter models for large values of the lepton asymmetry parameter, which is particularly interesting as it is one of the models proposed to explain the 3.55 keV excess observed in X-ray spectra of DM-dominated objects.\n\n3/6', 'How can we improve the results further? Currently, the MW satellites and Lyman-alpha forest provide the strongest constraints on the half mode mass; however, both analyses are subject to assumptions about galaxy formation and feedback processes.\n\n4/6', 'Other uncertainties peculiar to each approach also creep in (e.g. the MW mass affects the satellite galaxy luminosity function; the choice of IGM thermal history can make the Lyman-alpha forest consistent with both CDM and WDM models). So, plenty of areas for improvement!\n\n5/6', ""It's a great piece of work and I encourage anyone interested in astrophysical constraints on DM models to have a read!\n\n6/6""]",20,10,1433
10,146,1271325739107770369,2563532985,Prof Anna Watts,"New group paper! Waves in thin oceans on oblate neutron stars, by van Baal, Chambers & Watts, MNRAS in press <LINK> We study how the different families of modes that exist in neutron star oceans are affected by rotationally-induced oblateness (in Newtonian gravity). This is an important factor for mode models that seek to explain thermonuclear burst oscillations. Along with nuclear burning, relativity, convection, flame spread, etc etc etc! And is therefore yet one more thing that we have to include in our models. <LINK> [One of the mode types we look at are Yanai waves. Interestingly meteorologist Michio Yanai doesn't seen to have a Wikipedia page. But UCLA have an annual Yanai Lecture, and you can read his bio and see some pics there. <LINK> ]",https://arxiv.org/abs/2006.06382,"Waves in thin fluid layers are important in various stellar and planetary problems. Due to rapid rotation such systems will become oblate, with a latitudinal variation in the gravitational acceleration across the surface of the object. In the case of accreting neutron stars, rapid rotation could lead to a polar radius smaller than the equatorial radius by a factor $\sim 0.8$. We investigate how the oblateness and a changing gravitational acceleration affect different hydrodynamic modes that exist in such fluid layers through analytic approximations and numerical calculations. The wave vectors of $g$-modes and Yanai modes increase for more oblate systems compared to spherical counterparts, although the impact of variations in the changing gravitational acceleration is effectively negligible. We find that for increased oblateness, Kelvin modes show less equatorial confinement and little change in their wave vector. For $r$-modes, we find that for more oblate systems the wave vector decreases. The exact manner of these changes for the $r$-modes depends on the model for the gravitational acceleration across the surface. ",Waves in Thin Oceans on Oblate Neutron Stars,4,"['New group paper! Waves in thin oceans on oblate neutron stars, by van Baal, Chambers &amp; Watts, MNRAS in press <LINK>', 'We study how the different families of modes that exist in neutron star oceans are affected by rotationally-induced oblateness (in Newtonian gravity).  This is an important factor for mode models that seek to explain thermonuclear burst oscillations.', 'Along with nuclear burning, relativity, convection, flame spread, etc etc etc!  And is therefore yet one more thing that we have to include in our models. https://t.co/UxLMXPrVJG', ""[One of the mode types we look at are Yanai waves. Interestingly meteorologist Michio Yanai doesn't seen to have a Wikipedia page. But UCLA have an annual Yanai Lecture, and you can read his bio and see some pics there. https://t.co/onE9Gp2msH  ]""]",20,06,755
11,137,1317129255973654528,2432031889,Ana Marasoviƒá,"üì¢ New at Findings #EMNLP2020 üì¢  ""Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs"" w/ @_csBhagav @jae_sung_park96 @Ronan_LeBras @nlpnoah @YejinChoinka üìñ Paper: <LINK> Thread üëá <LINK> Why natural language rationales? Explaining higher-level conceptual reasoning cannot be well conveyed *only* by attributing individual pixels or words---the cause behind prediction is often *not* explicitly grounded in the input (""she doesn‚Äôt know whose order is whose"") 1/ The key challenge of visual-textual reasoning rationalization is image understanding beyond explicit content (highlighting objects): understanding contextual content like the relations among objects through action predicates (semantics) and the action's intent (pragmatics) 2/ We combine GPT-2 with object recognition, grounded visual semantic frames, and commonsense inferences inferred from an image and an optional event predicted from a visual commonsense graph 3/ <LINK> GPT-2 benefits from visual adaptation across complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering; adapted models generate more plausible rationales that are less likely to mention content irrelevant to an image 4/ <LINK> Our best performing models for visual commonsense reasoning and visual-textual entailment are still notably behind human-written rationales showing that free-text rationalization remains a challenging task despite our improvements 5/ I'm really excited about this work and the numerous open questions.  Can natural language rationales be used to persuade users? Yes, don't generate rationales independently after the prediction. We need evaluations of association btw rationale generation and label prediction 7/ Is generation of natural language rationales possible only with human-written rationales? I don't believe so. Recent related work that uses weak supervision is a promising direction, but we need more exploration there 8/ <LINK> Are there alternatives to human evaluation of plausibility? Maybe. BLEU and co. are not suitable, but someone should investigate newly emerging *learned* evaluation measures such as BLEURT 9/ And I'm sure there are many more. Feel free to reach out! 10/10 :)",http://arxiv.org/abs/2010.07526,"Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks. ","Natural Language Rationales with Full-Stack Visual Reasoning: From
  Pixels to Semantic Frames to Commonsense Graphs",10,"['üì¢ New at Findings #EMNLP2020 üì¢ \n\n""Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs""\n\nw/ @_csBhagav @jae_sung_park96 @Ronan_LeBras @nlpnoah @YejinChoinka\n\nüìñ Paper: <LINK>\n\nThread üëá <LINK>', 'Why natural language rationales?\n\nExplaining higher-level conceptual reasoning cannot be well conveyed *only* by attributing individual pixels or words---the cause behind prediction is often *not* explicitly grounded in the input (""she doesn‚Äôt know whose order is whose"") 1/', ""The key challenge of visual-textual reasoning rationalization is image understanding beyond explicit content (highlighting objects): understanding contextual content like the relations among objects through action predicates (semantics) and the action's intent (pragmatics) 2/"", 'We combine GPT-2 with object recognition, grounded visual semantic frames, and commonsense inferences inferred from an image and an optional event predicted from a visual commonsense graph 3/ https://t.co/U8eND0geM1', 'GPT-2 benefits from visual adaptation across complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering; adapted models generate more plausible rationales that are less likely to mention content irrelevant to an image 4/ https://t.co/HwKFIoHq3A', 'Our best performing models for visual commonsense reasoning and visual-textual entailment are still notably behind human-written rationales showing that free-text rationalization remains a challenging task despite our improvements 5/', ""I'm really excited about this work and the numerous open questions. \n\nCan natural language rationales be used to persuade users? Yes, don't generate rationales independently after the prediction. We need evaluations of association btw rationale generation and label prediction 7/"", ""Is generation of natural language rationales possible only with human-written rationales? I don't believe so. Recent related work that uses weak supervision is a promising direction, but we need more exploration there  8/ https://t.co/JkHLhvPTp3"", 'Are there alternatives to human evaluation of plausibility? Maybe. BLEU and co. are not suitable, but someone should investigate newly emerging *learned* evaluation measures such as BLEURT 9/', ""And I'm sure there are many more. Feel free to reach out! 10/10 :)""]",20,10,2286
12,113,1370563872168374275,1665897810,Kevin Frans,"New paper on how Population-based Evolution is a natural meta-learning algorithm! With @okw at @crosslabstokyo <LINK> @okw @crosslabstokyo Basic idea: In evolutionary algorithms, a strong gene is a gene which survives for many generations. Thus, strong genes should grant fitness not only to an individual but also to all of the individual's offspring. This means that in non-stationary environments, genes that increase the *adaptive ability* of a genome are naturally selected for. Over time, the learning ability of a population increases. This perspective can help explain why biological systems are so adaptable, or why languages are so robust -- with large populations, evolution naturally selects for systems that are good at adapting to new tasks.",https://arxiv.org/abs/2103.06435,"Meta-learning models, or models that learn to learn, have been a long-desired target for their ability to quickly solve new tasks. Traditional meta-learning methods can require expensive inner and outer loops, thus there is demand for algorithms that discover strong learners without explicitly searching for them. We draw parallels to the study of evolvable genomes in evolutionary systems -- genomes with a strong capacity to adapt -- and propose that meta-learning and adaptive evolvability optimize for the same objective: high performance after a set of learning iterations. We argue that population-based evolutionary systems with non-static fitness landscapes naturally bias towards high-evolvability genomes, and therefore optimize for populations with strong learning ability. We demonstrate this claim with a simple evolutionary algorithm, Population-Based Meta Learning (PBML), that consistently discovers genomes which display higher rates of improvement over generations, and can rapidly adapt to solve sparse fitness and robotic control tasks. ",Population-Based Evolution Optimizes a Meta-Learning Objective,4,"['New paper on how Population-based Evolution is a natural meta-learning algorithm! With @okw at @crosslabstokyo <LINK>', ""@okw @crosslabstokyo Basic idea: In evolutionary algorithms, a strong gene is a gene which survives for many generations. Thus, strong genes should grant fitness not only to an individual but also to all of the individual's offspring."", 'This means that in non-stationary environments, genes that increase the *adaptive ability* of a genome are naturally selected for. Over time, the learning ability of a population increases.', 'This perspective can help explain why biological systems are so adaptable, or why languages are so robust -- with large populations, evolution naturally selects for systems that are good at adapting to new tasks.']",21,03,755
13,197,1334017173849858048,1274360135591309313,Carlos Gonzalez-Ballestero,New preprint out: <LINK> We propose using solid-state spin baths (e.g. NV centers) to tailor and probe spin waves & discuss exciting prospects for spintronics. In collaboration between us (@Romero_Isart_G @uniinnsbruck @iqoqi) and Toeno van der Sar at @tudelft. <LINK>,https://arxiv.org/abs/2012.00540,"Spin waves have risen as promising candidate information carriers for the next generation of information technologies. Recent experimental demonstrations of their detection using electron spins in diamond pave the way towards studying the back-action of a controllable paramagnetic spin bath on the spin waves. Here, we present a quantum theory describing the interaction between spin waves and paramagnetic spins. As a case study we consider an ensemble of nitrogen-vacancy spins in diamond in the vicinity of an Yttrium-Iron-Garnet thin film. We show how the back-action of the ensemble results in strong and tuneable modifications of the spin-wave spectrum and propagation properties. These modifications include the full suppression of spin-wave propagation and, in a different parameter regime, the enhancement of their propagation length by $\sim 50\%$. Furthermore, we show how the spin wave thermal fluctuations induce a measurable frequency shift of the paramagnetic spins in the bath. This shift results in a thermal dispersion force that can be measured optically and/or mechanically with a diamond mechanical resonator. In addition, we use our theory to compute the spin wave-mediated interaction between the spins in the bath. We show that all the above effects are measurable by state-of-the-art experiments. Our results provide the theoretical foundation for describing hybrid quantum systems of spin waves and spin baths, and establish the potential of quantum spins as active control, sensing, and interfacing tools for spintronics. ","Towards a quantum interface between spin waves and paramagnetic spin
  baths",1,['New preprint out: <LINK> We propose using solid-state spin baths (e.g. NV centers) to tailor and probe spin waves &amp; discuss exciting prospects for spintronics. In collaboration between us (@Romero_Isart_G @uniinnsbruck @iqoqi) and Toeno van der Sar at @tudelft. <LINK>'],20,12,268
14,158,1131035045626490882,15132384,Kyosuke Nishida,"Our #ACL2019_Italy paper about explainable multi-hop QA is out on arXiv! <LINK> We propose a query-based extractive summarization model, QFE, and the multi-task learning of QA and evidence extraction. Our model achieves SOTA in evidence extraction on HotpotQA! <LINK>",https://arxiv.org/abs/1905.08511,"Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention. This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model. QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence. It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence. Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database. ","Answering while Summarizing: Multi-task Learning for Multi-hop QA with
  Evidence Extraction",1,"['Our #ACL2019_Italy paper about explainable multi-hop QA is out on arXiv!  <LINK> We propose a query-based extractive summarization model, QFE, and the multi-task learning of QA and evidence extraction. Our model achieves SOTA in evidence extraction on HotpotQA! <LINK>']",19,05,267
15,120,1422663599181361152,333826633,Evan Nunez,"Check out my new paper <LINK> w/ Evan Kirby and Chuck Steidel! (For the thread: DT=detailed-ish explanation) 1/8 Using very metal poor damped Lyman alpha absorbers (VMP DLAs) we constrain the amount of metals ejected from the first stars! DT: place empirical constraints on the core collapse supernova (CCSN) yields of zero- and low-metallicity stars 2/8 VMP DLAs are distant, dense, almost metal-free gas clouds that have been exclusively enriched by the first stars; meaning the metals we measure in them are reflective of the metals from the first stars. DT: we use the median of the abundance ratios from ~80 VMP DLAs) 3/8 We show that for some elements, using VMP DLAs is superior to using the abundances from VMP stars for this work. DT: measuring abundances from VMP DLAs cool, dense and neutral gas avoids stellar atmosphere and evolution corrections needed to infer abundances in VMP stars 4/8 We compare our yields to widely used theoretical yields and find that all models do a good job on reproducing Si and S, but vary otherwise for C, N, Al, O, and Fe. (DT: Woosley&Weaver1995, Nomoto+2006, Heger&Woosley2010, Limongi&Chieffi2018, PUSH collaboration) 5/8 DT: We adopt a SN explosion landscape and Initial Distribution of Rotation Velocities onto HW10 and LC18 respectively, and find no change for HW10 and a slight improvement for LC18. When we impose and explosion energy landscape HW10 is unable to reproduce our yields. 6/8 Excitingly, our constraints can be used, right now, in models concerned with the chemical evolution of galaxies i.e., galactic chemical evolution models! 7/8 Future work would benefit from discovering more VMP DLAs, having more high resolution VMP DLA measurements, and measuring more elemental abundances in existing VMP DLA spectra. 8/8",https://arxiv.org/abs/2108.00659,"We place empirical constraints on the yields from zero- and low-metallicity core collapse supernovae (CCSNe) using abundances measured in very metal-poor (VMP; [Fe/H] $\leq$ $-2$) Damped Lyman Alpha Absorbers (DLAs). For some abundance ratios ([N,Al,S/Fe]), VMP DLAs constrain the metal yields of the first SNe more reliably than VMP stars. We compile a large sample of high-S/N VMP DLAs from over 30 years of literature, most with high resolution spectral measurements. We infer the IMF-averaged CCSNe yield from the median values from the DLA abundance ratios of C, N, O, Al, Si, S, and Fe (over Fe and O). We assume that the DLAs are metal-poor enough that they represent galaxies in their earliest stages of evolution, when CCSNe are the only nucleosynthetic sources of the metals we analyze. We compare five sets of zero- and low-metallicity theoretical yields to the empirical yields derived in this work. We find that the five models agree with the DLA yields for ratios containing Si and S. Only one model, Heger & Woosley (2010, hereafter HW10), reproduced the DLA values for N, and one other model, Limongi & Chieffi (2018, hereafter LC18), reproduced [N/O]. We found little change in the theoretical yields with the adoption of a SN explosion landscape (where certain progenitor masses collapse into black holes, contributing no yields) onto HW10, but fixing explosion energy to progenitor mass results in wide disagreements between the predictions and DLA abundances. We investigate the adoption of a simple, observationally motivated Initial Distribution of Rotational Velocities for LC18 and find a slight improvement. ","Empirical Constraints on Core Collapse Supernova Yields using Very Metal
  Poor Damped Lyman Alpha Absorbers",8,"['Check out my new paper <LINK> w/ Evan Kirby and Chuck Steidel! (For the thread: DT=detailed-ish explanation) 1/8', 'Using very metal poor damped Lyman alpha absorbers (VMP DLAs) we constrain the amount of metals ejected from the first stars! DT: place empirical constraints on the core collapse supernova (CCSN) yields of zero- and low-metallicity stars 2/8', 'VMP DLAs are distant, dense, almost metal-free gas clouds that have been exclusively enriched by the first stars; meaning the metals we measure in them are reflective of the metals from the first stars. DT: we use the median of the abundance ratios from ~80 VMP DLAs) 3/8', 'We show that for some elements, using VMP DLAs is superior to using the abundances from VMP stars for this work. DT: measuring abundances from VMP DLAs cool, dense and neutral gas avoids stellar atmosphere and evolution corrections needed to infer abundances in VMP stars 4/8', 'We compare our yields to widely used  theoretical yields and find that all models do a good job on reproducing Si and S, but vary otherwise for C, N, Al, O, and Fe.  (DT: Woosley&amp;Weaver1995, Nomoto+2006, Heger&amp;Woosley2010, Limongi&amp;Chieffi2018, PUSH collaboration) 5/8', 'DT: We adopt a SN explosion landscape and Initial Distribution of Rotation Velocities onto HW10 and LC18 respectively, and find no change for HW10 and a slight improvement for LC18. When we impose and explosion energy landscape HW10 is unable to reproduce our yields. 6/8', 'Excitingly, our constraints can be used, right now, in models concerned with the chemical evolution of galaxies i.e., galactic chemical evolution models! 7/8', 'Future work would benefit from discovering more VMP DLAs, having more high resolution VMP DLA measurements, and measuring more elemental abundances in existing VMP DLA spectra. 8/8']",21,08,1779
16,54,1441068497031741442,790033937531703296,Yi Tay,"New paper alert! üòÄ ""Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"" We study scaling laws of Transformers pertaining to both upstream & downstream transfer by pretraining over 200+ T5 models. Paper: <LINK> @GoogleAI @DeepMind <LINK> What we found: 1) Scaling Laws differ in upstream/downstream. While upstream pre-training performance measured by perplexity scales with model size quite independently from the model shape, the downstream performance does not. Be cautious about over indexing on perplexity scores! <LINK> 2) Scaling protocols can differ in diff compute regions! A scaling strategy may work at a smaller compute region but not at large-scale (or vice versa). So iterating on scaling strategy at a smaller size and scaling up later may be a challenging endeavor. 3) We analyze the pareto-frontier of the compute-performance trade-off of over 200+ pretrained models. We find that not all knobs are created equal. Some influence the pareto-frontier greatly while some not so much. Depth is easily one of the biggest influencers. <LINK> 4) By analyzing the pareto-frontier, we find that existing canonical configs (base/large etc) of T5 are slightly pareto-inefficient and one could achieve a much more efficient model with better performance. 5) From our analysis we propose a scaling protocol which we call ""DeepNarrow"". We show that this protocol applies to all sizes (small-&gt;XXL). At base, we arrive at a model with 50% less parameters and 40% faster with better downstream performance. <LINK> 6) To further test the generality and robustness of these findings, we also conduct experiments on the vision domain (ViT) and also 12 other diverse NLP tasks to make it a total of almost 30 NLP tasks. <LINK> 7) Finally, we will release and open source all 100+ pretrained checkpoints to the community. This is slated for early Q4 2021. Release link in the paper PDF itself. Thanks to all amazing collaborators at @GoogleAI and @DeepMind , @m__dehghani (co-first author), @Jeffy_Sailing @LiamFedus @samiraabnar @hwchung27 @sharan0909 @DaniYogatama @ashVaswani @metzlerd",http://arxiv.org/abs/2109.10686,"There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\% fewer parameters and training 40\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis. ","Scale Efficiently: Insights from Pre-training and Fine-tuning
  Transformers",9,"['New paper alert! üòÄ ""Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers""\n\nWe study scaling laws of Transformers pertaining to both upstream &amp; downstream transfer by pretraining over 200+ T5 models.\n\nPaper: <LINK>\n@GoogleAI @DeepMind <LINK>', 'What we found:\n1) Scaling Laws differ in upstream/downstream. While upstream pre-training performance measured by perplexity scales with model size quite\nindependently from the model shape, the downstream performance does not. Be cautious about over indexing on perplexity scores! https://t.co/uRYjHzX99D', '2) Scaling protocols can differ in diff compute regions! A scaling strategy may work at a smaller compute region but not at large-scale (or vice versa). So iterating on scaling strategy at a smaller size and scaling up later may be a challenging endeavor.', '3) We analyze the pareto-frontier of the compute-performance trade-off of over 200+ pretrained models. We find that not all knobs are created equal. Some influence the pareto-frontier greatly while some not so much. Depth is easily one of the biggest influencers. https://t.co/F7FYcWVuLm', '4) By analyzing the pareto-frontier, we find that existing canonical configs (base/large etc) of T5 are slightly pareto-inefficient and one could achieve a much more efficient model with better performance.', '5) From our analysis we propose a scaling protocol which we call ""DeepNarrow"". We show that this protocol applies to all sizes (small-&gt;XXL). At base, we arrive at a model with 50% less parameters and 40% faster with better downstream performance. https://t.co/4pJItFrPkI', '6) To further test the generality and robustness of these findings, we also conduct experiments on the vision domain (ViT) and also 12 other diverse NLP tasks to make it a total of almost 30 NLP tasks. https://t.co/VVlH4TT4Gu', '7) Finally, we will release and open source all 100+ pretrained checkpoints to the community. This is slated for early Q4 2021. Release link in the paper PDF itself.', 'Thanks to all amazing collaborators at @GoogleAI and @DeepMind , @m__dehghani (co-first author), @Jeffy_Sailing @LiamFedus @samiraabnar @hwchung27 @sharan0909 @DaniYogatama @ashVaswani @metzlerd']",21,09,2109
17,46,1309129229037121544,382961853,Jo Dunkley,"New @ACT_Pol paper (and data) today with more than 4000 galaxy clusters detected via the Sunyaev-Zel'dovich effect, reaching billions of light yrs away! - Hilton et al <LINK>, with redshifts from optical data including @theDESurvey, HSC, KiDS and @sdssurveys.",https://arxiv.org/abs/2009.11043,"We present a catalog of 4195 optically confirmed Sunyaev-Zel'dovich (SZ) selected galaxy clusters detected with signal-to-noise > 4 in 13,211 deg$^2$ of sky surveyed by the Atacama Cosmology Telescope (ACT). Cluster candidates were selected by applying a multi-frequency matched filter to 98 and 150 GHz maps constructed from ACT observations obtained from 2008-2018, and confirmed using deep, wide-area optical surveys. The clusters span the redshift range 0.04 < z < 1.91 (median z = 0.52). The catalog contains 222 z > 1 clusters, and a total of 868 systems are new discoveries. Assuming an SZ-signal vs. mass scaling relation calibrated from X-ray observations, the sample has a 90% completeness mass limit of M500c > 3.8 x 10$^{14}$ MSun, evaluated at z = 0.5, for clusters detected at signal-to-noise ratio > 5 in maps filtered at an angular scale of 2.4'. The survey has a large overlap with deep optical weak-lensing surveys that are being used to calibrate the SZ-signal mass-scaling relation, such as the Dark Energy Survey (4566 deg$^2$), the Hyper Suprime-Cam Subaru Strategic Program (469 deg$^2$), and the Kilo Degree Survey (825 deg$^2$). We highlight some noteworthy objects in the sample, including potentially projected systems; clusters with strong lensing features; clusters with active central galaxies or star formation; and systems of multiple clusters that may be physically associated. The cluster catalog will be a useful resource for future cosmological analyses, and studying the evolution of the intracluster medium and galaxies in massive clusters over the past 10 Gyr. ","The Atacama Cosmology Telescope: A Catalog of &gt; 4000 Sunyaev-Zel'dovich
  Galaxy Clusters",1,"[""New @ACT_Pol paper (and data) today with more than 4000 galaxy clusters detected via the Sunyaev-Zel'dovich effect, reaching billions of light yrs away!  - Hilton et al <LINK>, with redshifts from optical data including @theDESurvey, HSC, KiDS and @sdssurveys.""]",20,09,259
18,151,1490725739930562569,2180768821,Erik Hoel,"1/ How can we identify and agree upon cases of emergence in complex systems? A new paper by myself and @renzocom is out today, showing that across measures of causation there is widespread agreement over what's emergent and what's not. <LINK> <LINK> 2/ This is critically important because we need to move beyond metaphysical debates about emergence and instead develop a science of emergence analogous to that of complex systems science. <LINK> 3/ Specifically, the theory of causal emergence states that macroscales can minimize noise in causal relations and increase their strength. It turns out that this effect of macroscale causation via error-correction is supported by every measure of causation we examined. <LINK> 4/ Note that the ubiquity of causal emergence holds true across background assumptions (like what intervention distributions you use) and individual instances of causal emergence vanish in *no* conditions in *any* measure. 5/ Additionally, we show that measures of causation are *consilient* in that they rely on the same small number of fundamental terms: ""causal primitives."" Causal primitives themselves can be greater at the macroscale (green is greater at macroscale), ensuring the measures are <LINK> 6/ This consilience is why people keep rediscovering measures of causation! We show that, for instance, Judea Pearl ended up rediscovering a measure first proposed by David Lewis. But it also gives us a family of measures that generally agree (so objectivity about causes). 7/ There's a lot of really cool extra stuff in here. It's worth checking out for the menagerie of measures of causation alone. I think it's the most comprehensive in the literature, demonstrating they are all related and behave quite similarly. Some people (in no particular order) who might be interested @anilkseth @_fernando_rosas @PedroMediano @adambarrett81 @RCarhartHarris @DanielBor @Sara_Imari @GaneshNatesh @DrYohanJohn etc Also @joe_dewhurst @GKBesterfriend @renzocom I don't think Aristotle's concepts map cleanly onto modern concepts without some bending. Probably counterfactuals are closest to formal causes, but you could argue efficient/material causes too. @NiethammerLab ""Strong"" vs. ""weak"" emergence, for instance. Various notions of downward causation that don't work when you model them out. Etc.",https://arxiv.org/abs/2202.01854,"Causal emergence is the theory that macroscales can reduce the noise in causal relationships, leading to stronger causes at the macroscale. First identified using the effective information and later the integrated information in model systems, causal emergence has been analyzed in real data across the sciences since. But is it simply a quirk of these original measures? To answer this question we examined over a dozen popular measures of causation, all independently developed and widely used, and spanning different fields from philosophy to statistics to psychology to genetics. All showed cases of causal emergence. This is because, we prove, measures of causation are based on a small set of related ""causal primitives."" This consilience of independently-developed measures of causation shows that macroscale causation is a general fact about causal relationships, is scientifically detectable, and is not a quirk of any particular measure of causation. This finding sets the science of emergence on firmer ground, opening the door for the detection of intrinsic scales of function in complex systems, as well as assisting with scientific modeling and experimental interventions. ",Causal emergence is widespread across measures of causation,11,"[""1/ How can we identify and agree upon cases of emergence in complex systems? A new paper by myself and @renzocom is out today, showing that across measures of causation there is widespread agreement over what's emergent and what's not. <LINK> <LINK>"", '2/ This is critically important because we need to move beyond metaphysical debates about emergence and instead develop a science of emergence analogous to that of complex systems science. https://t.co/MhUMTOCoah', '3/ Specifically, the theory of causal emergence states that macroscales can minimize noise in causal relations and increase their strength. It turns out that this effect of macroscale causation via error-correction is supported by every measure of causation we examined. https://t.co/MyxuQfdfEk', '4/ Note that the ubiquity of causal emergence holds true across background assumptions (like what intervention distributions you use) and individual instances of causal emergence vanish in *no* conditions in *any* measure.', '5/ Additionally, we show that measures of causation are *consilient* in that they rely on the same small number of fundamental terms: ""causal primitives."" Causal primitives themselves can be greater at the macroscale (green is greater at macroscale), ensuring the measures are https://t.co/H0HVszrMm2', '6/ This consilience is why people keep rediscovering measures of causation! We show that, for instance, Judea Pearl ended up rediscovering a measure first proposed by David Lewis. But it also gives us a family of measures that generally agree (so objectivity about causes).', ""7/ There's a lot of really cool extra stuff in here. It's worth checking out for the menagerie of measures of causation alone. I think it's the most comprehensive in the literature, demonstrating they are all related and behave quite similarly."", 'Some people (in no particular order) who might be interested @anilkseth @_fernando_rosas @PedroMediano @adambarrett81 @RCarhartHarris @DanielBor @Sara_Imari @GaneshNatesh @DrYohanJohn etc', 'Also @joe_dewhurst', ""@GKBesterfriend @renzocom I don't think Aristotle's concepts map cleanly onto modern concepts without some bending. Probably counterfactuals are closest to formal causes, but you could argue efficient/material causes too."", '@NiethammerLab ""Strong"" vs. ""weak"" emergence, for instance. Various notions of downward causation that don\'t work when you model them out. Etc.']",22,02,2322
19,98,1480364616710606853,1424524891785674752,Francisco Villaescusa-Navarro,"Can we infer cosmological parameters with one single galaxy? We have investigated this crazy idea in our new paper <LINK>, and the answer, according to our simulations and models, seems to be yes. <LINK> We have trained neural networks using hundreds of thousands of individual galaxies from the @camels_project. We find that knowing the properties of a single, generic, galaxy allows our models to infer the value of Omega_m with a 10% precision, accounting for astrophysical uncertainties, as modeled in CAMELS. We believe that the explanation behind these results is that galaxy properties live in a low-dimensional manifold that is sensitive to Omega_m (or perhaps Omega_b/Omega_m). <LINK> We note however that our models are not robust (yet!); i.e. they are simulation dependent. We would love to get feedback on this surprising result. With @DavidSpergel, @eelregit, @PabloLemosP",https://arxiv.org/abs/2201.02202,"Galaxies can be characterized by many internal properties such as stellar mass, gas metallicity, and star-formation rate. We quantify the amount of cosmological and astrophysical information that the internal properties of individual galaxies and their host dark matter halos contain. We train neural networks using hundreds of thousands of galaxies from 2,000 state-of-the-art hydrodynamic simulations with different cosmologies and astrophysical models of the CAMELS project to perform likelihood-free inference on the value of the cosmological and astrophysical parameters. We find that knowing the internal properties of a single galaxy allow our models to infer the value of $\Omega_{\rm m}$, at fixed $\Omega_{\rm b}$, with a $\sim10\%$ precision, while no constraint can be placed on $\sigma_8$. Our results hold for any type of galaxy, central or satellite, massive or dwarf, at all considered redshifts, $z\leq3$, and they incorporate uncertainties in astrophysics as modeled in CAMELS. However, our models are not robust to changes in subgrid physics due to the large intrinsic differences the two considered models imprint on galaxy properties. We find that the stellar mass, stellar metallicity, and maximum circular velocity are among the most important galaxy properties to determine the value of $\Omega_{\rm m}$. We believe that our results can be explained taking into account that changes in the value of $\Omega_{\rm m}$, or potentially $\Omega_{\rm b}/\Omega_{\rm m}$, affect the dark matter content of galaxies. That effect leaves a distinct signature in galaxy properties to the one induced by galactic processes. Our results suggest that the low-dimensional manifold hosting galaxy properties provides a tight direct link between cosmology and astrophysics. ",Cosmology with one galaxy?,4,"['Can we infer cosmological parameters with one single galaxy? We have investigated this crazy idea in our new paper <LINK>, and the answer, according to our simulations and models, seems to be yes. <LINK>', 'We have trained neural networks using hundreds of thousands of individual galaxies from the @camels_project.  We find that knowing the properties of a single, generic, galaxy allows our models to infer the value of Omega_m with a 10% precision, accounting for astrophysical', 'uncertainties, as modeled in CAMELS. We believe that the explanation behind these results is that galaxy properties live in a low-dimensional manifold that is sensitive to Omega_m (or perhaps Omega_b/Omega_m). https://t.co/pp3sGtXmQv', 'We note however that our models are not robust (yet!); i.e. they are simulation dependent. We would love to get feedback on this surprising result. With @DavidSpergel, @eelregit, @PabloLemosP']",22,01,885
20,104,1424986072895926283,2416610796,Gorka Mu√±oz-Gil,"If you want a nice read for your holidays, our new paper on machine learning for anomalous diffusion is out! We asked ourselves if unsupervised learning could be used to learn features of stochastic processes. Answer: Yes!! Check the details here: <LINK>",https://arxiv.org/abs/2108.03411,"The characterization of diffusion processes is a keystone in our understanding of a variety of physical phenomena. Many of these deviate from Brownian motion, giving rise to anomalous diffusion. Various theoretical models exists nowadays to describe such processes, but their application to experimental setups is often challenging, due to the stochastic nature of the phenomena and the difficulty to harness reliable data. The latter often consists on short and noisy trajectories, which are hard to characterize with usual statistical approaches. In recent years, we have witnessed an impressive effort to bridge theory and experiments by means of supervised machine learning techniques, with astonishing results. In this work, we explore the use of unsupervised methods in anomalous diffusion data. We show that the main diffusion characteristics can be learnt without the need of any labelling of the data. We use such method to discriminate between anomalous diffusion models and extract their physical parameters. Moreover, we explore the feasibility of finding novel types of diffusion, in this case represented by compositions of existing diffusion models. At last, we showcase the use of the method in experimental data and demonstrate its advantages for cases where supervised learning is not applicable. ",Unsupervised learning of anomalous diffusion data,1,"['If you want a nice read for your holidays, our new paper on machine learning for anomalous diffusion is out!\n\nWe asked ourselves if unsupervised learning could be used to learn features of stochastic processes. Answer: Yes!!\n\nCheck the details here: <LINK>']",21,08,254
21,80,983994363880591360,3350628106,Christoffer Nellaker,Super excited! Our new paper on deep learning for computational phenotyping of placenta histology. Pleasure working with @C_Glastonbury @michaelferlaino @ceclindgren @bdi_oxford and coauthors in the Nuffield Department of Women's & Reproductive Health.  <LINK>,https://arxiv.org/abs/1804.03270,"The placenta is a complex organ, playing multiple roles during fetal development. Very little is known about the association between placental morphological abnormalities and fetal physiology. In this work, we present an open sourced, computationally tractable deep learning pipeline to analyse placenta histology at the level of the cell. By utilising two deep Convolutional Neural Network architectures and transfer learning, we can robustly localise and classify placental cells within five classes with an accuracy of 89%. Furthermore, we learn deep embeddings encoding phenotypic knowledge that is capable of both stratifying five distinct cell populations and learn intraclass phenotypic variance. We envisage that the automation of this pipeline to population scale studies of placenta histology has the potential to improve our understanding of basic cellular placental biology and its variations, particularly its role in predicting adverse birth outcomes. ",Towards Deep Cellular Phenotyping in Placental Histology,1,"[""Super excited!  Our new paper on deep learning for computational phenotyping of placenta histology. Pleasure working with @C_Glastonbury @michaelferlaino @ceclindgren @bdi_oxford and coauthors in the Nuffield Department of Women's &amp; Reproductive Health.   <LINK>""]",18,04,260
22,183,1306757838258286592,983857052840636417,Rob Corless,"I am delighted to announce that our paper on Integrals of Functions Containing Parameters has been accepted to the Mathematical Gazette. You may find a copy (sans an enlightening cartoon which we have license to publish in a journal but not online) at <LINK> You may find the cartoon at <LINK> The paper addresses an annoying difficulty with integrals of continuous functions containing parameters: unless you're careful, the integral will not be correct for all possible values of the parameters. We have a suggested method... It's not difficult, but it does go against how we teach Calculus today. I hope that you enjoy it!",http://arxiv.org/abs/2009.08431,"This paper offers what seems at first to be a minor technical correction to the current practice of computing indefinite integrals, and introduces the idea of a ""Kahanian constant of integration"". However, the total impact of this minor correction is potentially large because the current practice is taught early at the university level and to very many students---most of whom do not go on to become mathematics majors. Moreover, computer algebra systems have become widespread, including good free ones, some of which are available for smartphones. Most current computer algebra systems apply current textbook rules and amplify the effects of fundamental ""minor"" errors such as the error in continuity that we address in this article. So in practice, the correction we present is important. ",Integrals of functions containing parameters,4,"['I am delighted to announce that our paper on Integrals of Functions Containing Parameters has been accepted to the Mathematical Gazette. You may find a copy (sans an enlightening cartoon which we have license to publish in a journal but not online) at <LINK>', 'You may find the cartoon at https://t.co/hdTSRJgGA2', ""The paper addresses an annoying difficulty with integrals of continuous functions containing parameters: unless you're careful, the integral will not be correct for all possible values of the parameters. We have a suggested method... It's not difficult, but it does go against"", 'how we teach Calculus today. I hope that you enjoy it!']",20,09,625
23,147,1258555474775064576,35926248,Thomas Paula,"Our paper ""A Proposal for Intelligent Agents with Episodic Memory"" is on Arxiv! We propose an alternative look at the role of episodic memory for intelligent agents. We hope this view can help to spark new discussions and ideas! @dfm794 @JulianoVacaro  <LINK>",https://arxiv.org/abs/2005.03182,"In the future we can expect that artificial intelligent agents, once deployed, will be required to learn continually from their experience during their operational lifetime. Such agents will also need to communicate with humans and other agents regarding the content of their experience, in the context of passing along their learnings, for the purpose of explaining their actions in specific circumstances or simply to relate more naturally to humans concerning experiences the agent acquires that are not necessarily related to their assigned tasks. We argue that to support these goals, an agent would benefit from an episodic memory; that is, a memory that encodes the agent's experience in such a way that the agent can relive the experience, communicate about it and use its past experience, inclusive of the agents own past actions, to learn more effective models and policies. In this short paper, we propose one potential approach to provide an AI agent with such capabilities. We draw upon the ever-growing body of work examining the function and operation of the Medial Temporal Lobe (MTL) in mammals to guide us in adding an episodic memory capability to an AI agent composed of artificial neural networks (ANNs). Based on that, we highlight important aspects to be considered in the memory organization and we propose an architecture combining ANNs and standard Computer Science techniques for supporting storage and retrieval of episodic memories. Despite being initial work, we hope this short paper can spark discussions around the creation of intelligent agents with memory or, at least, provide a different point of view on the subject. ",A Proposal for Intelligent Agents with Episodic Memory,1,"['Our paper ""A Proposal for Intelligent Agents with Episodic Memory"" is on Arxiv! We propose an alternative look at the role of episodic memory for intelligent agents. We hope this view can help to spark new discussions and ideas! @dfm794  @JulianoVacaro \n\n<LINK>']",20,05,259
24,126,1334389947265126401,802543221943439360,Andrea Caputo,"<LINK> Paper out, pretty excited with it üö¶üö¶üö¶ With Jose Luis Bernal and Marc Kamionkowski we propose a new way to detect dark matter decay using Line Intensity Mapping! We essentially suggest to treat the dark matter as a line interloper and look for it üßêüßê <LINK>",https://arxiv.org/abs/2012.00771,"The nature of dark matter is a longstanding mystery in cosmology, which can be studied with laboratory or collider experiments, as well as astrophysical and cosmological observations. In this work, we propose realistic and efficient strategies to detect radiative products from dark-matter decays with line-intensity mapping (LIM) experiments. This radiation will behave as a line interloper for the atomic and molecular spectral lines targeted by LIM surveys. The most distinctive signatures of the contribution from dark-matter radiative decays are an extra anisotropy on the LIM power spectrum due to projection effects, as well as a narrowing and a shift towards higher intensities of the voxel intensity distribution. We forecast the minimum rate of decays into two photons that LIM surveys will be sensitive to as function of the dark-matter mass in the range $\sim 10^{-6}-10$ eV, and discuss how to reinterpret such results for dark matter that decays into a photon and another particle. We find that both the power spectrum and the voxel intensity distribution are expected to be very sensitive to the dark-matter contribution, with the voxel intensity distribution being more promising for most experiments considered. Interpreting our results in terms of the axion, we show that LIM surveys will be extremely competitive to detect its decay products, improving several orders of magnitudes (depending on the mass) the sensitivity of laboratory and astrophysical searches, especially in the mass range $\sim 1-10$ eV. ",Strategies to Detect Dark-Matter Decays with Line-Intensity Mapping,1,"['<LINK>\n\nPaper out, pretty excited with it üö¶üö¶üö¶\nWith Jose Luis Bernal and  Marc Kamionkowski we propose a new way to detect dark matter decay using Line Intensity Mapping! We essentially suggest to treat the dark matter as a line interloper and look for it üßêüßê <LINK>']",20,12,262
25,37,1298250595313184768,561899047,Aki Vehtari,"Our new paper analyzes one of the limitations of cross-validation ""Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison"" with Tuomas Sivula and @MansMeg <LINK> <LINK> Normal distribution has been used to present the uncertainty in CV for a single model (e.g. elpd_loo SE) and in model comparison (e.g. elpd_diff SE), and there have been couple papers discussing the limitations, but it was still unclear when we can trust the model comparison. Tuomas did great work on deriving new finite case and asymptotic results (24p. paper and 64p. appendix with proofs). tl;dr Do model checking before model comparison, uncertainty calibration bad for small differences but then the differences are small anyway, small n is difficult. Conclusions: cross-validation model comparison uncertainty estimates can perform badly when 1. the models make very similar predictions, 2. the models are misspecified with outliers in the data, and 3. the number of observations is small. <LINK> Corresponding consequences: 1. When the models make similar predictions there is not much difference in the predictive performance, but the bad calibration makes LOO-CV less useful for separating very small effect sizes from zero effect sizes. 2. The model misspecification in model comparison should be avoided by proper model checking and expansion before using LOO-CV. 3. Small differences in the predictive performance can not reliably be detected by LOO-CV if the number of observations is small. @lei_zhang_lz If the predictions are very similar model averaging predictions will also be similar to individual model predictions. Model averaging is useful if the models have similar predictive performance, but make different kind of predictions. See <LINK> for more. @lei_zhang_lz Instead of ""less sense"" I'd say for prediction in such cases it's less important to decide on one single model unless there is, e.g., costs for making the future measurements for the predictors (think e.g. predicting disease status and bunch of potentially useful medical tests).",https://arxiv.org/abs/2008.10296,"Leave-one-out cross-validation (LOO-CV) is a popular method for comparing Bayesian models based on their estimated predictive performance on new, unseen, data. As leave-one-out cross-validation is based on finite observed data, there is uncertainty about the expected predictive performance on new data. By modeling this uncertainty when comparing two models, we can compute the probability that one model has a better predictive performance than the other. Modeling this uncertainty well is not trivial, and for example, it is known that the commonly used standard error estimate is often too small. We study the properties of the Bayesian LOO-CV estimator and the related uncertainty estimates when comparing two models. We provide new results of the properties both theoretically in the linear regression case and empirically for multiple different models and discuss the challenges of modeling the uncertainty. We show that problematic cases include: comparing models with similar predictions, misspecified models, and small data. In these cases, there is a weak connection in the skewness of the individual leave-one-out terms and the distribution of the error of the Bayesian LOO-CV estimator. We show that it is possible that the problematic skewness of the error distribution, which occurs when the models make similar predictions, does not fade away when the data size grows to infinity in certain situations. Based on the results, we also provide practical recommendations for the users of Bayesian LOO-CV for model comparison. ","Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model
  Comparison",8,"['Our new paper analyzes one of the limitations of cross-validation\n""Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison"" with Tuomas Sivula and @MansMeg  <LINK> <LINK>', 'Normal distribution has been used to present the uncertainty in CV for a single model (e.g. elpd_loo SE) and in model comparison (e.g. elpd_diff SE), and there have been couple papers discussing the limitations, but it was still unclear when we can trust the model comparison.', 'Tuomas did great work on deriving new finite case and asymptotic results (24p. paper and 64p. appendix with proofs). tl;dr Do model checking before model comparison, uncertainty calibration bad for small differences but then the differences are small anyway, small n is difficult.', 'Conclusions: cross-validation model comparison uncertainty estimates can perform badly when\n 1. the models make very similar predictions,\n 2. the models are misspecified with outliers in the data, and\n 3. the number of observations is small. https://t.co/Np4iF5yqsg', 'Corresponding consequences:\n 1. When the models make similar predictions there is not much difference in the predictive performance, but the bad calibration makes LOO-CV less useful for separating very small effect sizes from zero effect sizes.', '2. The model misspecification in model comparison should be avoided by proper model checking and expansion before using LOO-CV.\n 3. Small differences in the predictive performance can not reliably be detected by LOO-CV if the number of observations is small.', '@lei_zhang_lz If the predictions are very similar model averaging predictions will also be similar to individual model predictions. Model averaging is useful if the models have similar predictive performance, but make different kind of predictions. See https://t.co/KkoRy5wuO3 for more.', '@lei_zhang_lz Instead of ""less sense"" I\'d say for prediction in such cases it\'s less important to decide on one single model unless there is, e.g., costs for making the future measurements for the predictors (think e.g. predicting disease status and bunch of potentially useful medical tests).']",20,08,2061
26,79,1060124541589090305,991380306,James Jackman,"I have a new paper out on arXiv today, on the detection of a giant flare from a young (~2 Myr) M star with NGTS. The flare exhibited relatively rare quasi-periodic pulsations which we used to probe its behaviour: <LINK> @WardHoward4Him @ProfAbelMendez Cheers! We were observing in the NGTS bandpass, which is between about 500 to 900 nm (<LINK>) - so observations in g' would complement it really well, particularly for comparisons of flare amplitude.",https://arxiv.org/abs/1811.02008,"We present the detection of an energetic flare on the pre-main sequence M3 star NGTS J121939.5-355557, which we estimate as only 2 Myr old. The flare had an energy of $3.2\pm^{0.4}_{0.3}\times 10^{36}$erg and a fractional amplitude of $7.2\pm0.8$, making it one of the most energetic flares seen on an M star. The star is also X-ray active, in the saturated regime with $log L_{X}/L_{Bol} = -3.1$. In the flare peak we have identified multi-mode quasi-periodic pulsations formed of two statistically significant periods of approximately 320 and 660 seconds. This flare is one of the largest amplitude events to exhibit such pulsations. The shorter period mode is observed to start after a short-lived spike in flux lasting around 30 seconds, which would not have been resolved in Kepler or TESS short cadence modes. Our data shows how the high cadence of NGTS can be used to apply solar techniques to stellar flares and identify potential causes of the observed oscillations. We also discuss the implications of this flare for the habitability of planets around M star hosts and how NGTS can aid in our understanding of this. ","Detection of a giant flare displaying quasi-periodic pulsations from a
  pre-main sequence M star with NGTS",2,"['I have a new paper out on arXiv today, on the detection of a giant flare from a young (~2 Myr) M star with NGTS. The flare exhibited relatively rare quasi-periodic pulsations which we used to probe its behaviour: <LINK>', ""@WardHoward4Him @ProfAbelMendez Cheers! We were observing in the NGTS bandpass, which is between about 500 to 900 nm (https://t.co/pF1Ucby3A3) - so observations in g' would complement it really well, particularly for comparisons of flare amplitude.""]",18,11,451
27,42,1321499533377966083,772809603046334464,Jonathan Mackey,"New @hesstelescopes paper on @arxiv today, ""An extreme particle accelerator in the Galactic plane: HESS J1826-130"", investigating a very hard-spectrum (but faint), unidentified gamma-ray source. #DIASdiscovers @davit_zargaryan @DIAS_Dublin @DIASAstronomy <LINK>",https://arxiv.org/abs/2010.13101,"The unidentified very-high-energy (VHE; E $>$ 0.1 TeV) $\gamma$-ray source, HESS J1826$-$130, was discovered with the High Energy Stereoscopic System (HESS) in the Galactic plane. The analysis of 215 h of HESS data has revealed a steady $\gamma$-ray flux from HESS J1826$-$130, which appears extended with a half-width of 0.21$^{\circ}$ $\pm$ 0.02$^{\circ}_{\text{stat}}$ $\pm$ 0.05$^{\circ}_{\text{sys}}$. The source spectrum is best fit with either a power-law function with a spectral index $\Gamma$ = 1.78 $\pm$ 0.10$_{\text{stat}}$ $\pm$ 0.20$_{\text{sys}}$ and an exponential cut-off at 15.2$^{+5.5}_{-3.2}$ TeV, or a broken power-law with $\Gamma_{1}$ = 1.96 $\pm$ 0.06$_{\text{stat}}$ $\pm$ 0.20$_{\text{sys}}$, $\Gamma_{2}$ = 3.59 $\pm$ 0.69$_{\text{stat}}$ $\pm$ 0.20$_{\text{sys}}$ for energies below and above $E_{\rm{br}}$ = 11.2 $\pm$ 2.7 TeV, respectively. The VHE flux from HESS J1826$-$130 is contaminated by the extended emission of the bright, nearby pulsar wind nebula (PWN), HESS J1825$-$137, particularly at the low end of the energy spectrum. Leptonic scenarios for the origin of HESS J1826$-$130 VHE emission related to PSR J1826$-$1256 are confronted by our spectral and morphological analysis. In a hadronic framework, taking into account the properties of dense gas regions surrounding HESS J1826$-$130, the source spectrum would imply an astrophysical object capable of accelerating the parent particle population up to $\gtrsim$200 TeV. Our results are also discussed in a multiwavelength context, accounting for both the presence of nearby supernova remnants (SNRs), molecular clouds, and counterparts detected in radio, X-rays, and TeV energies. ",An extreme particle accelerator in the Galactic plane: HESS J1826$-$130,1,"['New @hesstelescopes paper on @arxiv today, ""An extreme particle accelerator in the Galactic plane: HESS J1826-130"", investigating a very hard-spectrum (but faint), unidentified gamma-ray source. #DIASdiscovers @davit_zargaryan @DIAS_Dublin @DIASAstronomy \n<LINK>']",20,10,261
28,171,1366706548752400384,1682932567,Fabian R. Lux,"New paper on ArXiv! We study the observable algebra of multi-q magnetization textures and find it determined by a noncommutative torus. The 2D quantum topological Hall effect is thus a manifestation of effective 4D physics with two ""curled up dimensions"" <LINK>",https://arxiv.org/abs/2103.01047,"The nontrivial topology of spin systems such as skyrmions in real space can promote complex electronic states. Here, we provide a general viewpoint at the emergence of topological electronic states in spin systems based on the methods of noncommutative K-theory. By realizing that the structure of the observable algebra of spin textures is determined by the algebraic properties of the noncommutative hypertorus, we arrive at a unified understanding of topological electronic states which we predict to arise in various noncollinear setups. The power of our approach lies in an ability to categorize emergent topological states algebraically without referring to smooth real- or reciprocal-space quantities. This opens a way towards an educated design of topological phases in aperiodic, disordered, or non-smooth textures of spins and charges containing topological defects. ","Unified topological characterization of electronic states in spin
  textures from noncommutative K-theory",1,"['New paper on ArXiv! We study the observable algebra of multi-q magnetization textures and find it determined by a noncommutative torus. The 2D quantum topological Hall effect is thus a manifestation of effective 4D physics with two ""curled up dimensions""\n\n<LINK>']",21,03,261
29,47,1231779284592812033,1105632244092321793,Dr. Jiayi Sun,My new paper is out! <LINK> It's about testing a decades-old hypothesis re: molecular clouds in various galactic environments. Yet it requires a joint analysis of so many types of data (from UV to IR to mm to radio). Super happy that I finally succeeded! @j_tharindu Thanks ^_^,https://arxiv.org/abs/2002.08964,"We compare the observed turbulent pressure in molecular gas, $P_\mathrm{turb}$, to the required pressure for the interstellar gas to stay in equilibrium in the gravitational potential of a galaxy, $P_\mathrm{DE}$. To do this, we combine arcsecond resolution CO data from PHANGS-ALMA with multi-wavelength data that traces the atomic gas, stellar structure, and star formation rate (SFR) for 28 nearby star-forming galaxies. We find that $P_\mathrm{turb}$ correlates with, but almost always exceeds the estimated $P_\mathrm{DE}$ on kiloparsec scales. This indicates that the molecular gas is over-pressurized relative to the large-scale environment. We show that this over-pressurization can be explained by the clumpy nature of molecular gas; a revised estimate of $P_\mathrm{DE}$ on cloud scales, which accounts for molecular gas self-gravity, external gravity, and ambient pressure, agrees well with the observed $P_\mathrm{turb}$ in galaxy disks. We also find that molecular gas with cloud-scale ${P_\mathrm{turb}}\approx{P_\mathrm{DE}}\gtrsim{10^5\,k_\mathrm{B}\,\mathrm{K\,cm^{-3}}}$ in our sample is more likely to be self-gravitating, whereas gas at lower pressure appears more influenced by ambient pressure and/or external gravity. Furthermore, we show that the ratio between $P_\mathrm{turb}$ and the observed SFR surface density, $\Sigma_\mathrm{SFR}$, is compatible with stellar feedback-driven momentum injection in most cases, while a subset of the regions may show evidence of turbulence driven by additional sources. The correlation between $\Sigma_\mathrm{SFR}$ and kpc-scale $P_\mathrm{DE}$ in galaxy disks is consistent with the expectation from self-regulated star formation models. Finally, we confirm the empirical correlation between molecular-to-atomic gas ratio and kpc-scale $P_\mathrm{DE}$ reported in previous works. ","Dynamical Equilibrium in the Molecular ISM in 28 Nearby Star-Forming
  Galaxies",2,"[""My new paper is out!\n\n<LINK>\n\nIt's about testing a decades-old hypothesis re: molecular clouds in various galactic environments. Yet it requires a joint analysis of so many types of data (from UV to IR to mm to radio). Super happy that I finally succeeded!"", '@j_tharindu Thanks ^_^']",20,02,277
30,167,1365210166019444736,2984713195,Behnam Javanmardi,"(1/5) In our recent study (led by myself and including @PierreKervella & @LBreuval) we inspected the Cepheid distance ladder and the Hubble Constant (H0) by an independent and complete re-analysis of the (raw) HST data of the SNIa host galaxy NGC 5584: <LINK> <LINK> (2/5) Why we chose NGC 5584? because the periods and amplitude ratios of its Cepheids in different HST bands played a key role in the local measurement of H0 by SH0ES (see our abstract). <LINK> (3/5) We intentionally chose different analysis methods and tools, in particular, different tools for photometry and a completely different method for light curve analysis. <LINK> (4/5) Result: we do not find a systematic difference between our results compared to those reported by SH0ES. The significant Hubble Constant tension remains a problem for the LCDM standard model of cosmology. <LINK> (5 /5) Our paper is accepted for publication in the ApJ and my talk on the results of this study at the ""Cosmology at the Crossroads"" conference is available here: <LINK> See also the Tweet from @jrdmb: <LINK>",https://arxiv.org/abs/2102.12489,"The current tension between the direct and the early Universe measurements of the Hubble Constant, $H_0$, requires detailed scrutiny of all the data and methods used in the studies on both sides of the debate. The Cepheids in the type Ia supernova (SNIa) host galaxy NGC 5584 played a key role in the local measurement of $H_0$. The SH0ES project used the observations of this galaxy to derive a relation between Cepheids' periods and ratios of their amplitudes in different optical bands of the Hubble Space Telescope (HST), and used these relations to analyse the light curves of the Cepheids in around half of the current sample of local SNIa host galaxies. In this work, we present an independent detailed analysis of the Cepheids in NGC 5584. We employ different tools for our photometric analysis and a completely different method for our light curve analysis, and we do not find a systematic difference between our period and mean magnitude measurements compared to those reported by SH0ES. By adopting a period-luminosity relation calibrated by the Cepheids in the Milky Way, we measure a distance modulus $\mu=31.810\pm0.047$ (mag) which is in agreement with $\mu=31.786\pm0.046$ (mag) measured by SH0ES. In addition, the relations we find between periods and amplitude ratios of the Cepheids in NGC 5584 are significantly tighter than those of SH0ES and their potential impact on the direct $H_0$ measurement will be investigated in future studies. ","Inspecting the Cepheid distance ladder: The Hubble Space Telescope
  distance to the SNIa host galaxy NGC 5584",5,"['(1/5)\nIn our recent study (led by myself and including @PierreKervella &amp; @LBreuval) we inspected the Cepheid distance ladder and the Hubble Constant (H0) by an independent and complete re-analysis of the (raw) HST data of the SNIa host galaxy NGC 5584: <LINK> <LINK>', '(2/5)\nWhy we chose NGC 5584? because the periods and amplitude ratios of its Cepheids in different HST bands played a key role in the local measurement of H0 by SH0ES (see our abstract). https://t.co/2hiX0OAPYt', '(3/5)\nWe intentionally chose different analysis methods and tools, in particular, different tools for photometry and a completely different method for light curve analysis. https://t.co/yUtlcBrg4T', '(4/5)\nResult: we do not find a systematic difference between our results compared to those reported by SH0ES. The significant Hubble Constant tension remains a problem for the LCDM standard model of cosmology. https://t.co/TTVGYe9ozM', '(5 /5)\nOur paper is accepted for publication in the ApJ and my talk on the results of this study at the ""Cosmology at the Crossroads"" conference is available here: https://t.co/NbxWrx0OrE\nSee also the Tweet from @jrdmb:\nhttps://t.co/auAHsTU7IZ']",21,02,1067
31,12,1387952437823221763,1614231872,Aaron Fisher,"New paper alert!  A way to augment moving window models with features that describe long-term patterns, such as ""time since last instance of state k."" <LINK> It's based on optimizing a well-known, but heuristic method for actigraphy (Webster's rescoring rules) <LINK> Please feel welcome to DM me (or reply here, up to you!) if you have any thoughts, questions, criticisms or comments. They'd be very much appreciated.",https://arxiv.org/abs/2104.14291,"Analyzing temporal data (e.g., wearable device data) requires a decision about how to combine information from the recent and distant past. In the context of classifying sleep status from actigraphy, Webster's rescoring rules offer one popular solution based on the long-term patterns in the output of a moving-window model. Unfortunately, the question of how to optimize rescoring rules for any given setting has remained unsolved. To address this problem and expand the possible use cases of rescoring rules, we propose rephrasing these rules in terms of epoch-specific features. Our features take two general forms: (1) the time lag between now and the most recent [or closest upcoming] bout of time spent in a given state, and (2) the length of the most recent [or closest upcoming] bout of time spent in a given state. Given any initial moving window model, these features can be defined recursively, allowing for straightforward optimization of rescoring rules. Joint optimization of the moving window model and the subsequent rescoring rules can also be implemented using gradient-based optimization software, such as Tensorflow. Beyond binary classification problems (e.g., sleep-wake), the same approach can be applied to summarize long-term patterns for multi-state classification problems (e.g., sitting, walking, or stair climbing). We find that optimized rescoring rules improve the performance of sleep-wake classifiers, achieving accuracy comparable to that of certain neural network architectures. ","Optimizing Rescoring Rules with Interpretable Representations of
  Long-Term Information",2,"['New paper alert! \n\nA way to augment moving window models with features that describe long-term patterns, such as ""time since last instance of state k.""\n\n<LINK>\n\nIt\'s based on optimizing a well-known, but heuristic method for actigraphy (Webster\'s rescoring rules) <LINK>', ""Please feel welcome to DM me (or reply here, up to you!) if you have any thoughts, questions, criticisms or comments. They'd be very much appreciated.""]",21,04,418
32,74,1294311608227856385,56113666,Mengye Ren,"Towards more interactive #selfdriving, we propose a new motion forecasting network based on the transformer architecture to explicitly model interaction among actors. Check out our recent IROS'20 paper, available on arXiv: <LINK> #SelfDrivingCars @uber @uberatg <LINK> Joint work with Lingyun (Luke) Li, Bin Yang, Ming Liang @zengwenyuan1995 @seanseg @RaquelUrtasun",https://arxiv.org/abs/2008.05927,"In this paper, we tackle the problem of detecting objects in 3D and forecasting their future motion in the context of self-driving. Towards this goal, we design a novel approach that explicitly takes into account the interactions between actors. To capture their spatial-temporal dependencies, we propose a recurrent neural network with a novel Transformer architecture, which we call the Interaction Transformer. Importantly, our model can be trained end-to-end, and runs in real-time. We validate our approach on two challenging real-world datasets: ATG4D and nuScenes. We show that our approach can outperform the state-of-the-art on both datasets. In particular, we significantly improve the social compliance between the estimated future trajectories, resulting in far fewer collisions between the predicted actors. ","End-to-end Contextual Perception and Prediction with Interaction
  Transformer",2,"[""Towards more interactive #selfdriving, we propose a new motion forecasting network based on the transformer architecture to explicitly model interaction among actors. Check out our recent IROS'20 paper, available on arXiv:\n<LINK>\n\n#SelfDrivingCars @uber @uberatg <LINK>"", 'Joint work with Lingyun (Luke) Li, Bin Yang, Ming Liang @zengwenyuan1995  @seanseg  @RaquelUrtasun']",20,08,365
33,16,1209601174535434240,374233623,Shane Barratt,"New paper - Learning Convex Optimization Control Policies, w/ @akshaykagrawal, @b_stellato, and Stephen Boyd. Paper: <LINK> @akshaykagrawal @b_stellato We consider the problem of tuning convex optimization control policies (COCPs), which are control policies that compute the input or action by solving a convex optimization problem that depends on the current state and some parameters. <LINK> @akshaykagrawal @b_stellato Many control policies used in practice are in fact COCPs. Some examples include the linear quadratic regulator (LQR), convex model predictive control (MPC), and convex approximate dynamic programming (ADP) or control-Lyapunov policies. @akshaykagrawal @b_stellato Tuning the parameters in these policies is often done by hand, or by grid search. In this paper we propose a method to automate this process, by adjusting the parameters using an approximate gradient of a performance metric with respect to the parameters. @akshaykagrawal @b_stellato Our method relies on recently developed methods that can efficiently evaluate the derivative of the solution of a convex optimization problem with respect to its parameters, namely cvxpylayers (<LINK>). @akshaykagrawal @b_stellato As for numerical examples, we start by applying our method to the classical LQR problem, where the parameters are the coefficients of an approximate (quadratic) value function. We see that our method is able to recover a policy with the same cost as the LQR solution. <LINK> @akshaykagrawal @b_stellato Next we apply our method to a box-constrained LQR problem, which has no known exact solution. We use the same COCP as the LQR problem except we add the constraint that the input is in a box. <LINK> @akshaykagrawal @b_stellato Our (simple) method is able to reach the performance of a (sophisticated) method based on LMIs, introduced by Wang & Boyd (<LINK>). <LINK> @akshaykagrawal @b_stellato We provide examples in portfolio optimization (""Tuning a Markowitz policy to maximize utility""), vehicle control (""Tuning a vehicle controller to track curved paths""), and supply-chain management ("" Tuning a supply chain policy to maximize profit""). Check them out in the paper! <LINK> @akshaykagrawal @b_stellato Code for all of the examples is available online: <LINK>.",https://arxiv.org/abs/1912.09529,"Many control policies used in various applications determine the input or action by solving a convex optimization problem that depends on the current state and some parameters. Common examples of such convex optimization control policies (COCPs) include the linear quadratic regulator (LQR), convex model predictive control (MPC), and convex control-Lyapunov or approximate dynamic programming (ADP) policies. These types of control policies are tuned by varying the parameters in the optimization problem, such as the LQR weights, to obtain good performance, judged by application-specific metrics. Tuning is often done by hand, or by simple methods such as a crude grid search. In this paper we propose a method to automate this process, by adjusting the parameters using an approximate gradient of the performance metric with respect to the parameters. Our method relies on recently developed methods that can efficiently evaluate the derivative of the solution of a convex optimization problem with respect to its parameters. We illustrate our method on several examples. ",Learning Convex Optimization Control Policies,10,"['New paper - Learning Convex Optimization Control Policies, w/ @akshaykagrawal, @b_stellato, and Stephen Boyd.\n\nPaper: <LINK>', '@akshaykagrawal @b_stellato We consider the problem of tuning convex optimization control policies (COCPs), which are control policies that compute the input or action by solving a convex optimization problem that depends on the current state and some parameters. https://t.co/rcXAX9AdrZ', '@akshaykagrawal @b_stellato Many control policies used in practice are in fact COCPs. Some examples include the linear quadratic regulator (LQR), convex model predictive control (MPC), and convex approximate dynamic programming (ADP) or control-Lyapunov policies.', '@akshaykagrawal @b_stellato Tuning the parameters in these policies is often done by hand, or by grid search. In this paper we propose a method to automate this process, by adjusting the parameters using an approximate gradient of a performance metric with respect to the parameters.', '@akshaykagrawal @b_stellato Our method relies on recently developed methods that can efficiently evaluate the derivative of the solution of a convex optimization problem with respect to its parameters, namely cvxpylayers (https://t.co/J8WqcTPDsf).', '@akshaykagrawal @b_stellato As for numerical examples, we start by applying our method to the classical LQR problem, where the parameters are the coefficients of an approximate (quadratic) value function. We see that our method is able to recover a policy with the same cost as the LQR solution. https://t.co/9DjGH6KHXe', '@akshaykagrawal @b_stellato Next we apply our method to a box-constrained LQR problem, which has no known exact solution. We use the same COCP as the LQR problem except we add the constraint that the input is in a box. https://t.co/Pgkt46TG04', '@akshaykagrawal @b_stellato Our (simple) method is able to reach the performance of a (sophisticated) method based on LMIs, introduced by Wang &amp; Boyd (https://t.co/1cusAMKp7Q). https://t.co/KwT7RMdISN', '@akshaykagrawal @b_stellato We provide examples in portfolio optimization (""Tuning a Markowitz policy to maximize utility""), vehicle control (""Tuning a vehicle controller to track curved paths""), and supply-chain management ("" Tuning a supply chain policy to maximize profit""). Check them out in the paper! https://t.co/uIQoCnEJZR', '@akshaykagrawal @b_stellato Code for all of the examples is available online: https://t.co/4biJaag0wb.']",19,12,2269
34,213,1409758535857082369,869647136480219136,Turlier lab,"What are the physical principles underlying the formation of a blastocoel, the first biological cavity in animal development? We wrote a small review to draw new perspectives on this surprisingly little studied question. <LINK> Some teaser: &lt;Blastulation might not be ‚Äúthe most important time in our life‚Äù, to refer to the famous quote of Lewis Wolpert on gastrulation, but it remains the very first morphogenetic event common to most animals.&gt; @WalentekLab Well, actually not in all species, which may raise the question of its function(s) we believe. @WalentekLab Happy to get any feedbacks !",https://arxiv.org/abs/2106.14509,"The blastocoel is a fluid-filled cavity characteristic of animal embryos at the blastula stage. Its emergence is commonly described as the result of cleavage patterning, but this historical view conceals a large diversity of mechanisms and overlooks many unsolved questions from a biophysics perspective. In this review, we describe generic mechanisms for blastocoel morphogenesis, rooted in biological literature and simple physical principles. We propose novel directions of study and emphasize the importance to study blastocoel morphogenesis as an evolutionary and physical continuum. ",Blastocoel morphogenesis: a biophysics perspective,4,"['What are the physical principles underlying the formation of a blastocoel, the first biological cavity in animal development? We wrote a small review to draw new perspectives on this surprisingly little studied question.\n<LINK>', 'Some teaser: &lt;Blastulation might not be ‚Äúthe most important time in our life‚Äù, to refer to the famous quote of Lewis Wolpert on gastrulation, but it remains the very first morphogenetic event common to most animals.&gt;', '@WalentekLab Well, actually not in all species, which may raise the question of its function(s) we believe.', '@WalentekLab Happy to get any feedbacks !']",21,06,600
